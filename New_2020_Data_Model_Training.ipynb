{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all the train, val, and test file names and labels\n",
    "train_excel = pd.read_excel('G:/School/2020 Mozilla Dataset/Excel Files/train.xlsx', sheet_name=0)\n",
    "val_excel = pd.read_excel('G:/School/2020 Mozilla Dataset/Excel Files/validation.xlsx', sheet_name=0)\n",
    "test_excel = pd.read_excel('G:/School/2020 Mozilla Dataset/Excel Files/test.xlsx', sheet_name=0)\n",
    "\n",
    "# Train\n",
    "train_filenames_labels = (train_excel['path'].tolist(), train_excel['sentence'].tolist())\n",
    "# filename_train = train_excel['path'].tolist()\n",
    "# labels_train = train_excel['sentence'].tolist()\n",
    "\n",
    "# Validation\n",
    "val_filenames_labels = (val_excel['path'].tolist(), val_excel['sentence'].tolist())\n",
    "# filename_val = val_excel['path'].tolist()\n",
    "# labels_val = val_excel['sentence'].tolist()\n",
    "\n",
    "# Test\n",
    "test_filenames_labels = (test_excel['path'].tolist(), test_excel['sentence'].tolist())\n",
    "# filename_test = test_excel['path'].tolist()\n",
    "# labels_test = test_excel['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type in the directory of the unzipped mp3_to_np file (ex. r'G:/School/WAV_to_np')\n",
    "root =  r'G:/School/2020 Mozilla Dataset/mp3_to_np'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for processing data\n",
    "def avg_wer(wer_scores, combined_ref_len):\n",
    "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
    "\n",
    "\n",
    "def _levenshtein_distance(ref, hyp):\n",
    "    \"\"\"Levenshtein distance is a string metric for measuring the difference\n",
    "    between two sequences. Informally, the levenshtein disctance is defined as\n",
    "    the minimum number of single-character edits (substitutions, insertions or\n",
    "    deletions) required to change one word into the other. We can naturally\n",
    "    extend the edits to word level when calculate levenshtein disctance for\n",
    "    two sentences.\n",
    "    \"\"\"\n",
    "    m = len(ref)\n",
    "    n = len(hyp)\n",
    "\n",
    "    # special case\n",
    "    if ref == hyp:\n",
    "        return 0\n",
    "    if m == 0:\n",
    "        return n\n",
    "    if n == 0:\n",
    "        return m\n",
    "\n",
    "    if m < n:\n",
    "        ref, hyp = hyp, ref\n",
    "        m, n = n, m\n",
    "\n",
    "    # use O(min(m, n)) space\n",
    "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
    "\n",
    "    # initialize distance matrix\n",
    "    for j in range(0,n + 1):\n",
    "        distance[0][j] = j\n",
    "\n",
    "    # calculate levenshtein distance\n",
    "    for i in range(1, m + 1):\n",
    "        prev_row_idx = (i - 1) % 2\n",
    "        cur_row_idx = i % 2\n",
    "        distance[cur_row_idx][0] = i\n",
    "        for j in range(1, n + 1):\n",
    "            if ref[i - 1] == hyp[j - 1]:\n",
    "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
    "            else:\n",
    "                s_num = distance[prev_row_idx][j - 1] + 1\n",
    "                i_num = distance[cur_row_idx][j - 1] + 1\n",
    "                d_num = distance[prev_row_idx][j] + 1\n",
    "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
    "\n",
    "    return distance[m % 2][n]\n",
    "\n",
    "\n",
    "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in word-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Levenshtein distance and word number of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    ref_words = reference.split(delimiter)\n",
    "    hyp_words = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
    "    return float(edit_distance), len(ref_words)\n",
    "\n",
    "\n",
    "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Compute the levenshtein distance between reference sequence and\n",
    "    hypothesis sequence in char-level.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Levenshtein distance and length of reference sentence.\n",
    "    :rtype: list\n",
    "    \"\"\"\n",
    "    if ignore_case == True:\n",
    "        reference = reference.lower()\n",
    "        hypothesis = hypothesis.lower()\n",
    "\n",
    "    join_char = ' '\n",
    "    if remove_space == True:\n",
    "        join_char = ''\n",
    "\n",
    "    reference = join_char.join(filter(None, reference.split(' ')))\n",
    "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
    "\n",
    "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
    "    return float(edit_distance), len(reference)\n",
    "\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    \"\"\"Calculate word error rate (WER). WER compares reference text and\n",
    "    hypothesis text in word-level. WER is defined as:\n",
    "    .. math::\n",
    "        WER = (Sw + Dw + Iw) / Nw\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sw is the number of words subsituted,\n",
    "        Dw is the number of words deleted,\n",
    "        Iw is the number of words inserted,\n",
    "        Nw is the number of words in the reference\n",
    "    We can use levenshtein distance to calculate WER. Please draw an attention\n",
    "    that empty items will be removed when splitting sentences by delimiter.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param delimiter: Delimiter of input sentences.\n",
    "    :type delimiter: char\n",
    "    :return: Word error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If word number of reference is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
    "                                         delimiter)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
    "\n",
    "    wer = float(edit_distance) / ref_len\n",
    "    return wer\n",
    "\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    \"\"\"Calculate charactor error rate (CER). CER compares reference text and\n",
    "    hypothesis text in char-level. CER is defined as:\n",
    "    .. math::\n",
    "        CER = (Sc + Dc + Ic) / Nc\n",
    "    where\n",
    "    .. code-block:: text\n",
    "        Sc is the number of characters substituted,\n",
    "        Dc is the number of characters deleted,\n",
    "        Ic is the number of characters inserted\n",
    "        Nc is the number of characters in the reference\n",
    "    We can use levenshtein distance to calculate CER. Chinese input should be\n",
    "    encoded to unicode. Please draw an attention that the leading and tailing\n",
    "    space characters will be truncated and multiple consecutive space\n",
    "    characters in a sentence will be replaced by one space character.\n",
    "    :param reference: The reference sentence.\n",
    "    :type reference: basestring\n",
    "    :param hypothesis: The hypothesis sentence.\n",
    "    :type hypothesis: basestring\n",
    "    :param ignore_case: Whether case-sensitive or not.\n",
    "    :type ignore_case: bool\n",
    "    :param remove_space: Whether remove internal space characters\n",
    "    :type remove_space: bool\n",
    "    :return: Character error rate.\n",
    "    :rtype: float\n",
    "    :raises ValueError: If the reference length is zero.\n",
    "    \"\"\"\n",
    "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
    "                                         remove_space)\n",
    "\n",
    "    if ref_len == 0:\n",
    "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
    "\n",
    "    cer = float(edit_distance) / ref_len\n",
    "    return cer\n",
    "\n",
    "class TextTransform:\n",
    "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
    "    def __init__(self):\n",
    "        char_map_str = \"\"\"\n",
    "        ' 0\n",
    "        <SPACE> 1\n",
    "        a 2\n",
    "        b 3\n",
    "        c 4\n",
    "        d 5\n",
    "        e 6\n",
    "        f 7\n",
    "        g 8\n",
    "        h 9\n",
    "        i 10\n",
    "        j 11\n",
    "        k 12\n",
    "        l 13\n",
    "        m 14\n",
    "        n 15\n",
    "        o 16\n",
    "        p 17\n",
    "        q 18\n",
    "        r 19\n",
    "        s 20\n",
    "        t 21\n",
    "        u 22\n",
    "        v 23\n",
    "        w 24\n",
    "        x 25\n",
    "        y 26\n",
    "        z 27\n",
    "        \"\"\"\n",
    "        self.char_map = {}\n",
    "        self.index_map = {}\n",
    "        for line in char_map_str.strip().split('\\n'):\n",
    "            ch, index = line.split()\n",
    "            self.char_map[ch] = int(index)\n",
    "            self.index_map[int(index)] = ch\n",
    "        self.index_map[1] = ' '\n",
    "\n",
    "    def text_to_int(self, text):\n",
    "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
    "        int_sequence = []\n",
    "        for c in text:\n",
    "            if c == ' ':\n",
    "                ch = self.char_map['<SPACE>']\n",
    "            else:\n",
    "                ch = self.char_map[c]\n",
    "            int_sequence.append(ch)\n",
    "        return int_sequence\n",
    "\n",
    "    def int_to_text(self, labels):\n",
    "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
    "        string = []\n",
    "        for i in labels:\n",
    "            string.append(self.index_map[i])\n",
    "        return ''.join(string).replace('<SPACE>', ' ')\n",
    "\n",
    "text_transform = TextTransform()\n",
    "\n",
    "def data_processing(data):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    for (spec, utterance) in data:\n",
    "        spectrograms.append(torch.Tensor(spec).detach())\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label.detach())\n",
    "        input_lengths.append(torch.Tensor(spec).detach().shape[0]//2)\n",
    "        label_lengths.append(len(label.detach()))\n",
    "\n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n",
    "\n",
    "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
    "    arg_maxes = torch.argmax(output, dim=2)\n",
    "    decodes = []\n",
    "    targets = []\n",
    "    for i, args in enumerate(arg_maxes):\n",
    "      decode = []\n",
    "      targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
    "      for j, index in enumerate(args):\n",
    "        if index != blank_label:\n",
    "          if collapse_repeated and j != 0 and index == args[j -1]:\n",
    "            continue\n",
    "          decode.append(index.item())\n",
    "      decodes.append(text_transform.int_to_text(decode))\n",
    "    return decodes, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Data Loader that can load Mozilla AudioSet\n",
    "\n",
    "def load_speech_item(file_name, label_text, path):\n",
    "    fpath = path + '/' + file_name\n",
    "\n",
    "    # Load Audio\n",
    "    spec = np.load(fpath)   \n",
    "\n",
    "    return (spec, label_text)\n",
    "\n",
    "class Data_Loader(Dataset):\n",
    "    def __init__(self, root, filenames_labels):\n",
    "        self._path = root\n",
    "        self._filenames, self._labels = filenames_labels\n",
    "\n",
    "    def __getitem__(self, n):\n",
    "        file_name = self._filenames[n] + \".npy\"\n",
    "        label_text = self._labels[n]\n",
    "        return load_speech_item(file_name, label_text, self._path)\n",
    "      \n",
    "    def __len__(self):\n",
    "        return len(self._filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layer_norm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (batch, channel, feature, time)\n",
    "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
    "        x = self.layer_norm(x)\n",
    "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
    "\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
    "        except with layer norm instead of batch norm\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "\n",
    "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x  # (batch, channel, feature, time)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x # (batch, channel, feature, time)\n",
    "\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim, hidden_size=hidden_size,\n",
    "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats//2\n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
    "\n",
    "        # n residual cnn layers with filter size of 32\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        sizes = x.size()\n",
    "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
    "        x = x.transpose(1, 2) # (batch, time, feature)\n",
    "        x = self.fully_connected(x)\n",
    "        x = self.birnn_layers(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation\n",
    "\n",
    "class IterMeter(object):\n",
    "    \"\"\"keeps track of total iterations\"\"\"\n",
    "    def __init__(self):\n",
    "        self.val = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.val += 1\n",
    "\n",
    "    def get(self):\n",
    "        return self.val\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
    "    model.train()\n",
    "    data_len = len(train_loader.dataset)\n",
    "    losses = []\n",
    "\n",
    "    for batch_idx, _data in enumerate(train_loader):\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data \n",
    "        if spectrograms.shape[3] > 2600:\n",
    "            print(\"Skipped a batch because it is too large (>2600). Sequence length is\", spectrograms.shape[3])\n",
    "            continue\n",
    "\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(spectrograms)  # (batch, time, n_class)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "        \n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        losses.append(round(loss.detach().item(), 4))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        iter_meter.step()\n",
    "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(spectrograms), data_len,\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "        torch.cuda.empty_cache()\n",
    "        del spectrograms, labels\n",
    "        gc.collect()  \n",
    "    \n",
    "    return losses\n",
    "\n",
    "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    losses, test_cer, test_wer = [], [], []\n",
    "    data_len = len(test_loader.dataset)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, _data in enumerate(test_loader):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data \n",
    "            if spectrograms.shape[3] > 2600:\n",
    "                print(\"Skipped a batch because it is too large (>2600). Sequence length is\", spectrograms.shape[3])\n",
    "                continue                \n",
    "\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            output = model(spectrograms)  # (batch, time, n_class)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
    "\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            test_loss += loss.item() / len(test_loader)\n",
    "\n",
    "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
    "            \n",
    "            if i % 1000 == 0 or i == data_len:\n",
    "                print(decoded_preds, decoded_targets)\n",
    "                \n",
    "            for j in range(len(decoded_preds)):\n",
    "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
    "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            del spectrograms, labels\n",
    "            gc.collect()   \n",
    "\n",
    "    avg_cer = sum(test_cer)/len(test_cer)\n",
    "    avg_wer = sum(test_wer)/len(test_wer)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
    "    \n",
    "    return round(test_loss, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_filenames_labels, val_filenames_labels, root, attempt_number, learning_rate=5e-4, batch_size=20, epochs=10):\n",
    "    \n",
    "    losses_train, losses_val = [], []\n",
    "    hparams = {\n",
    "        \"n_cnn_layers\": 3,\n",
    "        \"n_rnn_layers\": 5,\n",
    "        \"rnn_dim\": 512,\n",
    "        \"n_class\": 29,\n",
    "        \"n_feats\": 128,\n",
    "        \"stride\":2,\n",
    "        \"dropout\": 0.1,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(7)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    train_dataset = Data_Loader(root, train_filenames_labels)\n",
    "    test_dataset = Data_Loader(root, val_filenames_labels)\n",
    "\n",
    "    kwargs = {'num_workers': 0, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=True,\n",
    "                                collate_fn=lambda x: data_processing(x),\n",
    "                                **kwargs)\n",
    "    test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                                batch_size=hparams['batch_size'],\n",
    "                                shuffle=False,\n",
    "                                collate_fn=lambda x: data_processing(x),\n",
    "                                **kwargs)\n",
    "\n",
    "    model = SpeechRecognitionModel(\n",
    "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
    "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
    "        ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
    "    criterion = nn.CTCLoss(blank=28).to(device)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
    "                                            steps_per_epoch=int(len(train_loader)),\n",
    "                                            epochs=hparams['epochs'],\n",
    "                                            anneal_strategy='linear')\n",
    "\n",
    "    iter_meter = IterMeter()\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        losses_train.append(train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter))\n",
    "        losses_val.append(test(model, device, test_loader, criterion, epoch, iter_meter))\n",
    "        \n",
    "        state = {\n",
    "            'epoch': epoch,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }  \n",
    "        model_name = f'attemptnumber_{attempt_number}_epoch_{epoch}_lr_{learning_rate}_batchsize{batch_size}.pt'\n",
    "        torch.save(state, r'G:/School/2020 Mozilla Dataset/saved_models/' + model_name)\n",
    "    return losses_train, losses_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/244675 (0%)]\tLoss: 29.139435\n",
      "Train Epoch: 1 [1000/244675 (0%)]\tLoss: 3.362687\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 1 [2000/244675 (1%)]\tLoss: 3.102915\n",
      "Train Epoch: 1 [3000/244675 (1%)]\tLoss: 3.000282\n",
      "Train Epoch: 1 [4000/244675 (2%)]\tLoss: 3.080284\n",
      "Train Epoch: 1 [5000/244675 (2%)]\tLoss: 2.944926\n",
      "Train Epoch: 1 [6000/244675 (2%)]\tLoss: 2.960075\n",
      "Train Epoch: 1 [7000/244675 (3%)]\tLoss: 2.986109\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 1 [8000/244675 (3%)]\tLoss: 3.062364\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 1 [9000/244675 (4%)]\tLoss: 3.345297\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 1 [10000/244675 (4%)]\tLoss: 2.993338\n",
      "Train Epoch: 1 [11000/244675 (4%)]\tLoss: 3.164021\n",
      "Train Epoch: 1 [12000/244675 (5%)]\tLoss: 3.546245\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 1 [13000/244675 (5%)]\tLoss: 3.036560\n",
      "Train Epoch: 1 [14000/244675 (6%)]\tLoss: 2.986198\n",
      "Train Epoch: 1 [15000/244675 (6%)]\tLoss: 2.991124\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 1 [16000/244675 (7%)]\tLoss: 2.967310\n",
      "Train Epoch: 1 [17000/244675 (7%)]\tLoss: 2.975071\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 1 [18000/244675 (7%)]\tLoss: 2.958479\n",
      "Train Epoch: 1 [19000/244675 (8%)]\tLoss: 3.044359\n",
      "Train Epoch: 1 [20000/244675 (8%)]\tLoss: 2.972967\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 1 [21000/244675 (9%)]\tLoss: 2.986978\n",
      "Train Epoch: 1 [22000/244675 (9%)]\tLoss: 3.809187\n",
      "Train Epoch: 1 [23000/244675 (9%)]\tLoss: 2.997754\n",
      "Train Epoch: 1 [24000/244675 (10%)]\tLoss: 2.967813\n",
      "Train Epoch: 1 [25000/244675 (10%)]\tLoss: 2.960966\n",
      "Train Epoch: 1 [26000/244675 (11%)]\tLoss: 3.169337\n",
      "Train Epoch: 1 [27000/244675 (11%)]\tLoss: 2.935278\n",
      "Train Epoch: 1 [28000/244675 (11%)]\tLoss: 2.926864\n",
      "Train Epoch: 1 [29000/244675 (12%)]\tLoss: 3.006287\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 1 [30000/244675 (12%)]\tLoss: 2.960284\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 1 [31000/244675 (13%)]\tLoss: 2.937628\n",
      "Train Epoch: 1 [32000/244675 (13%)]\tLoss: 2.883636\n",
      "Train Epoch: 1 [33000/244675 (13%)]\tLoss: 2.962560\n",
      "Train Epoch: 1 [34000/244675 (14%)]\tLoss: 2.922884\n",
      "Train Epoch: 1 [35000/244675 (14%)]\tLoss: 2.960386\n",
      "Train Epoch: 1 [36000/244675 (15%)]\tLoss: 2.923353\n",
      "Train Epoch: 1 [37000/244675 (15%)]\tLoss: 2.924795\n",
      "Train Epoch: 1 [38000/244675 (16%)]\tLoss: 3.014340\n",
      "Train Epoch: 1 [39000/244675 (16%)]\tLoss: 2.845397\n",
      "Train Epoch: 1 [40000/244675 (16%)]\tLoss: 2.874441\n",
      "Train Epoch: 1 [41000/244675 (17%)]\tLoss: 2.909559\n",
      "Train Epoch: 1 [42000/244675 (17%)]\tLoss: 2.885537\n",
      "Train Epoch: 1 [43000/244675 (18%)]\tLoss: 2.891413\n",
      "Train Epoch: 1 [44000/244675 (18%)]\tLoss: 2.858110\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 1 [45000/244675 (18%)]\tLoss: 2.861561\n",
      "Train Epoch: 1 [46000/244675 (19%)]\tLoss: 2.962059\n",
      "Train Epoch: 1 [47000/244675 (19%)]\tLoss: 2.791901\n",
      "Train Epoch: 1 [48000/244675 (20%)]\tLoss: 2.805856\n",
      "Train Epoch: 1 [49000/244675 (20%)]\tLoss: 2.818002\n",
      "Train Epoch: 1 [50000/244675 (20%)]\tLoss: 2.734771\n",
      "Train Epoch: 1 [51000/244675 (21%)]\tLoss: 2.788131\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 1 [52000/244675 (21%)]\tLoss: 2.613600\n",
      "Train Epoch: 1 [53000/244675 (22%)]\tLoss: 2.676653\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 1 [54000/244675 (22%)]\tLoss: 2.676399\n",
      "Train Epoch: 1 [55000/244675 (22%)]\tLoss: 2.561333\n",
      "Train Epoch: 1 [56000/244675 (23%)]\tLoss: 2.482237\n",
      "Train Epoch: 1 [57000/244675 (23%)]\tLoss: 2.599750\n",
      "Train Epoch: 1 [58000/244675 (24%)]\tLoss: 2.407194\n",
      "Train Epoch: 1 [59000/244675 (24%)]\tLoss: 2.348203\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 1 [60000/244675 (25%)]\tLoss: 2.326935\n",
      "Train Epoch: 1 [61000/244675 (25%)]\tLoss: 2.293340\n",
      "Train Epoch: 1 [62000/244675 (25%)]\tLoss: 2.428157\n",
      "Train Epoch: 1 [63000/244675 (26%)]\tLoss: 2.551507\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 1 [64000/244675 (26%)]\tLoss: 2.259838\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 1 [65000/244675 (27%)]\tLoss: 2.176702\n",
      "Train Epoch: 1 [66000/244675 (27%)]\tLoss: 2.365838\n",
      "Train Epoch: 1 [67000/244675 (27%)]\tLoss: 2.268743\n",
      "Train Epoch: 1 [68000/244675 (28%)]\tLoss: 2.271913\n",
      "Train Epoch: 1 [69000/244675 (28%)]\tLoss: 2.494700\n",
      "Train Epoch: 1 [70000/244675 (29%)]\tLoss: 2.246633\n",
      "Train Epoch: 1 [71000/244675 (29%)]\tLoss: 2.242627\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 1 [72000/244675 (29%)]\tLoss: 2.366516\n",
      "Train Epoch: 1 [73000/244675 (30%)]\tLoss: 2.163054\n",
      "Train Epoch: 1 [74000/244675 (30%)]\tLoss: 2.171823\n",
      "Train Epoch: 1 [75000/244675 (31%)]\tLoss: 2.129576\n",
      "Train Epoch: 1 [76000/244675 (31%)]\tLoss: 2.466631\n",
      "Train Epoch: 1 [77000/244675 (31%)]\tLoss: 2.183286\n",
      "Train Epoch: 1 [78000/244675 (32%)]\tLoss: 2.125629\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 1 [79000/244675 (32%)]\tLoss: 2.141282\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 1 [80000/244675 (33%)]\tLoss: 1.960231\n",
      "Train Epoch: 1 [81000/244675 (33%)]\tLoss: 2.058555\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 1 [82000/244675 (34%)]\tLoss: 2.147245\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 1 [83000/244675 (34%)]\tLoss: 2.144755\n",
      "Train Epoch: 1 [84000/244675 (34%)]\tLoss: 2.120065\n",
      "Train Epoch: 1 [85000/244675 (35%)]\tLoss: 2.020478\n",
      "Train Epoch: 1 [86000/244675 (35%)]\tLoss: 2.219733\n",
      "Train Epoch: 1 [87000/244675 (36%)]\tLoss: 2.124523\n",
      "Train Epoch: 1 [88000/244675 (36%)]\tLoss: 2.034905\n",
      "Train Epoch: 1 [89000/244675 (36%)]\tLoss: 2.230946\n",
      "Train Epoch: 1 [90000/244675 (37%)]\tLoss: 1.797822\n",
      "Train Epoch: 1 [91000/244675 (37%)]\tLoss: 2.030568\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 1 [92000/244675 (38%)]\tLoss: 1.830083\n",
      "Train Epoch: 1 [93000/244675 (38%)]\tLoss: 1.753836\n",
      "Train Epoch: 1 [94000/244675 (38%)]\tLoss: 2.077010\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 1 [95000/244675 (39%)]\tLoss: 1.964562\n",
      "Train Epoch: 1 [96000/244675 (39%)]\tLoss: 1.970056\n",
      "Train Epoch: 1 [97000/244675 (40%)]\tLoss: 2.139667\n",
      "Train Epoch: 1 [98000/244675 (40%)]\tLoss: 1.961112\n",
      "Train Epoch: 1 [99000/244675 (40%)]\tLoss: 1.971841\n",
      "Train Epoch: 1 [100000/244675 (41%)]\tLoss: 2.151895\n",
      "Train Epoch: 1 [101000/244675 (41%)]\tLoss: 1.980397\n",
      "Train Epoch: 1 [102000/244675 (42%)]\tLoss: 1.912812\n",
      "Train Epoch: 1 [103000/244675 (42%)]\tLoss: 1.982195\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 1 [105000/244675 (43%)]\tLoss: 2.164080\n",
      "Train Epoch: 1 [106000/244675 (43%)]\tLoss: 1.784073\n",
      "Train Epoch: 1 [107000/244675 (44%)]\tLoss: 1.821605\n",
      "Train Epoch: 1 [108000/244675 (44%)]\tLoss: 1.793590\n",
      "Train Epoch: 1 [109000/244675 (45%)]\tLoss: 1.706298\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 1 [110000/244675 (45%)]\tLoss: 1.875942\n",
      "Train Epoch: 1 [111000/244675 (45%)]\tLoss: 1.611878\n",
      "Train Epoch: 1 [112000/244675 (46%)]\tLoss: 1.843010\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 1 [113000/244675 (46%)]\tLoss: 1.815022\n",
      "Train Epoch: 1 [114000/244675 (47%)]\tLoss: 1.793116\n",
      "Train Epoch: 1 [115000/244675 (47%)]\tLoss: 1.853398\n",
      "Train Epoch: 1 [116000/244675 (47%)]\tLoss: 1.946780\n",
      "Train Epoch: 1 [117000/244675 (48%)]\tLoss: 1.728350\n",
      "Train Epoch: 1 [118000/244675 (48%)]\tLoss: 1.885384\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 1 [119000/244675 (49%)]\tLoss: 1.929479\n",
      "Train Epoch: 1 [120000/244675 (49%)]\tLoss: 1.691014\n",
      "Train Epoch: 1 [121000/244675 (49%)]\tLoss: 1.884965\n",
      "Train Epoch: 1 [122000/244675 (50%)]\tLoss: 1.721183\n",
      "Train Epoch: 1 [123000/244675 (50%)]\tLoss: 1.968542\n",
      "Train Epoch: 1 [124000/244675 (51%)]\tLoss: 1.710798\n",
      "Train Epoch: 1 [125000/244675 (51%)]\tLoss: 2.049209\n",
      "Train Epoch: 1 [126000/244675 (51%)]\tLoss: 1.858178\n",
      "Train Epoch: 1 [127000/244675 (52%)]\tLoss: 2.137756\n",
      "Train Epoch: 1 [128000/244675 (52%)]\tLoss: 1.908025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 1 [129000/244675 (53%)]\tLoss: 1.816744\n",
      "Train Epoch: 1 [130000/244675 (53%)]\tLoss: 1.750528\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 1 [131000/244675 (54%)]\tLoss: 1.856630\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 1 [132000/244675 (54%)]\tLoss: 1.806795\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 1 [133000/244675 (54%)]\tLoss: 1.780367\n",
      "Train Epoch: 1 [134000/244675 (55%)]\tLoss: 1.810966\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 1 [135000/244675 (55%)]\tLoss: 1.593030\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Train Epoch: 1 [136000/244675 (56%)]\tLoss: 1.931612\n",
      "Train Epoch: 1 [137000/244675 (56%)]\tLoss: 1.815022\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 1 [138000/244675 (56%)]\tLoss: 1.697827\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 1 [139000/244675 (57%)]\tLoss: 1.646136\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 1 [140000/244675 (57%)]\tLoss: 1.932273\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 1 [141000/244675 (58%)]\tLoss: 1.645047\n",
      "Train Epoch: 1 [142000/244675 (58%)]\tLoss: 1.689775\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 1 [143000/244675 (58%)]\tLoss: 1.803631\n",
      "Train Epoch: 1 [144000/244675 (59%)]\tLoss: 1.848297\n",
      "Train Epoch: 1 [145000/244675 (59%)]\tLoss: 1.607229\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 1 [146000/244675 (60%)]\tLoss: 2.181434\n",
      "Train Epoch: 1 [147000/244675 (60%)]\tLoss: 1.561962\n",
      "Train Epoch: 1 [148000/244675 (60%)]\tLoss: 1.644384\n",
      "Train Epoch: 1 [149000/244675 (61%)]\tLoss: 1.486946\n",
      "Train Epoch: 1 [150000/244675 (61%)]\tLoss: 1.605655\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 1 [151000/244675 (62%)]\tLoss: 1.668451\n",
      "Train Epoch: 1 [152000/244675 (62%)]\tLoss: 1.750969\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 1 [153000/244675 (63%)]\tLoss: 1.766783\n",
      "Train Epoch: 1 [154000/244675 (63%)]\tLoss: 1.918842\n",
      "Train Epoch: 1 [155000/244675 (63%)]\tLoss: 1.723148\n",
      "Train Epoch: 1 [156000/244675 (64%)]\tLoss: 1.827483\n",
      "Train Epoch: 1 [157000/244675 (64%)]\tLoss: 1.658946\n",
      "Train Epoch: 1 [158000/244675 (65%)]\tLoss: 1.770475\n",
      "Train Epoch: 1 [159000/244675 (65%)]\tLoss: 1.773637\n",
      "Train Epoch: 1 [160000/244675 (65%)]\tLoss: 1.586274\n",
      "Train Epoch: 1 [161000/244675 (66%)]\tLoss: 1.742173\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 1 [162000/244675 (66%)]\tLoss: 1.844682\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 1 [163000/244675 (67%)]\tLoss: 1.649040\n",
      "Train Epoch: 1 [164000/244675 (67%)]\tLoss: 1.944915\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 1 [165000/244675 (67%)]\tLoss: 1.527624\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 1 [166000/244675 (68%)]\tLoss: 1.795416\n",
      "Train Epoch: 1 [167000/244675 (68%)]\tLoss: 2.142402\n",
      "Train Epoch: 1 [168000/244675 (69%)]\tLoss: 1.768820\n",
      "Train Epoch: 1 [169000/244675 (69%)]\tLoss: 1.674205\n",
      "Train Epoch: 1 [170000/244675 (69%)]\tLoss: 1.741481\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 1 [171000/244675 (70%)]\tLoss: 1.526801\n",
      "Train Epoch: 1 [172000/244675 (70%)]\tLoss: 1.681927\n",
      "Train Epoch: 1 [173000/244675 (71%)]\tLoss: 1.880080\n",
      "Train Epoch: 1 [174000/244675 (71%)]\tLoss: 1.601396\n",
      "Train Epoch: 1 [175000/244675 (72%)]\tLoss: 1.546121\n",
      "Train Epoch: 1 [176000/244675 (72%)]\tLoss: 1.609703\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 1 [177000/244675 (72%)]\tLoss: 1.680351\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 1 [178000/244675 (73%)]\tLoss: 1.546112\n",
      "Train Epoch: 1 [179000/244675 (73%)]\tLoss: 1.519103\n",
      "Train Epoch: 1 [180000/244675 (74%)]\tLoss: 1.619134\n",
      "Train Epoch: 1 [181000/244675 (74%)]\tLoss: 1.869307\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 1 [182000/244675 (74%)]\tLoss: 1.554335\n",
      "Train Epoch: 1 [183000/244675 (75%)]\tLoss: 1.484567\n",
      "Train Epoch: 1 [184000/244675 (75%)]\tLoss: 1.599992\n",
      "Train Epoch: 1 [185000/244675 (76%)]\tLoss: 1.693794\n",
      "Train Epoch: 1 [186000/244675 (76%)]\tLoss: 1.577836\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 1 [187000/244675 (76%)]\tLoss: 1.177006\n",
      "Train Epoch: 1 [188000/244675 (77%)]\tLoss: 1.587974\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 1 [189000/244675 (77%)]\tLoss: 1.715011\n",
      "Train Epoch: 1 [190000/244675 (78%)]\tLoss: 1.721854\n",
      "Train Epoch: 1 [191000/244675 (78%)]\tLoss: 1.562271\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 1 [192000/244675 (78%)]\tLoss: 1.765295\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 1 [193000/244675 (79%)]\tLoss: 1.719031\n",
      "Train Epoch: 1 [194000/244675 (79%)]\tLoss: 1.771122\n",
      "Train Epoch: 1 [195000/244675 (80%)]\tLoss: 1.495778\n",
      "Train Epoch: 1 [196000/244675 (80%)]\tLoss: 1.686448\n",
      "Train Epoch: 1 [197000/244675 (81%)]\tLoss: 1.703599\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 1 [198000/244675 (81%)]\tLoss: 1.648257\n",
      "Train Epoch: 1 [199000/244675 (81%)]\tLoss: 1.518853\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 1 [200000/244675 (82%)]\tLoss: 1.753466\n",
      "Train Epoch: 1 [201000/244675 (82%)]\tLoss: 1.766514\n",
      "Train Epoch: 1 [202000/244675 (83%)]\tLoss: 1.676836\n",
      "Train Epoch: 1 [203000/244675 (83%)]\tLoss: 1.615322\n",
      "Train Epoch: 1 [204000/244675 (83%)]\tLoss: 1.581135\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 1 [205000/244675 (84%)]\tLoss: 1.354591\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 1 [207000/244675 (85%)]\tLoss: 1.610772\n",
      "Train Epoch: 1 [208000/244675 (85%)]\tLoss: 1.555699\n",
      "Train Epoch: 1 [209000/244675 (85%)]\tLoss: 1.658670\n",
      "Train Epoch: 1 [210000/244675 (86%)]\tLoss: 1.614906\n",
      "Train Epoch: 1 [211000/244675 (86%)]\tLoss: 1.714822\n",
      "Train Epoch: 1 [212000/244675 (87%)]\tLoss: 1.541964\n",
      "Train Epoch: 1 [213000/244675 (87%)]\tLoss: 1.685272\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 1 [214000/244675 (87%)]\tLoss: 1.659357\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 1 [215000/244675 (88%)]\tLoss: 1.828676\n",
      "Train Epoch: 1 [216000/244675 (88%)]\tLoss: 1.520838\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 1 [217000/244675 (89%)]\tLoss: 1.840512\n",
      "Train Epoch: 1 [218000/244675 (89%)]\tLoss: 1.666279\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 1 [219000/244675 (90%)]\tLoss: 1.627217\n",
      "Train Epoch: 1 [220000/244675 (90%)]\tLoss: 1.536057\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 1 [221000/244675 (90%)]\tLoss: 1.490893\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 1 [222000/244675 (91%)]\tLoss: 1.595113\n",
      "Train Epoch: 1 [223000/244675 (91%)]\tLoss: 1.431949\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 1 [224000/244675 (92%)]\tLoss: 1.766927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 1 [225000/244675 (92%)]\tLoss: 1.533284\n",
      "Train Epoch: 1 [226000/244675 (92%)]\tLoss: 1.462047\n",
      "Train Epoch: 1 [227000/244675 (93%)]\tLoss: 1.549921\n",
      "Train Epoch: 1 [228000/244675 (93%)]\tLoss: 1.490579\n",
      "Train Epoch: 1 [229000/244675 (94%)]\tLoss: 1.589989\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 1 [230000/244675 (94%)]\tLoss: 1.540252\n",
      "Train Epoch: 1 [231000/244675 (94%)]\tLoss: 1.614343\n",
      "Train Epoch: 1 [232000/244675 (95%)]\tLoss: 1.634687\n",
      "Train Epoch: 1 [233000/244675 (95%)]\tLoss: 1.495801\n",
      "Train Epoch: 1 [234000/244675 (96%)]\tLoss: 1.651552\n",
      "Train Epoch: 1 [235000/244675 (96%)]\tLoss: 1.493350\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 1 [236000/244675 (96%)]\tLoss: 1.681533\n",
      "Train Epoch: 1 [237000/244675 (97%)]\tLoss: 1.456264\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 1 [238000/244675 (97%)]\tLoss: 1.549565\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 1 [239000/244675 (98%)]\tLoss: 1.663319\n",
      "Train Epoch: 1 [240000/244675 (98%)]\tLoss: 1.446941\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 1 [241000/244675 (98%)]\tLoss: 1.385771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 1 [242000/244675 (99%)]\tLoss: 1.737445\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 1 [243000/244675 (99%)]\tLoss: 1.571348\n",
      "Train Epoch: 1 [244000/244675 (100%)]\tLoss: 1.588500\n",
      "['pato mon e tere rated hargen i seted lit as fieer a grema', 'a wa nat b wand basito base pla soa', 'a scokes spay is pafe e nafto word of bars in ete potinta etackers', 'wrd in doked tmare toleter dat yerr', 'as wo spo was patin o thecoeta bay cebe if tos', 'he ta o rardi trussn fli iton ba er ech ten sutationts', 'a pat mar refusa sonea a cest to baka', 'the provents is rich in maer theposits aclingod in coper', 'acoanto sosoe the geagrefecsxtetya maarigese deswof bexste na in tarmo myngrestets', \"it's undersurfis is conca fom thefor bacerd and con bexs fom sie to sid\"] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.4963, Average CER: 0.476626 Average WER: 0.9030\n",
      "\n",
      "Train Epoch: 2 [0/244675 (0%)]\tLoss: 1.675374\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 2 [1000/244675 (0%)]\tLoss: 1.490171\n",
      "Train Epoch: 2 [2000/244675 (1%)]\tLoss: 1.484278\n",
      "Train Epoch: 2 [3000/244675 (1%)]\tLoss: 1.644438\n",
      "Train Epoch: 2 [4000/244675 (2%)]\tLoss: 1.520552\n",
      "Train Epoch: 2 [5000/244675 (2%)]\tLoss: 1.579449\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 2 [6000/244675 (2%)]\tLoss: 1.689953\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Train Epoch: 2 [7000/244675 (3%)]\tLoss: 1.388515\n",
      "Train Epoch: 2 [8000/244675 (3%)]\tLoss: 1.582459\n",
      "Train Epoch: 2 [9000/244675 (4%)]\tLoss: 1.645878\n",
      "Train Epoch: 2 [10000/244675 (4%)]\tLoss: 1.664837\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 2 [11000/244675 (4%)]\tLoss: 1.296465\n",
      "Train Epoch: 2 [12000/244675 (5%)]\tLoss: 1.497714\n",
      "Train Epoch: 2 [13000/244675 (5%)]\tLoss: 1.570198\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 2 [14000/244675 (6%)]\tLoss: 1.620485\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 2 [15000/244675 (6%)]\tLoss: 1.385747\n",
      "Train Epoch: 2 [16000/244675 (7%)]\tLoss: 1.347690\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 2 [17000/244675 (7%)]\tLoss: 1.595949\n",
      "Train Epoch: 2 [18000/244675 (7%)]\tLoss: 1.421678\n",
      "Train Epoch: 2 [19000/244675 (8%)]\tLoss: 1.213068\n",
      "Train Epoch: 2 [20000/244675 (8%)]\tLoss: 1.546253\n",
      "Train Epoch: 2 [21000/244675 (9%)]\tLoss: 1.386381\n",
      "Train Epoch: 2 [22000/244675 (9%)]\tLoss: 1.387800\n",
      "Train Epoch: 2 [23000/244675 (9%)]\tLoss: 1.460084\n",
      "Train Epoch: 2 [24000/244675 (10%)]\tLoss: 1.440172\n",
      "Train Epoch: 2 [25000/244675 (10%)]\tLoss: 1.479973\n",
      "Train Epoch: 2 [26000/244675 (11%)]\tLoss: 1.348963\n",
      "Train Epoch: 2 [27000/244675 (11%)]\tLoss: 1.383030\n",
      "Train Epoch: 2 [28000/244675 (11%)]\tLoss: 1.400824\n",
      "Train Epoch: 2 [29000/244675 (12%)]\tLoss: 1.622250\n",
      "Train Epoch: 2 [30000/244675 (12%)]\tLoss: 1.362298\n",
      "Train Epoch: 2 [31000/244675 (13%)]\tLoss: 1.584545\n",
      "Train Epoch: 2 [32000/244675 (13%)]\tLoss: 1.444667\n",
      "Train Epoch: 2 [33000/244675 (13%)]\tLoss: 1.536114\n",
      "Train Epoch: 2 [34000/244675 (14%)]\tLoss: 1.644060\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 2 [35000/244675 (14%)]\tLoss: 1.336541\n",
      "Train Epoch: 2 [36000/244675 (15%)]\tLoss: 1.449341\n",
      "Train Epoch: 2 [37000/244675 (15%)]\tLoss: 1.653570\n",
      "Train Epoch: 2 [38000/244675 (16%)]\tLoss: 1.423682\n",
      "Train Epoch: 2 [39000/244675 (16%)]\tLoss: 1.648381\n",
      "Train Epoch: 2 [40000/244675 (16%)]\tLoss: 1.486150\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 2 [41000/244675 (17%)]\tLoss: 1.517316\n",
      "Train Epoch: 2 [42000/244675 (17%)]\tLoss: 1.420737\n",
      "Train Epoch: 2 [43000/244675 (18%)]\tLoss: 1.367508\n",
      "Train Epoch: 2 [44000/244675 (18%)]\tLoss: 1.535391\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 2 [45000/244675 (18%)]\tLoss: 1.312596\n",
      "Train Epoch: 2 [46000/244675 (19%)]\tLoss: 1.553876\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 2 [47000/244675 (19%)]\tLoss: 1.512787\n",
      "Train Epoch: 2 [48000/244675 (20%)]\tLoss: 1.383061\n",
      "Train Epoch: 2 [49000/244675 (20%)]\tLoss: 1.161327\n",
      "Train Epoch: 2 [50000/244675 (20%)]\tLoss: 1.232528\n",
      "Train Epoch: 2 [51000/244675 (21%)]\tLoss: 1.414714\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 2 [52000/244675 (21%)]\tLoss: 1.414210\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 2 [53000/244675 (22%)]\tLoss: 1.549711\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 2 [54000/244675 (22%)]\tLoss: 1.527953\n",
      "Train Epoch: 2 [55000/244675 (22%)]\tLoss: 1.393708\n",
      "Train Epoch: 2 [56000/244675 (23%)]\tLoss: 1.438277\n",
      "Train Epoch: 2 [57000/244675 (23%)]\tLoss: 1.687111\n",
      "Train Epoch: 2 [58000/244675 (24%)]\tLoss: 1.657554\n",
      "Train Epoch: 2 [59000/244675 (24%)]\tLoss: 1.629489\n",
      "Train Epoch: 2 [60000/244675 (25%)]\tLoss: 1.709175\n",
      "Train Epoch: 2 [61000/244675 (25%)]\tLoss: 1.346640\n",
      "Train Epoch: 2 [62000/244675 (25%)]\tLoss: 1.573625\n",
      "Train Epoch: 2 [63000/244675 (26%)]\tLoss: 1.234740\n",
      "Train Epoch: 2 [64000/244675 (26%)]\tLoss: 1.391984\n",
      "Train Epoch: 2 [65000/244675 (27%)]\tLoss: 1.472104\n",
      "Train Epoch: 2 [66000/244675 (27%)]\tLoss: 1.278598\n",
      "Train Epoch: 2 [67000/244675 (27%)]\tLoss: 1.219607\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 2 [68000/244675 (28%)]\tLoss: 1.480851\n",
      "Train Epoch: 2 [69000/244675 (28%)]\tLoss: 1.193830\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 2 [70000/244675 (29%)]\tLoss: 1.468134\n",
      "Train Epoch: 2 [71000/244675 (29%)]\tLoss: 1.305653\n",
      "Train Epoch: 2 [72000/244675 (29%)]\tLoss: 1.345321\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 2 [73000/244675 (30%)]\tLoss: 1.397290\n",
      "Train Epoch: 2 [74000/244675 (30%)]\tLoss: 1.210556\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 2 [75000/244675 (31%)]\tLoss: 1.666336\n",
      "Train Epoch: 2 [76000/244675 (31%)]\tLoss: 1.294590\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 2 [77000/244675 (31%)]\tLoss: 1.411859\n",
      "Train Epoch: 2 [78000/244675 (32%)]\tLoss: 1.597253\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 2 [79000/244675 (32%)]\tLoss: 1.116163\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 2 [80000/244675 (33%)]\tLoss: 1.425229\n",
      "Train Epoch: 2 [81000/244675 (33%)]\tLoss: 1.456314\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 2 [82000/244675 (34%)]\tLoss: 1.293424\n",
      "Train Epoch: 2 [83000/244675 (34%)]\tLoss: 1.546514\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 2 [84000/244675 (34%)]\tLoss: 1.694699\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 2 [85000/244675 (35%)]\tLoss: 1.639446\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 2 [86000/244675 (35%)]\tLoss: 1.101024\n",
      "Train Epoch: 2 [87000/244675 (36%)]\tLoss: 1.601735\n",
      "Train Epoch: 2 [88000/244675 (36%)]\tLoss: 1.488856\n",
      "Train Epoch: 2 [89000/244675 (36%)]\tLoss: 1.499887\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 2 [90000/244675 (37%)]\tLoss: 1.338661\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 2 [91000/244675 (37%)]\tLoss: 1.623255\n",
      "Train Epoch: 2 [92000/244675 (38%)]\tLoss: 1.507517\n",
      "Train Epoch: 2 [93000/244675 (38%)]\tLoss: 1.363702\n",
      "Train Epoch: 2 [94000/244675 (38%)]\tLoss: 1.591218\n",
      "Train Epoch: 2 [95000/244675 (39%)]\tLoss: 1.444620\n",
      "Train Epoch: 2 [96000/244675 (39%)]\tLoss: 1.349950\n",
      "Train Epoch: 2 [97000/244675 (40%)]\tLoss: 1.436654\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 2 [98000/244675 (40%)]\tLoss: 1.372558\n",
      "Train Epoch: 2 [99000/244675 (40%)]\tLoss: 1.173955\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 2 [100000/244675 (41%)]\tLoss: 1.304398\n",
      "Train Epoch: 2 [101000/244675 (41%)]\tLoss: 1.379627\n",
      "Train Epoch: 2 [102000/244675 (42%)]\tLoss: 1.309751\n",
      "Train Epoch: 2 [103000/244675 (42%)]\tLoss: 1.411795\n",
      "Train Epoch: 2 [104000/244675 (43%)]\tLoss: 1.105611\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 2 [105000/244675 (43%)]\tLoss: 1.203808\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 2 [106000/244675 (43%)]\tLoss: 1.319846\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 2 [107000/244675 (44%)]\tLoss: 1.705137\n",
      "Train Epoch: 2 [108000/244675 (44%)]\tLoss: 1.222301\n",
      "Train Epoch: 2 [109000/244675 (45%)]\tLoss: 1.180997\n",
      "Train Epoch: 2 [110000/244675 (45%)]\tLoss: 1.340141\n",
      "Train Epoch: 2 [111000/244675 (45%)]\tLoss: 1.440126\n",
      "Train Epoch: 2 [112000/244675 (46%)]\tLoss: 1.329255\n",
      "Train Epoch: 2 [113000/244675 (46%)]\tLoss: 1.530958\n",
      "Train Epoch: 2 [114000/244675 (47%)]\tLoss: 1.348621\n",
      "Train Epoch: 2 [115000/244675 (47%)]\tLoss: 1.443885\n",
      "Train Epoch: 2 [116000/244675 (47%)]\tLoss: 1.537448\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 2 [117000/244675 (48%)]\tLoss: 1.396563\n",
      "Train Epoch: 2 [118000/244675 (48%)]\tLoss: 1.329017\n",
      "Train Epoch: 2 [119000/244675 (49%)]\tLoss: 1.465621\n",
      "Train Epoch: 2 [120000/244675 (49%)]\tLoss: 1.726922\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 2 [121000/244675 (49%)]\tLoss: 1.374954\n",
      "Train Epoch: 2 [122000/244675 (50%)]\tLoss: 1.310634\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 2 [123000/244675 (50%)]\tLoss: 1.648216\n",
      "Train Epoch: 2 [124000/244675 (51%)]\tLoss: 1.368242\n",
      "Train Epoch: 2 [125000/244675 (51%)]\tLoss: 1.379242\n",
      "Train Epoch: 2 [126000/244675 (51%)]\tLoss: 1.373932\n",
      "Train Epoch: 2 [127000/244675 (52%)]\tLoss: 1.312574\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 2 [128000/244675 (52%)]\tLoss: 1.336939\n",
      "Train Epoch: 2 [129000/244675 (53%)]\tLoss: 1.537948\n",
      "Train Epoch: 2 [130000/244675 (53%)]\tLoss: 1.559012\n",
      "Train Epoch: 2 [131000/244675 (54%)]\tLoss: 1.252715\n",
      "Train Epoch: 2 [132000/244675 (54%)]\tLoss: 1.332789\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 2 [133000/244675 (54%)]\tLoss: 1.421304\n",
      "Train Epoch: 2 [134000/244675 (55%)]\tLoss: 1.198596\n",
      "Train Epoch: 2 [135000/244675 (55%)]\tLoss: 1.441373\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 2 [136000/244675 (56%)]\tLoss: 1.576612\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 2 [137000/244675 (56%)]\tLoss: 1.301432\n",
      "Train Epoch: 2 [138000/244675 (56%)]\tLoss: 1.161237\n",
      "Train Epoch: 2 [139000/244675 (57%)]\tLoss: 1.436510\n",
      "Train Epoch: 2 [140000/244675 (57%)]\tLoss: 0.966910\n",
      "Train Epoch: 2 [141000/244675 (58%)]\tLoss: 1.104941\n",
      "Train Epoch: 2 [142000/244675 (58%)]\tLoss: 1.266095\n",
      "Train Epoch: 2 [143000/244675 (58%)]\tLoss: 1.439826\n",
      "Train Epoch: 2 [144000/244675 (59%)]\tLoss: 1.135351\n",
      "Train Epoch: 2 [145000/244675 (59%)]\tLoss: 1.368332\n",
      "Train Epoch: 2 [146000/244675 (60%)]\tLoss: 1.433782\n",
      "Train Epoch: 2 [147000/244675 (60%)]\tLoss: 1.374053\n",
      "Train Epoch: 2 [148000/244675 (60%)]\tLoss: 1.309805\n",
      "Train Epoch: 2 [149000/244675 (61%)]\tLoss: 1.502794\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 2 [150000/244675 (61%)]\tLoss: 1.458844\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 2 [151000/244675 (62%)]\tLoss: 1.651670\n",
      "Train Epoch: 2 [152000/244675 (62%)]\tLoss: 1.259219\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 2 [153000/244675 (63%)]\tLoss: 1.191252\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 2 [154000/244675 (63%)]\tLoss: 1.446578\n",
      "Train Epoch: 2 [155000/244675 (63%)]\tLoss: 1.666660\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 2 [156000/244675 (64%)]\tLoss: 1.421715\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 2 [157000/244675 (64%)]\tLoss: 1.416390\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 2 [158000/244675 (65%)]\tLoss: 1.333422\n",
      "Train Epoch: 2 [159000/244675 (65%)]\tLoss: 1.332913\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 2 [160000/244675 (65%)]\tLoss: 1.246490\n",
      "Train Epoch: 2 [161000/244675 (66%)]\tLoss: 1.443133\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 2 [162000/244675 (66%)]\tLoss: 1.238512\n",
      "Train Epoch: 2 [163000/244675 (67%)]\tLoss: 1.324014\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 2 [164000/244675 (67%)]\tLoss: 1.270389\n",
      "Train Epoch: 2 [165000/244675 (67%)]\tLoss: 1.375716\n",
      "Train Epoch: 2 [166000/244675 (68%)]\tLoss: 1.447998\n",
      "Train Epoch: 2 [167000/244675 (68%)]\tLoss: 1.325505\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 2 [168000/244675 (69%)]\tLoss: 1.273640\n",
      "Train Epoch: 2 [169000/244675 (69%)]\tLoss: 1.572595\n",
      "Train Epoch: 2 [170000/244675 (69%)]\tLoss: 1.433862\n",
      "Train Epoch: 2 [171000/244675 (70%)]\tLoss: 1.152396\n",
      "Train Epoch: 2 [172000/244675 (70%)]\tLoss: 1.352792\n",
      "Train Epoch: 2 [173000/244675 (71%)]\tLoss: 1.578872\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 2 [174000/244675 (71%)]\tLoss: 1.226182\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 2 [175000/244675 (72%)]\tLoss: 1.525229\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 2 [176000/244675 (72%)]\tLoss: 1.563282\n",
      "Train Epoch: 2 [177000/244675 (72%)]\tLoss: 1.339494\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 2 [178000/244675 (73%)]\tLoss: 1.354606\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 2 [179000/244675 (73%)]\tLoss: 1.463705\n",
      "Train Epoch: 2 [180000/244675 (74%)]\tLoss: 1.361085\n",
      "Train Epoch: 2 [181000/244675 (74%)]\tLoss: 1.370786\n",
      "Train Epoch: 2 [182000/244675 (74%)]\tLoss: 1.250625\n",
      "Train Epoch: 2 [183000/244675 (75%)]\tLoss: 1.490418\n",
      "Train Epoch: 2 [184000/244675 (75%)]\tLoss: 1.323087\n",
      "Train Epoch: 2 [185000/244675 (76%)]\tLoss: 1.387106\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 2 [186000/244675 (76%)]\tLoss: 1.047734\n",
      "Train Epoch: 2 [187000/244675 (76%)]\tLoss: 1.151896\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 2 [188000/244675 (77%)]\tLoss: 1.276120\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 2 [189000/244675 (77%)]\tLoss: 1.364198\n",
      "Train Epoch: 2 [190000/244675 (78%)]\tLoss: 1.495165\n",
      "Train Epoch: 2 [191000/244675 (78%)]\tLoss: 1.504907\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 2 [192000/244675 (78%)]\tLoss: 1.352263\n",
      "Train Epoch: 2 [193000/244675 (79%)]\tLoss: 1.322143\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 2 [194000/244675 (79%)]\tLoss: 1.685197\n",
      "Train Epoch: 2 [195000/244675 (80%)]\tLoss: 1.295656\n",
      "Train Epoch: 2 [196000/244675 (80%)]\tLoss: 1.416959\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 2 [197000/244675 (81%)]\tLoss: 1.120609\n",
      "Train Epoch: 2 [198000/244675 (81%)]\tLoss: 1.404741\n",
      "Train Epoch: 2 [199000/244675 (81%)]\tLoss: 1.067000\n",
      "Train Epoch: 2 [200000/244675 (82%)]\tLoss: 1.229051\n",
      "Train Epoch: 2 [201000/244675 (82%)]\tLoss: 1.258767\n",
      "Train Epoch: 2 [202000/244675 (83%)]\tLoss: 1.433696\n",
      "Train Epoch: 2 [203000/244675 (83%)]\tLoss: 1.528749\n",
      "Train Epoch: 2 [204000/244675 (83%)]\tLoss: 1.255697\n",
      "Train Epoch: 2 [205000/244675 (84%)]\tLoss: 1.460567\n",
      "Train Epoch: 2 [206000/244675 (84%)]\tLoss: 1.288337\n",
      "Train Epoch: 2 [207000/244675 (85%)]\tLoss: 1.393891\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 2 [208000/244675 (85%)]\tLoss: 1.071120\n",
      "Train Epoch: 2 [209000/244675 (85%)]\tLoss: 1.277806\n",
      "Train Epoch: 2 [210000/244675 (86%)]\tLoss: 1.372728\n",
      "Train Epoch: 2 [211000/244675 (86%)]\tLoss: 1.357530\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 2 [212000/244675 (87%)]\tLoss: 1.374441\n",
      "Train Epoch: 2 [213000/244675 (87%)]\tLoss: 1.485783\n",
      "Train Epoch: 2 [214000/244675 (87%)]\tLoss: 1.673368\n",
      "Train Epoch: 2 [215000/244675 (88%)]\tLoss: 1.150006\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 2 [216000/244675 (88%)]\tLoss: 1.473109\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 2 [217000/244675 (89%)]\tLoss: 1.434500\n",
      "Train Epoch: 2 [218000/244675 (89%)]\tLoss: 1.544084\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 2 [219000/244675 (90%)]\tLoss: 1.519286\n",
      "Train Epoch: 2 [220000/244675 (90%)]\tLoss: 1.343191\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 2 [221000/244675 (90%)]\tLoss: 1.344082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 2 [222000/244675 (91%)]\tLoss: 1.472867\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 2 [223000/244675 (91%)]\tLoss: 1.509995\n",
      "Train Epoch: 2 [224000/244675 (92%)]\tLoss: 1.361209\n",
      "Train Epoch: 2 [225000/244675 (92%)]\tLoss: 1.420477\n",
      "Train Epoch: 2 [226000/244675 (92%)]\tLoss: 1.548709\n",
      "Train Epoch: 2 [227000/244675 (93%)]\tLoss: 1.408556\n",
      "Train Epoch: 2 [228000/244675 (93%)]\tLoss: 1.487811\n",
      "Train Epoch: 2 [229000/244675 (94%)]\tLoss: 1.435914\n",
      "Train Epoch: 2 [230000/244675 (94%)]\tLoss: 1.241284\n",
      "Train Epoch: 2 [231000/244675 (94%)]\tLoss: 1.162477\n",
      "Train Epoch: 2 [232000/244675 (95%)]\tLoss: 1.319983\n",
      "Train Epoch: 2 [233000/244675 (95%)]\tLoss: 1.314661\n",
      "Train Epoch: 2 [234000/244675 (96%)]\tLoss: 1.322094\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 2 [235000/244675 (96%)]\tLoss: 1.468782\n",
      "Train Epoch: 2 [236000/244675 (96%)]\tLoss: 1.410905\n",
      "Train Epoch: 2 [237000/244675 (97%)]\tLoss: 1.356396\n",
      "Train Epoch: 2 [238000/244675 (97%)]\tLoss: 1.095714\n",
      "Train Epoch: 2 [239000/244675 (98%)]\tLoss: 1.234701\n",
      "Train Epoch: 2 [240000/244675 (98%)]\tLoss: 1.302912\n",
      "Train Epoch: 2 [241000/244675 (98%)]\tLoss: 1.163685\n",
      "Train Epoch: 2 [242000/244675 (99%)]\tLoss: 1.339684\n",
      "Train Epoch: 2 [243000/244675 (99%)]\tLoss: 1.207134\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 2 [244000/244675 (100%)]\tLoss: 1.323649\n",
      "['ate mong eterfrated hargon i seted lite as fater a greme', 'iwa bi and bos es se vose plyso', 'a stites spray is poupoinaf to word of bars in o ther potintial atackers', 'ward in dockets mary aledo da ear', 'as wae snopo was piin of thepedite bay che be of comers', 'hed evout raatyn trest en flo icun baor inextren sutations', 'a pat marerfes es sonia ac ces tor backa', 'the provents is rich in maneral teposits anclegod in coper', 'a corinto soser the jegreffecxtedianlanguages deals with bextur na inturna myngristis', 'its underserfis is concae fom thefor backord and con bex fomsied to sad '] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.3081, Average CER: 0.418165 Average WER: 0.8387\n",
      "\n",
      "Train Epoch: 3 [0/244675 (0%)]\tLoss: 1.352964\n",
      "Train Epoch: 3 [1000/244675 (0%)]\tLoss: 1.436984\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Train Epoch: 3 [2000/244675 (1%)]\tLoss: 1.513710\n",
      "Train Epoch: 3 [3000/244675 (1%)]\tLoss: 1.154664\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 3 [4000/244675 (2%)]\tLoss: 1.168119\n",
      "Train Epoch: 3 [5000/244675 (2%)]\tLoss: 1.384858\n",
      "Train Epoch: 3 [6000/244675 (2%)]\tLoss: 1.235318\n",
      "Train Epoch: 3 [7000/244675 (3%)]\tLoss: 1.486977\n",
      "Train Epoch: 3 [8000/244675 (3%)]\tLoss: 1.465814\n",
      "Train Epoch: 3 [9000/244675 (4%)]\tLoss: 1.395762\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 3 [10000/244675 (4%)]\tLoss: 1.212435\n",
      "Train Epoch: 3 [11000/244675 (4%)]\tLoss: 1.535366\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 3 [12000/244675 (5%)]\tLoss: 1.121361\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 3 [13000/244675 (5%)]\tLoss: 1.415397\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 3 [14000/244675 (6%)]\tLoss: 1.451452\n",
      "Train Epoch: 3 [15000/244675 (6%)]\tLoss: 1.221987\n",
      "Train Epoch: 3 [16000/244675 (7%)]\tLoss: 1.111270\n",
      "Train Epoch: 3 [17000/244675 (7%)]\tLoss: 1.435665\n",
      "Train Epoch: 3 [18000/244675 (7%)]\tLoss: 1.546556\n",
      "Train Epoch: 3 [19000/244675 (8%)]\tLoss: 1.303124\n",
      "Train Epoch: 3 [20000/244675 (8%)]\tLoss: 1.378707\n",
      "Train Epoch: 3 [21000/244675 (9%)]\tLoss: 1.336705\n",
      "Train Epoch: 3 [22000/244675 (9%)]\tLoss: 1.567946\n",
      "Train Epoch: 3 [23000/244675 (9%)]\tLoss: 1.287803\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 3 [24000/244675 (10%)]\tLoss: 1.451298\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 3 [25000/244675 (10%)]\tLoss: 1.480446\n",
      "Train Epoch: 3 [26000/244675 (11%)]\tLoss: 1.190611\n",
      "Train Epoch: 3 [27000/244675 (11%)]\tLoss: 1.195025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 3 [28000/244675 (11%)]\tLoss: 1.413314\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 3 [29000/244675 (12%)]\tLoss: 1.338869\n",
      "Train Epoch: 3 [30000/244675 (12%)]\tLoss: 1.209032\n",
      "Train Epoch: 3 [31000/244675 (13%)]\tLoss: 1.129322\n",
      "Train Epoch: 3 [32000/244675 (13%)]\tLoss: 1.277353\n",
      "Train Epoch: 3 [33000/244675 (13%)]\tLoss: 1.089261\n",
      "Train Epoch: 3 [34000/244675 (14%)]\tLoss: 1.161961\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 3 [35000/244675 (14%)]\tLoss: 1.367554\n",
      "Train Epoch: 3 [36000/244675 (15%)]\tLoss: 1.310266\n",
      "Train Epoch: 3 [37000/244675 (15%)]\tLoss: 1.388513\n",
      "Train Epoch: 3 [38000/244675 (16%)]\tLoss: 1.218512\n",
      "Train Epoch: 3 [39000/244675 (16%)]\tLoss: 1.367208\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 3 [40000/244675 (16%)]\tLoss: 1.259024\n",
      "Train Epoch: 3 [41000/244675 (17%)]\tLoss: 1.503939\n",
      "Train Epoch: 3 [42000/244675 (17%)]\tLoss: 1.333334\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 3 [43000/244675 (18%)]\tLoss: 1.194715\n",
      "Train Epoch: 3 [44000/244675 (18%)]\tLoss: 1.338641\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 3 [45000/244675 (18%)]\tLoss: 1.305515\n",
      "Train Epoch: 3 [46000/244675 (19%)]\tLoss: 1.351349\n",
      "Train Epoch: 3 [47000/244675 (19%)]\tLoss: 1.291496\n",
      "Train Epoch: 3 [48000/244675 (20%)]\tLoss: 1.258354\n",
      "Train Epoch: 3 [49000/244675 (20%)]\tLoss: 1.199990\n",
      "Train Epoch: 3 [50000/244675 (20%)]\tLoss: 1.117479\n",
      "Train Epoch: 3 [51000/244675 (21%)]\tLoss: 1.099985\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 3 [52000/244675 (21%)]\tLoss: 1.262264\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 3 [53000/244675 (22%)]\tLoss: 1.484174\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 3 [54000/244675 (22%)]\tLoss: 1.422881\n",
      "Train Epoch: 3 [55000/244675 (22%)]\tLoss: 1.440694\n",
      "Train Epoch: 3 [56000/244675 (23%)]\tLoss: 1.599552\n",
      "Train Epoch: 3 [57000/244675 (23%)]\tLoss: 1.345805\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 3 [58000/244675 (24%)]\tLoss: 1.482830\n",
      "Train Epoch: 3 [59000/244675 (24%)]\tLoss: 1.226758\n",
      "Train Epoch: 3 [60000/244675 (25%)]\tLoss: 1.297747\n",
      "Train Epoch: 3 [61000/244675 (25%)]\tLoss: 1.350316\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 3 [62000/244675 (25%)]\tLoss: 1.175632\n",
      "Train Epoch: 3 [63000/244675 (26%)]\tLoss: 1.554282\n",
      "Train Epoch: 3 [64000/244675 (26%)]\tLoss: 1.453457\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 3 [65000/244675 (27%)]\tLoss: 1.188361\n",
      "Train Epoch: 3 [66000/244675 (27%)]\tLoss: 1.309914\n",
      "Train Epoch: 3 [67000/244675 (27%)]\tLoss: 1.339191\n",
      "Train Epoch: 3 [68000/244675 (28%)]\tLoss: 1.347135\n",
      "Train Epoch: 3 [69000/244675 (28%)]\tLoss: 1.392211\n",
      "Train Epoch: 3 [70000/244675 (29%)]\tLoss: 1.762896\n",
      "Train Epoch: 3 [71000/244675 (29%)]\tLoss: 1.204300\n",
      "Train Epoch: 3 [72000/244675 (29%)]\tLoss: 1.323402\n",
      "Train Epoch: 3 [73000/244675 (30%)]\tLoss: 1.433117\n",
      "Train Epoch: 3 [74000/244675 (30%)]\tLoss: 1.194671\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 3 [75000/244675 (31%)]\tLoss: 1.449955\n",
      "Train Epoch: 3 [76000/244675 (31%)]\tLoss: 1.190804\n",
      "Train Epoch: 3 [77000/244675 (31%)]\tLoss: 1.329639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 3 [78000/244675 (32%)]\tLoss: 1.397882\n",
      "Train Epoch: 3 [79000/244675 (32%)]\tLoss: 1.182475\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 3 [80000/244675 (33%)]\tLoss: 1.346742\n",
      "Train Epoch: 3 [81000/244675 (33%)]\tLoss: 1.244717\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 3 [82000/244675 (34%)]\tLoss: 1.319781\n",
      "Train Epoch: 3 [83000/244675 (34%)]\tLoss: 1.373578\n",
      "Train Epoch: 3 [84000/244675 (34%)]\tLoss: 0.896380\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 3 [85000/244675 (35%)]\tLoss: 1.138707\n",
      "Train Epoch: 3 [86000/244675 (35%)]\tLoss: 1.622529\n",
      "Train Epoch: 3 [87000/244675 (36%)]\tLoss: 1.264110\n",
      "Train Epoch: 3 [88000/244675 (36%)]\tLoss: 1.314720\n",
      "Train Epoch: 3 [89000/244675 (36%)]\tLoss: 1.244313\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 3 [90000/244675 (37%)]\tLoss: 1.610848\n",
      "Train Epoch: 3 [91000/244675 (37%)]\tLoss: 1.104391\n",
      "Train Epoch: 3 [92000/244675 (38%)]\tLoss: 1.410606\n",
      "Train Epoch: 3 [93000/244675 (38%)]\tLoss: 1.067428\n",
      "Train Epoch: 3 [94000/244675 (38%)]\tLoss: 1.462439\n",
      "Train Epoch: 3 [95000/244675 (39%)]\tLoss: 1.279681\n",
      "Train Epoch: 3 [96000/244675 (39%)]\tLoss: 1.109756\n",
      "Train Epoch: 3 [97000/244675 (40%)]\tLoss: 1.085807\n",
      "Train Epoch: 3 [98000/244675 (40%)]\tLoss: 1.335578\n",
      "Train Epoch: 3 [99000/244675 (40%)]\tLoss: 1.414466\n",
      "Train Epoch: 3 [100000/244675 (41%)]\tLoss: 1.258261\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 3 [101000/244675 (41%)]\tLoss: 1.144945\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 3 [102000/244675 (42%)]\tLoss: 1.109651\n",
      "Train Epoch: 3 [103000/244675 (42%)]\tLoss: 1.034452\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 3 [104000/244675 (43%)]\tLoss: 1.269213\n",
      "Train Epoch: 3 [105000/244675 (43%)]\tLoss: 1.388126\n",
      "Train Epoch: 3 [106000/244675 (43%)]\tLoss: 1.319301\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 3 [107000/244675 (44%)]\tLoss: 1.424192\n",
      "Train Epoch: 3 [108000/244675 (44%)]\tLoss: 1.399211\n",
      "Train Epoch: 3 [109000/244675 (45%)]\tLoss: 1.525422\n",
      "Train Epoch: 3 [110000/244675 (45%)]\tLoss: 1.365444\n",
      "Train Epoch: 3 [111000/244675 (45%)]\tLoss: 1.307775\n",
      "Train Epoch: 3 [112000/244675 (46%)]\tLoss: 1.068897\n",
      "Train Epoch: 3 [113000/244675 (46%)]\tLoss: 1.165577\n",
      "Train Epoch: 3 [114000/244675 (47%)]\tLoss: 1.339959\n",
      "Train Epoch: 3 [115000/244675 (47%)]\tLoss: 1.287218\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 3 [116000/244675 (47%)]\tLoss: 1.388439\n",
      "Train Epoch: 3 [117000/244675 (48%)]\tLoss: 1.341784\n",
      "Train Epoch: 3 [118000/244675 (48%)]\tLoss: 1.088216\n",
      "Train Epoch: 3 [119000/244675 (49%)]\tLoss: 1.200942\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 3 [120000/244675 (49%)]\tLoss: 1.115608\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 3 [121000/244675 (49%)]\tLoss: 1.222908\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 3 [122000/244675 (50%)]\tLoss: 1.407745\n",
      "Train Epoch: 3 [123000/244675 (50%)]\tLoss: 1.265219\n",
      "Train Epoch: 3 [124000/244675 (51%)]\tLoss: 1.431831\n",
      "Train Epoch: 3 [125000/244675 (51%)]\tLoss: 1.236702\n",
      "Train Epoch: 3 [126000/244675 (51%)]\tLoss: 1.422946\n",
      "Train Epoch: 3 [127000/244675 (52%)]\tLoss: 1.431889\n",
      "Train Epoch: 3 [128000/244675 (52%)]\tLoss: 1.392369\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 3 [129000/244675 (53%)]\tLoss: 1.186167\n",
      "Train Epoch: 3 [130000/244675 (53%)]\tLoss: 1.301482\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 3 [131000/244675 (54%)]\tLoss: 1.271784\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 3 [132000/244675 (54%)]\tLoss: 1.185589\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 3 [133000/244675 (54%)]\tLoss: 1.404928\n",
      "Train Epoch: 3 [134000/244675 (55%)]\tLoss: 1.312561\n",
      "Train Epoch: 3 [135000/244675 (55%)]\tLoss: 1.048965\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 3 [136000/244675 (56%)]\tLoss: 1.269599\n",
      "Train Epoch: 3 [137000/244675 (56%)]\tLoss: 1.184753\n",
      "Train Epoch: 3 [138000/244675 (56%)]\tLoss: 1.632345\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 3 [139000/244675 (57%)]\tLoss: 1.274064\n",
      "Train Epoch: 3 [140000/244675 (57%)]\tLoss: 0.997441\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 3 [141000/244675 (58%)]\tLoss: 1.422662\n",
      "Train Epoch: 3 [142000/244675 (58%)]\tLoss: 1.050815\n",
      "Train Epoch: 3 [143000/244675 (58%)]\tLoss: 1.489213\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 3 [144000/244675 (59%)]\tLoss: 1.184304\n",
      "Train Epoch: 3 [145000/244675 (59%)]\tLoss: 1.127784\n",
      "Train Epoch: 3 [146000/244675 (60%)]\tLoss: 1.239944\n",
      "Train Epoch: 3 [147000/244675 (60%)]\tLoss: 1.446667\n",
      "Train Epoch: 3 [148000/244675 (60%)]\tLoss: 1.433974\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 3 [149000/244675 (61%)]\tLoss: 1.243245\n",
      "Train Epoch: 3 [150000/244675 (61%)]\tLoss: 1.145040\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 3 [151000/244675 (62%)]\tLoss: 1.362720\n",
      "Train Epoch: 3 [152000/244675 (62%)]\tLoss: 1.383552\n",
      "Train Epoch: 3 [153000/244675 (63%)]\tLoss: 1.314737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 3 [154000/244675 (63%)]\tLoss: 1.365643\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 3 [155000/244675 (63%)]\tLoss: 1.420711\n",
      "Train Epoch: 3 [156000/244675 (64%)]\tLoss: 1.184459\n",
      "Train Epoch: 3 [157000/244675 (64%)]\tLoss: 1.491500\n",
      "Train Epoch: 3 [158000/244675 (65%)]\tLoss: 1.244240\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 3 [159000/244675 (65%)]\tLoss: 1.115989\n",
      "Train Epoch: 3 [160000/244675 (65%)]\tLoss: 1.407615\n",
      "Train Epoch: 3 [161000/244675 (66%)]\tLoss: 1.390616\n",
      "Train Epoch: 3 [162000/244675 (66%)]\tLoss: 1.379665\n",
      "Train Epoch: 3 [163000/244675 (67%)]\tLoss: 1.252069\n",
      "Train Epoch: 3 [164000/244675 (67%)]\tLoss: 1.289800\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 3 [165000/244675 (67%)]\tLoss: 1.178807\n",
      "Train Epoch: 3 [166000/244675 (68%)]\tLoss: 1.257788\n",
      "Train Epoch: 3 [167000/244675 (68%)]\tLoss: 1.330269\n",
      "Train Epoch: 3 [168000/244675 (69%)]\tLoss: 1.315574\n",
      "Train Epoch: 3 [169000/244675 (69%)]\tLoss: 0.985311\n",
      "Train Epoch: 3 [170000/244675 (69%)]\tLoss: 1.315424\n",
      "Train Epoch: 3 [171000/244675 (70%)]\tLoss: 1.389661\n",
      "Train Epoch: 3 [172000/244675 (70%)]\tLoss: 1.070504\n",
      "Train Epoch: 3 [173000/244675 (71%)]\tLoss: 1.246000\n",
      "Train Epoch: 3 [174000/244675 (71%)]\tLoss: 1.135064\n",
      "Train Epoch: 3 [175000/244675 (72%)]\tLoss: 1.412705\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 3 [176000/244675 (72%)]\tLoss: 1.164771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 3 [177000/244675 (72%)]\tLoss: 1.035716\n",
      "Train Epoch: 3 [178000/244675 (73%)]\tLoss: 1.465768\n",
      "Train Epoch: 3 [179000/244675 (73%)]\tLoss: 1.067343\n",
      "Train Epoch: 3 [180000/244675 (74%)]\tLoss: 1.203708\n",
      "Train Epoch: 3 [181000/244675 (74%)]\tLoss: 1.148418\n",
      "Train Epoch: 3 [182000/244675 (74%)]\tLoss: 1.217146\n",
      "Train Epoch: 3 [183000/244675 (75%)]\tLoss: 1.238729\n",
      "Train Epoch: 3 [184000/244675 (75%)]\tLoss: 1.497949\n",
      "Train Epoch: 3 [185000/244675 (76%)]\tLoss: 1.238327\n",
      "Train Epoch: 3 [186000/244675 (76%)]\tLoss: 1.483611\n",
      "Train Epoch: 3 [187000/244675 (76%)]\tLoss: 1.255429\n",
      "Train Epoch: 3 [188000/244675 (77%)]\tLoss: 1.366821\n",
      "Train Epoch: 3 [189000/244675 (77%)]\tLoss: 1.179087\n",
      "Train Epoch: 3 [190000/244675 (78%)]\tLoss: 1.297684\n",
      "Train Epoch: 3 [191000/244675 (78%)]\tLoss: 1.425709\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 3 [192000/244675 (78%)]\tLoss: 1.264752\n",
      "Train Epoch: 3 [193000/244675 (79%)]\tLoss: 1.473248\n",
      "Train Epoch: 3 [194000/244675 (79%)]\tLoss: 1.205936\n",
      "Train Epoch: 3 [195000/244675 (80%)]\tLoss: 1.065138\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 3 [196000/244675 (80%)]\tLoss: 0.900777\n",
      "Train Epoch: 3 [197000/244675 (81%)]\tLoss: 1.161352\n",
      "Train Epoch: 3 [198000/244675 (81%)]\tLoss: 1.255912\n",
      "Train Epoch: 3 [199000/244675 (81%)]\tLoss: 1.088376\n",
      "Train Epoch: 3 [200000/244675 (82%)]\tLoss: 1.144881\n",
      "Train Epoch: 3 [201000/244675 (82%)]\tLoss: 1.077026\n",
      "Train Epoch: 3 [202000/244675 (83%)]\tLoss: 1.110414\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 3 [203000/244675 (83%)]\tLoss: 1.360950\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 3 [204000/244675 (83%)]\tLoss: 1.298716\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 3 [205000/244675 (84%)]\tLoss: 1.291802\n",
      "Train Epoch: 3 [206000/244675 (84%)]\tLoss: 1.191708\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 3 [207000/244675 (85%)]\tLoss: 1.148916\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 3 [208000/244675 (85%)]\tLoss: 1.315121\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 3 [209000/244675 (85%)]\tLoss: 1.162364\n",
      "Train Epoch: 3 [210000/244675 (86%)]\tLoss: 1.044801\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 3 [211000/244675 (86%)]\tLoss: 1.570548\n",
      "Train Epoch: 3 [212000/244675 (87%)]\tLoss: 1.301619\n",
      "Train Epoch: 3 [213000/244675 (87%)]\tLoss: 1.378945\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 3 [214000/244675 (87%)]\tLoss: 1.511752\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 3 [215000/244675 (88%)]\tLoss: 1.300686\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 3 [216000/244675 (88%)]\tLoss: 1.213756\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 3 [217000/244675 (89%)]\tLoss: 1.382973\n",
      "Train Epoch: 3 [218000/244675 (89%)]\tLoss: 1.297047\n",
      "Train Epoch: 3 [219000/244675 (90%)]\tLoss: 1.733078\n",
      "Train Epoch: 3 [220000/244675 (90%)]\tLoss: 1.293461\n",
      "Train Epoch: 3 [221000/244675 (90%)]\tLoss: 1.439979\n",
      "Train Epoch: 3 [222000/244675 (91%)]\tLoss: 1.061764\n",
      "Train Epoch: 3 [223000/244675 (91%)]\tLoss: 1.477525\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 3 [224000/244675 (92%)]\tLoss: 1.174927\n",
      "Train Epoch: 3 [225000/244675 (92%)]\tLoss: 1.344650\n",
      "Train Epoch: 3 [226000/244675 (92%)]\tLoss: 1.289188\n",
      "Train Epoch: 3 [227000/244675 (93%)]\tLoss: 1.046014\n",
      "Train Epoch: 3 [228000/244675 (93%)]\tLoss: 1.452672\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 3 [229000/244675 (94%)]\tLoss: 1.312539\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 3 [230000/244675 (94%)]\tLoss: 1.213413\n",
      "Train Epoch: 3 [231000/244675 (94%)]\tLoss: 1.364569\n",
      "Train Epoch: 3 [232000/244675 (95%)]\tLoss: 1.303766\n",
      "Train Epoch: 3 [233000/244675 (95%)]\tLoss: 1.316352\n",
      "Train Epoch: 3 [234000/244675 (96%)]\tLoss: 1.419246\n",
      "Train Epoch: 3 [235000/244675 (96%)]\tLoss: 1.543406\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 3 [236000/244675 (96%)]\tLoss: 1.263517\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 3 [237000/244675 (97%)]\tLoss: 1.330317\n",
      "Train Epoch: 3 [238000/244675 (97%)]\tLoss: 1.469776\n",
      "Train Epoch: 3 [239000/244675 (98%)]\tLoss: 1.539894\n",
      "Train Epoch: 3 [240000/244675 (98%)]\tLoss: 1.459199\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 3 [241000/244675 (98%)]\tLoss: 1.332339\n",
      "Train Epoch: 3 [242000/244675 (99%)]\tLoss: 1.416512\n",
      "Train Epoch: 3 [243000/244675 (99%)]\tLoss: 1.346546\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 3 [244000/244675 (100%)]\tLoss: 1.476825\n",
      "['oti mong e thergreted hegoen e seted lite as farter  gremet', 'i a not bulon wersyse vose l sowo', 'the scos spary is potein nofto wardof baers in othe potintral atacers', 'wor an dokens mary tlato dat yer', 'is moo se was pai of the proute dely ce wi of coms', 'he tevout eroli trestin flit ico peaer extren setrations', 'apal mard refhus i sonia acces ton backa', 'the proventes is rich in maneral deposits eclegood in coper', 'a coring to sose the jegrefec tedionlang mages deos wit thde tero nat in terno myngristes', 'its underserfise is concay from the for bacwerd andcon ves fromside tosid'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.2891, Average CER: 0.410923 Average WER: 0.8289\n",
      "\n",
      "Train Epoch: 4 [0/244675 (0%)]\tLoss: 1.476516\n",
      "Train Epoch: 4 [1000/244675 (0%)]\tLoss: 0.931030\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 4 [2000/244675 (1%)]\tLoss: 1.036030\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 4 [3000/244675 (1%)]\tLoss: 1.121431\n",
      "Train Epoch: 4 [4000/244675 (2%)]\tLoss: 1.344449\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 4 [5000/244675 (2%)]\tLoss: 1.117143\n",
      "Train Epoch: 4 [6000/244675 (2%)]\tLoss: 1.386991\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 4 [7000/244675 (3%)]\tLoss: 1.437601\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 4 [8000/244675 (3%)]\tLoss: 1.097082\n",
      "Train Epoch: 4 [9000/244675 (4%)]\tLoss: 1.286089\n",
      "Train Epoch: 4 [10000/244675 (4%)]\tLoss: 1.146809\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 4 [11000/244675 (4%)]\tLoss: 1.339901\n",
      "Train Epoch: 4 [12000/244675 (5%)]\tLoss: 1.175464\n",
      "Train Epoch: 4 [13000/244675 (5%)]\tLoss: 1.074453\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 4 [14000/244675 (6%)]\tLoss: 1.037739\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 4 [15000/244675 (6%)]\tLoss: 1.106656\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 4 [16000/244675 (7%)]\tLoss: 1.322626\n",
      "Train Epoch: 4 [17000/244675 (7%)]\tLoss: 1.306707\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 4 [18000/244675 (7%)]\tLoss: 1.324550\n",
      "Train Epoch: 4 [19000/244675 (8%)]\tLoss: 1.412627\n",
      "Train Epoch: 4 [20000/244675 (8%)]\tLoss: 1.191280\n",
      "Train Epoch: 4 [21000/244675 (9%)]\tLoss: 1.424134\n",
      "Train Epoch: 4 [22000/244675 (9%)]\tLoss: 1.251717\n",
      "Train Epoch: 4 [23000/244675 (9%)]\tLoss: 1.449718\n",
      "Train Epoch: 4 [24000/244675 (10%)]\tLoss: 1.006442\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 4 [25000/244675 (10%)]\tLoss: 1.371258\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 4 [26000/244675 (11%)]\tLoss: 1.025725\n",
      "Train Epoch: 4 [27000/244675 (11%)]\tLoss: 1.234023\n",
      "Train Epoch: 4 [28000/244675 (11%)]\tLoss: 1.353722\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 4 [29000/244675 (12%)]\tLoss: 1.119411\n",
      "Train Epoch: 4 [30000/244675 (12%)]\tLoss: 1.330852\n",
      "Train Epoch: 4 [31000/244675 (13%)]\tLoss: 1.144083\n",
      "Train Epoch: 4 [32000/244675 (13%)]\tLoss: 1.234267\n",
      "Train Epoch: 4 [33000/244675 (13%)]\tLoss: 1.387699\n",
      "Train Epoch: 4 [34000/244675 (14%)]\tLoss: 1.171131\n",
      "Train Epoch: 4 [35000/244675 (14%)]\tLoss: 1.232721\n",
      "Train Epoch: 4 [36000/244675 (15%)]\tLoss: 1.179587\n",
      "Train Epoch: 4 [37000/244675 (15%)]\tLoss: 1.160329\n",
      "Train Epoch: 4 [38000/244675 (16%)]\tLoss: 1.225558\n",
      "Train Epoch: 4 [39000/244675 (16%)]\tLoss: 1.364854\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 4 [40000/244675 (16%)]\tLoss: 1.164094\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 4 [41000/244675 (17%)]\tLoss: 1.740895\n",
      "Train Epoch: 4 [42000/244675 (17%)]\tLoss: 1.140646\n",
      "Train Epoch: 4 [43000/244675 (18%)]\tLoss: 1.248661\n",
      "Train Epoch: 4 [44000/244675 (18%)]\tLoss: 1.374235\n",
      "Train Epoch: 4 [45000/244675 (18%)]\tLoss: 1.216198\n",
      "Train Epoch: 4 [46000/244675 (19%)]\tLoss: 1.441737\n",
      "Train Epoch: 4 [47000/244675 (19%)]\tLoss: 1.477401\n",
      "Train Epoch: 4 [48000/244675 (20%)]\tLoss: 1.121527\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 4 [49000/244675 (20%)]\tLoss: 1.140820\n",
      "Train Epoch: 4 [50000/244675 (20%)]\tLoss: 1.354202\n",
      "Train Epoch: 4 [51000/244675 (21%)]\tLoss: 1.050185\n",
      "Train Epoch: 4 [52000/244675 (21%)]\tLoss: 1.279395\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 4 [53000/244675 (22%)]\tLoss: 1.513570\n",
      "Train Epoch: 4 [54000/244675 (22%)]\tLoss: 1.112499\n",
      "Train Epoch: 4 [55000/244675 (22%)]\tLoss: 1.281823\n",
      "Train Epoch: 4 [56000/244675 (23%)]\tLoss: 1.173799\n",
      "Train Epoch: 4 [57000/244675 (23%)]\tLoss: 1.451177\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 4 [58000/244675 (24%)]\tLoss: 1.184786\n",
      "Train Epoch: 4 [59000/244675 (24%)]\tLoss: 1.250001\n",
      "Train Epoch: 4 [60000/244675 (25%)]\tLoss: 1.304492\n",
      "Train Epoch: 4 [61000/244675 (25%)]\tLoss: 1.419852\n",
      "Train Epoch: 4 [62000/244675 (25%)]\tLoss: 1.263698\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 4 [63000/244675 (26%)]\tLoss: 1.231519\n",
      "Train Epoch: 4 [64000/244675 (26%)]\tLoss: 1.454496\n",
      "Train Epoch: 4 [65000/244675 (27%)]\tLoss: 1.047235\n",
      "Train Epoch: 4 [66000/244675 (27%)]\tLoss: 1.647130\n",
      "Train Epoch: 4 [67000/244675 (27%)]\tLoss: 1.200759\n",
      "Train Epoch: 4 [68000/244675 (28%)]\tLoss: 1.228920\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 4 [69000/244675 (28%)]\tLoss: 1.208771\n",
      "Train Epoch: 4 [70000/244675 (29%)]\tLoss: 1.502737\n",
      "Train Epoch: 4 [71000/244675 (29%)]\tLoss: 1.177840\n",
      "Train Epoch: 4 [72000/244675 (29%)]\tLoss: 1.055104\n",
      "Train Epoch: 4 [73000/244675 (30%)]\tLoss: 1.396363\n",
      "Train Epoch: 4 [74000/244675 (30%)]\tLoss: 0.934842\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 4 [75000/244675 (31%)]\tLoss: 1.174372\n",
      "Train Epoch: 4 [76000/244675 (31%)]\tLoss: 1.216636\n",
      "Train Epoch: 4 [77000/244675 (31%)]\tLoss: 1.442623\n",
      "Train Epoch: 4 [78000/244675 (32%)]\tLoss: 1.195021\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 4 [79000/244675 (32%)]\tLoss: 1.199153\n",
      "Train Epoch: 4 [80000/244675 (33%)]\tLoss: 1.344999\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 4 [81000/244675 (33%)]\tLoss: 1.425543\n",
      "Train Epoch: 4 [82000/244675 (34%)]\tLoss: 1.174908\n",
      "Train Epoch: 4 [83000/244675 (34%)]\tLoss: 1.298374\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 4 [84000/244675 (34%)]\tLoss: 1.169921\n",
      "Train Epoch: 4 [85000/244675 (35%)]\tLoss: 1.352017\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 4 [86000/244675 (35%)]\tLoss: 1.220565\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 4 [87000/244675 (36%)]\tLoss: 1.371222\n",
      "Train Epoch: 4 [88000/244675 (36%)]\tLoss: 1.433030\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 4 [89000/244675 (36%)]\tLoss: 1.055843\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 4 [90000/244675 (37%)]\tLoss: 1.188075\n",
      "Train Epoch: 4 [91000/244675 (37%)]\tLoss: 1.087462\n",
      "Train Epoch: 4 [92000/244675 (38%)]\tLoss: 1.046513\n",
      "Train Epoch: 4 [93000/244675 (38%)]\tLoss: 1.224356\n",
      "Train Epoch: 4 [94000/244675 (38%)]\tLoss: 1.361512\n",
      "Train Epoch: 4 [95000/244675 (39%)]\tLoss: 1.111188\n",
      "Train Epoch: 4 [96000/244675 (39%)]\tLoss: 1.073841\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 4 [97000/244675 (40%)]\tLoss: 1.198238\n",
      "Train Epoch: 4 [98000/244675 (40%)]\tLoss: 1.109544\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 4 [99000/244675 (40%)]\tLoss: 1.027177\n",
      "Train Epoch: 4 [100000/244675 (41%)]\tLoss: 1.373286\n",
      "Train Epoch: 4 [101000/244675 (41%)]\tLoss: 1.076993\n",
      "Train Epoch: 4 [102000/244675 (42%)]\tLoss: 1.352670\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 4 [103000/244675 (42%)]\tLoss: 1.456574\n",
      "Train Epoch: 4 [104000/244675 (43%)]\tLoss: 1.120862\n",
      "Train Epoch: 4 [105000/244675 (43%)]\tLoss: 1.148241\n",
      "Train Epoch: 4 [106000/244675 (43%)]\tLoss: 1.129260\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 4 [107000/244675 (44%)]\tLoss: 1.220567\n",
      "Train Epoch: 4 [108000/244675 (44%)]\tLoss: 1.343484\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 4 [109000/244675 (45%)]\tLoss: 1.115470\n",
      "Train Epoch: 4 [110000/244675 (45%)]\tLoss: 1.145007\n",
      "Train Epoch: 4 [111000/244675 (45%)]\tLoss: 0.994635\n",
      "Train Epoch: 4 [112000/244675 (46%)]\tLoss: 1.106191\n",
      "Train Epoch: 4 [113000/244675 (46%)]\tLoss: 1.083738\n",
      "Train Epoch: 4 [114000/244675 (47%)]\tLoss: 1.721200\n",
      "Train Epoch: 4 [115000/244675 (47%)]\tLoss: 1.188282\n",
      "Train Epoch: 4 [116000/244675 (47%)]\tLoss: 1.213194\n",
      "Train Epoch: 4 [117000/244675 (48%)]\tLoss: 1.188996\n",
      "Train Epoch: 4 [118000/244675 (48%)]\tLoss: 0.983357\n",
      "Train Epoch: 4 [119000/244675 (49%)]\tLoss: 1.207792\n",
      "Train Epoch: 4 [120000/244675 (49%)]\tLoss: 1.352453\n",
      "Train Epoch: 4 [121000/244675 (49%)]\tLoss: 1.478369\n",
      "Train Epoch: 4 [122000/244675 (50%)]\tLoss: 1.116493\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 4 [123000/244675 (50%)]\tLoss: 1.013481\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 4 [124000/244675 (51%)]\tLoss: 1.063042\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 4 [125000/244675 (51%)]\tLoss: 1.499763\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 4 [126000/244675 (51%)]\tLoss: 1.205577\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 4 [127000/244675 (52%)]\tLoss: 1.210087\n",
      "Train Epoch: 4 [128000/244675 (52%)]\tLoss: 0.986410\n",
      "Train Epoch: 4 [129000/244675 (53%)]\tLoss: 1.134474\n",
      "Train Epoch: 4 [130000/244675 (53%)]\tLoss: 1.303894\n",
      "Train Epoch: 4 [131000/244675 (54%)]\tLoss: 0.959977\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 4 [132000/244675 (54%)]\tLoss: 1.186008\n",
      "Train Epoch: 4 [133000/244675 (54%)]\tLoss: 1.418849\n",
      "Train Epoch: 4 [134000/244675 (55%)]\tLoss: 1.222631\n",
      "Train Epoch: 4 [135000/244675 (55%)]\tLoss: 1.230541\n",
      "Train Epoch: 4 [136000/244675 (56%)]\tLoss: 1.347709\n",
      "Train Epoch: 4 [137000/244675 (56%)]\tLoss: 1.388985\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 4 [138000/244675 (56%)]\tLoss: 0.964961\n",
      "Train Epoch: 4 [139000/244675 (57%)]\tLoss: 1.047428\n",
      "Train Epoch: 4 [140000/244675 (57%)]\tLoss: 1.292565\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 4 [141000/244675 (58%)]\tLoss: 1.324203\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 4 [142000/244675 (58%)]\tLoss: 1.217617\n",
      "Train Epoch: 4 [143000/244675 (58%)]\tLoss: 1.141820\n",
      "Train Epoch: 4 [144000/244675 (59%)]\tLoss: 1.257057\n",
      "Train Epoch: 4 [145000/244675 (59%)]\tLoss: 1.674184\n",
      "Train Epoch: 4 [146000/244675 (60%)]\tLoss: 1.227590\n",
      "Train Epoch: 4 [147000/244675 (60%)]\tLoss: 1.110096\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 4 [148000/244675 (60%)]\tLoss: 1.086907\n",
      "Train Epoch: 4 [149000/244675 (61%)]\tLoss: 1.230450\n",
      "Train Epoch: 4 [150000/244675 (61%)]\tLoss: 1.223101\n",
      "Train Epoch: 4 [151000/244675 (62%)]\tLoss: 1.199922\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 4 [152000/244675 (62%)]\tLoss: 1.160519\n",
      "Train Epoch: 4 [153000/244675 (63%)]\tLoss: 1.226057\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Train Epoch: 4 [154000/244675 (63%)]\tLoss: 1.393239\n",
      "Train Epoch: 4 [155000/244675 (63%)]\tLoss: 1.015752\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 4 [156000/244675 (64%)]\tLoss: 1.039852\n",
      "Train Epoch: 4 [157000/244675 (64%)]\tLoss: 1.041700\n",
      "Train Epoch: 4 [158000/244675 (65%)]\tLoss: 1.442937\n",
      "Train Epoch: 4 [159000/244675 (65%)]\tLoss: 1.015896\n",
      "Train Epoch: 4 [160000/244675 (65%)]\tLoss: 1.302683\n",
      "Train Epoch: 4 [161000/244675 (66%)]\tLoss: 1.201957\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 4 [162000/244675 (66%)]\tLoss: 1.436911\n",
      "Train Epoch: 4 [163000/244675 (67%)]\tLoss: 1.473903\n",
      "Train Epoch: 4 [164000/244675 (67%)]\tLoss: 1.473173\n",
      "Train Epoch: 4 [165000/244675 (67%)]\tLoss: 1.277281\n",
      "Train Epoch: 4 [166000/244675 (68%)]\tLoss: 1.044856\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 4 [167000/244675 (68%)]\tLoss: 1.234424\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 4 [168000/244675 (69%)]\tLoss: 1.097052\n",
      "Train Epoch: 4 [169000/244675 (69%)]\tLoss: 1.300883\n",
      "Train Epoch: 4 [170000/244675 (69%)]\tLoss: 1.142433\n",
      "Train Epoch: 4 [171000/244675 (70%)]\tLoss: 0.984980\n",
      "Train Epoch: 4 [172000/244675 (70%)]\tLoss: 1.504848\n",
      "Train Epoch: 4 [173000/244675 (71%)]\tLoss: 1.216378\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 4 [174000/244675 (71%)]\tLoss: 1.366835\n",
      "Train Epoch: 4 [175000/244675 (72%)]\tLoss: 1.342915\n",
      "Train Epoch: 4 [176000/244675 (72%)]\tLoss: 1.157284\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 4 [177000/244675 (72%)]\tLoss: 1.346034\n",
      "Train Epoch: 4 [178000/244675 (73%)]\tLoss: 1.000556\n",
      "Train Epoch: 4 [179000/244675 (73%)]\tLoss: 1.103487\n",
      "Train Epoch: 4 [180000/244675 (74%)]\tLoss: 1.235727\n",
      "Train Epoch: 4 [181000/244675 (74%)]\tLoss: 1.291910\n",
      "Train Epoch: 4 [182000/244675 (74%)]\tLoss: 1.028202\n",
      "Train Epoch: 4 [183000/244675 (75%)]\tLoss: 1.207441\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 4 [184000/244675 (75%)]\tLoss: 1.249425\n",
      "Train Epoch: 4 [185000/244675 (76%)]\tLoss: 1.078216\n",
      "Train Epoch: 4 [186000/244675 (76%)]\tLoss: 1.342255\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 4 [187000/244675 (76%)]\tLoss: 1.287864\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 4 [188000/244675 (77%)]\tLoss: 1.476259\n",
      "Train Epoch: 4 [189000/244675 (77%)]\tLoss: 1.347663\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 4 [190000/244675 (78%)]\tLoss: 1.190363\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 4 [191000/244675 (78%)]\tLoss: 0.854545\n",
      "Train Epoch: 4 [192000/244675 (78%)]\tLoss: 1.065901\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 4 [193000/244675 (79%)]\tLoss: 1.056811\n",
      "Train Epoch: 4 [194000/244675 (79%)]\tLoss: 1.190412\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 4 [195000/244675 (80%)]\tLoss: 1.157609\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 4 [196000/244675 (80%)]\tLoss: 1.274882\n",
      "Train Epoch: 4 [197000/244675 (81%)]\tLoss: 1.285156\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 4 [198000/244675 (81%)]\tLoss: 1.172284\n",
      "Train Epoch: 4 [199000/244675 (81%)]\tLoss: 1.216961\n",
      "Train Epoch: 4 [200000/244675 (82%)]\tLoss: 1.147423\n",
      "Train Epoch: 4 [201000/244675 (82%)]\tLoss: 1.234794\n",
      "Train Epoch: 4 [202000/244675 (83%)]\tLoss: 1.435569\n",
      "Train Epoch: 4 [203000/244675 (83%)]\tLoss: 1.163467\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 4 [204000/244675 (83%)]\tLoss: 1.199449\n",
      "Train Epoch: 4 [205000/244675 (84%)]\tLoss: 1.246605\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 4 [206000/244675 (84%)]\tLoss: 1.170423\n",
      "Train Epoch: 4 [207000/244675 (85%)]\tLoss: 1.423564\n",
      "Train Epoch: 4 [208000/244675 (85%)]\tLoss: 1.242017\n",
      "Train Epoch: 4 [209000/244675 (85%)]\tLoss: 1.094354\n",
      "Train Epoch: 4 [210000/244675 (86%)]\tLoss: 0.903887\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 4 [211000/244675 (86%)]\tLoss: 1.202022\n",
      "Train Epoch: 4 [212000/244675 (87%)]\tLoss: 1.147735\n",
      "Train Epoch: 4 [213000/244675 (87%)]\tLoss: 1.120808\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 4 [214000/244675 (87%)]\tLoss: 1.528149\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 4 [215000/244675 (88%)]\tLoss: 1.094175\n",
      "Train Epoch: 4 [216000/244675 (88%)]\tLoss: 1.040796\n",
      "Train Epoch: 4 [217000/244675 (89%)]\tLoss: 1.025884\n",
      "Train Epoch: 4 [218000/244675 (89%)]\tLoss: 1.674736\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 4 [219000/244675 (90%)]\tLoss: 1.350799\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 4 [220000/244675 (90%)]\tLoss: 1.204343\n",
      "Train Epoch: 4 [221000/244675 (90%)]\tLoss: 1.324225\n",
      "Train Epoch: 4 [222000/244675 (91%)]\tLoss: 1.169101\n",
      "Train Epoch: 4 [223000/244675 (91%)]\tLoss: 1.333030\n",
      "Train Epoch: 4 [224000/244675 (92%)]\tLoss: 1.328983\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Train Epoch: 4 [225000/244675 (92%)]\tLoss: 1.255326\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 4 [226000/244675 (92%)]\tLoss: 1.130159\n",
      "Train Epoch: 4 [227000/244675 (93%)]\tLoss: 1.131677\n",
      "Train Epoch: 4 [228000/244675 (93%)]\tLoss: 1.350749\n",
      "Train Epoch: 4 [229000/244675 (94%)]\tLoss: 1.377633\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 4 [230000/244675 (94%)]\tLoss: 1.433681\n",
      "Train Epoch: 4 [231000/244675 (94%)]\tLoss: 1.258768\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 4 [232000/244675 (95%)]\tLoss: 1.241569\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 4 [233000/244675 (95%)]\tLoss: 1.171052\n",
      "Train Epoch: 4 [234000/244675 (96%)]\tLoss: 1.294074\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 4 [235000/244675 (96%)]\tLoss: 1.231568\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 4 [236000/244675 (96%)]\tLoss: 1.063854\n",
      "Train Epoch: 4 [237000/244675 (97%)]\tLoss: 1.064524\n",
      "Train Epoch: 4 [238000/244675 (97%)]\tLoss: 1.382743\n",
      "Train Epoch: 4 [239000/244675 (98%)]\tLoss: 1.154036\n",
      "Train Epoch: 4 [240000/244675 (98%)]\tLoss: 1.054939\n",
      "Train Epoch: 4 [241000/244675 (98%)]\tLoss: 1.209167\n",
      "Train Epoch: 4 [242000/244675 (99%)]\tLoss: 1.002093\n",
      "Train Epoch: 4 [243000/244675 (99%)]\tLoss: 1.070480\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 4 [244000/244675 (100%)]\tLoss: 1.187458\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "['bate mon e ter frated hagon a seted lite as parer a preme', 'i wi mot blind be er syso dos plasa', 'te scugs sprary is pofoin af towardaf bars in oter potintal atackerse', 'ward in dakedts mary telater tat yea', 'as wo snore was prin at the prorute bey cheber of cons', 'he de veut eroly a tresstin flit iun behayer iextren sutrations', 'apalt mar refuses sonia acest to baka', 'the proventes as rich in minerol deposits inclengod in coper', 'a coring tu soser the jea grefecstedialanggagese deas with exter nat in turno myngristis', 'its underserfice is conca from be for bacwerd and con ves fromside tesid '] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.1729, Average CER: 0.380519 Average WER: 0.7960\n",
      "\n",
      "Train Epoch: 5 [0/244675 (0%)]\tLoss: 0.991217\n",
      "Train Epoch: 5 [1000/244675 (0%)]\tLoss: 1.187500\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 5 [2000/244675 (1%)]\tLoss: 1.031234\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 5 [3000/244675 (1%)]\tLoss: 1.270536\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 5 [4000/244675 (2%)]\tLoss: 1.196290\n",
      "Train Epoch: 5 [5000/244675 (2%)]\tLoss: 1.351525\n",
      "Train Epoch: 5 [6000/244675 (2%)]\tLoss: 0.920472\n",
      "Train Epoch: 5 [7000/244675 (3%)]\tLoss: 1.175582\n",
      "Train Epoch: 5 [8000/244675 (3%)]\tLoss: 1.488589\n",
      "Train Epoch: 5 [9000/244675 (4%)]\tLoss: 1.342395\n",
      "Train Epoch: 5 [10000/244675 (4%)]\tLoss: 1.264149\n",
      "Train Epoch: 5 [11000/244675 (4%)]\tLoss: 1.316171\n",
      "Train Epoch: 5 [12000/244675 (5%)]\tLoss: 1.232339\n",
      "Train Epoch: 5 [13000/244675 (5%)]\tLoss: 1.163604\n",
      "Train Epoch: 5 [14000/244675 (6%)]\tLoss: 1.124493\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 5 [15000/244675 (6%)]\tLoss: 1.310916\n",
      "Train Epoch: 5 [16000/244675 (7%)]\tLoss: 1.178454\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 5 [17000/244675 (7%)]\tLoss: 1.135876\n",
      "Train Epoch: 5 [18000/244675 (7%)]\tLoss: 1.078778\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 5 [19000/244675 (8%)]\tLoss: 1.279779\n",
      "Train Epoch: 5 [20000/244675 (8%)]\tLoss: 1.348035\n",
      "Train Epoch: 5 [21000/244675 (9%)]\tLoss: 1.111847\n",
      "Train Epoch: 5 [22000/244675 (9%)]\tLoss: 1.062990\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 5 [23000/244675 (9%)]\tLoss: 1.220216\n",
      "Train Epoch: 5 [24000/244675 (10%)]\tLoss: 1.100662\n",
      "Train Epoch: 5 [25000/244675 (10%)]\tLoss: 1.112311\n",
      "Train Epoch: 5 [26000/244675 (11%)]\tLoss: 1.028655\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 5 [27000/244675 (11%)]\tLoss: 1.099445\n",
      "Train Epoch: 5 [28000/244675 (11%)]\tLoss: 1.198887\n",
      "Train Epoch: 5 [29000/244675 (12%)]\tLoss: 1.237440\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 5 [30000/244675 (12%)]\tLoss: 1.033969\n",
      "Train Epoch: 5 [31000/244675 (13%)]\tLoss: 1.534891\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 5 [32000/244675 (13%)]\tLoss: 1.175670\n",
      "Train Epoch: 5 [33000/244675 (13%)]\tLoss: 1.238953\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 5 [34000/244675 (14%)]\tLoss: 1.012390\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 5 [35000/244675 (14%)]\tLoss: 1.055270\n",
      "Train Epoch: 5 [36000/244675 (15%)]\tLoss: 1.122376\n",
      "Train Epoch: 5 [37000/244675 (15%)]\tLoss: 1.195302\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 5 [38000/244675 (16%)]\tLoss: 1.137449\n",
      "Train Epoch: 5 [39000/244675 (16%)]\tLoss: 1.180829\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 5 [40000/244675 (16%)]\tLoss: 1.136468\n",
      "Train Epoch: 5 [41000/244675 (17%)]\tLoss: 1.109843\n",
      "Train Epoch: 5 [42000/244675 (17%)]\tLoss: 1.278790\n",
      "Train Epoch: 5 [43000/244675 (18%)]\tLoss: 1.198890\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 5 [44000/244675 (18%)]\tLoss: 1.317518\n",
      "Train Epoch: 5 [45000/244675 (18%)]\tLoss: 1.175381\n",
      "Train Epoch: 5 [46000/244675 (19%)]\tLoss: 1.160826\n",
      "Train Epoch: 5 [47000/244675 (19%)]\tLoss: 1.211638\n",
      "Train Epoch: 5 [48000/244675 (20%)]\tLoss: 1.194630\n",
      "Train Epoch: 5 [49000/244675 (20%)]\tLoss: 1.119240\n",
      "Train Epoch: 5 [50000/244675 (20%)]\tLoss: 1.309386\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 5 [51000/244675 (21%)]\tLoss: 1.175043\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 5 [52000/244675 (21%)]\tLoss: 1.176788\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 5 [53000/244675 (22%)]\tLoss: 1.326767\n",
      "Train Epoch: 5 [54000/244675 (22%)]\tLoss: 1.208174\n",
      "Train Epoch: 5 [55000/244675 (22%)]\tLoss: 1.035399\n",
      "Train Epoch: 5 [56000/244675 (23%)]\tLoss: 1.362514\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 5 [57000/244675 (23%)]\tLoss: 1.051126\n",
      "Train Epoch: 5 [58000/244675 (24%)]\tLoss: 0.973348\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 5 [59000/244675 (24%)]\tLoss: 1.075265\n",
      "Train Epoch: 5 [60000/244675 (25%)]\tLoss: 1.053827\n",
      "Train Epoch: 5 [61000/244675 (25%)]\tLoss: 1.266261\n",
      "Train Epoch: 5 [62000/244675 (25%)]\tLoss: 1.122747\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 5 [63000/244675 (26%)]\tLoss: 1.243959\n",
      "Train Epoch: 5 [64000/244675 (26%)]\tLoss: 1.120555\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 5 [65000/244675 (27%)]\tLoss: 1.214894\n",
      "Train Epoch: 5 [66000/244675 (27%)]\tLoss: 1.475525\n",
      "Train Epoch: 5 [67000/244675 (27%)]\tLoss: 1.049067\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 5 [68000/244675 (28%)]\tLoss: 1.326598\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 5 [69000/244675 (28%)]\tLoss: 1.152439\n",
      "Train Epoch: 5 [70000/244675 (29%)]\tLoss: 1.123522\n",
      "Train Epoch: 5 [71000/244675 (29%)]\tLoss: 1.213629\n",
      "Train Epoch: 5 [72000/244675 (29%)]\tLoss: 1.372457\n",
      "Train Epoch: 5 [73000/244675 (30%)]\tLoss: 1.171842\n",
      "Train Epoch: 5 [74000/244675 (30%)]\tLoss: 1.180455\n",
      "Train Epoch: 5 [75000/244675 (31%)]\tLoss: 1.169523\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 5 [76000/244675 (31%)]\tLoss: 1.127421\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 5 [77000/244675 (31%)]\tLoss: 1.246162\n",
      "Train Epoch: 5 [78000/244675 (32%)]\tLoss: 1.332176\n",
      "Train Epoch: 5 [79000/244675 (32%)]\tLoss: 1.223114\n",
      "Train Epoch: 5 [80000/244675 (33%)]\tLoss: 1.290129\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 5 [81000/244675 (33%)]\tLoss: 1.079248\n",
      "Train Epoch: 5 [82000/244675 (34%)]\tLoss: 1.233953\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 5 [83000/244675 (34%)]\tLoss: 1.356429\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 5 [84000/244675 (34%)]\tLoss: 0.982076\n",
      "Train Epoch: 5 [85000/244675 (35%)]\tLoss: 1.149631\n",
      "Train Epoch: 5 [86000/244675 (35%)]\tLoss: 1.402508\n",
      "Train Epoch: 5 [87000/244675 (36%)]\tLoss: 1.054053\n",
      "Train Epoch: 5 [88000/244675 (36%)]\tLoss: 1.142129\n",
      "Train Epoch: 5 [89000/244675 (36%)]\tLoss: 1.128142\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 5 [90000/244675 (37%)]\tLoss: 1.081054\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 5 [91000/244675 (37%)]\tLoss: 1.127033\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 5 [92000/244675 (38%)]\tLoss: 0.901504\n",
      "Train Epoch: 5 [93000/244675 (38%)]\tLoss: 1.469168\n",
      "Train Epoch: 5 [94000/244675 (38%)]\tLoss: 1.244329\n",
      "Train Epoch: 5 [95000/244675 (39%)]\tLoss: 1.285163\n",
      "Train Epoch: 5 [96000/244675 (39%)]\tLoss: 1.158899\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 5 [97000/244675 (40%)]\tLoss: 1.083009\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 5 [98000/244675 (40%)]\tLoss: 0.837330\n",
      "Train Epoch: 5 [99000/244675 (40%)]\tLoss: 1.035280\n",
      "Train Epoch: 5 [100000/244675 (41%)]\tLoss: 1.345466\n",
      "Train Epoch: 5 [101000/244675 (41%)]\tLoss: 1.288991\n",
      "Train Epoch: 5 [102000/244675 (42%)]\tLoss: 1.065236\n",
      "Train Epoch: 5 [103000/244675 (42%)]\tLoss: 1.192185\n",
      "Train Epoch: 5 [104000/244675 (43%)]\tLoss: 1.376004\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 5 [105000/244675 (43%)]\tLoss: 1.035053\n",
      "Train Epoch: 5 [106000/244675 (43%)]\tLoss: 1.030900\n",
      "Train Epoch: 5 [107000/244675 (44%)]\tLoss: 1.217626\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 5 [108000/244675 (44%)]\tLoss: 1.304242\n",
      "Train Epoch: 5 [109000/244675 (45%)]\tLoss: 0.971042\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 5 [110000/244675 (45%)]\tLoss: 1.338109\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 5 [111000/244675 (45%)]\tLoss: 1.151484\n",
      "Train Epoch: 5 [112000/244675 (46%)]\tLoss: 0.916177\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 5 [113000/244675 (46%)]\tLoss: 1.212033\n",
      "Train Epoch: 5 [114000/244675 (47%)]\tLoss: 1.074040\n",
      "Train Epoch: 5 [115000/244675 (47%)]\tLoss: 0.972716\n",
      "Train Epoch: 5 [116000/244675 (47%)]\tLoss: 1.215302\n",
      "Train Epoch: 5 [117000/244675 (48%)]\tLoss: 1.207480\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 5 [118000/244675 (48%)]\tLoss: 1.211763\n",
      "Train Epoch: 5 [119000/244675 (49%)]\tLoss: 1.169805\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 5 [120000/244675 (49%)]\tLoss: 1.134587\n",
      "Train Epoch: 5 [121000/244675 (49%)]\tLoss: 1.158631\n",
      "Train Epoch: 5 [122000/244675 (50%)]\tLoss: 1.303867\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 5 [123000/244675 (50%)]\tLoss: 1.244032\n",
      "Train Epoch: 5 [124000/244675 (51%)]\tLoss: 1.120322\n",
      "Train Epoch: 5 [125000/244675 (51%)]\tLoss: 1.060569\n",
      "Train Epoch: 5 [126000/244675 (51%)]\tLoss: 1.017124\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 5 [127000/244675 (52%)]\tLoss: 1.349473\n",
      "Train Epoch: 5 [128000/244675 (52%)]\tLoss: 1.341709\n",
      "Train Epoch: 5 [129000/244675 (53%)]\tLoss: 0.951814\n",
      "Train Epoch: 5 [130000/244675 (53%)]\tLoss: 1.205277\n",
      "Train Epoch: 5 [131000/244675 (54%)]\tLoss: 1.137426\n",
      "Train Epoch: 5 [132000/244675 (54%)]\tLoss: 1.271891\n",
      "Train Epoch: 5 [133000/244675 (54%)]\tLoss: 1.162885\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 5 [134000/244675 (55%)]\tLoss: 1.024319\n",
      "Train Epoch: 5 [135000/244675 (55%)]\tLoss: 1.090779\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 5 [136000/244675 (56%)]\tLoss: 1.386251\n",
      "Train Epoch: 5 [137000/244675 (56%)]\tLoss: 1.043672\n",
      "Train Epoch: 5 [138000/244675 (56%)]\tLoss: 0.919341\n",
      "Train Epoch: 5 [139000/244675 (57%)]\tLoss: 1.657053\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 5 [140000/244675 (57%)]\tLoss: 0.941435\n",
      "Train Epoch: 5 [141000/244675 (58%)]\tLoss: 0.931715\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 5 [142000/244675 (58%)]\tLoss: 1.236512\n",
      "Train Epoch: 5 [143000/244675 (58%)]\tLoss: 1.420061\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 5 [144000/244675 (59%)]\tLoss: 1.053844\n",
      "Train Epoch: 5 [145000/244675 (59%)]\tLoss: 1.197691\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 5 [146000/244675 (60%)]\tLoss: 1.075357\n",
      "Train Epoch: 5 [147000/244675 (60%)]\tLoss: 1.062805\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 5 [148000/244675 (60%)]\tLoss: 1.334086\n",
      "Train Epoch: 5 [149000/244675 (61%)]\tLoss: 1.046766\n",
      "Train Epoch: 5 [150000/244675 (61%)]\tLoss: 1.018844\n",
      "Train Epoch: 5 [151000/244675 (62%)]\tLoss: 1.053828\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 5 [153000/244675 (63%)]\tLoss: 0.971959\n",
      "Train Epoch: 5 [154000/244675 (63%)]\tLoss: 1.012941\n",
      "Train Epoch: 5 [155000/244675 (63%)]\tLoss: 1.016637\n",
      "Train Epoch: 5 [156000/244675 (64%)]\tLoss: 1.174369\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 5 [157000/244675 (64%)]\tLoss: 1.099554\n",
      "Train Epoch: 5 [158000/244675 (65%)]\tLoss: 1.066381\n",
      "Train Epoch: 5 [159000/244675 (65%)]\tLoss: 0.873319\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 5 [160000/244675 (65%)]\tLoss: 1.193555\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 5 [161000/244675 (66%)]\tLoss: 1.023224\n",
      "Train Epoch: 5 [162000/244675 (66%)]\tLoss: 1.221086\n",
      "Train Epoch: 5 [163000/244675 (67%)]\tLoss: 1.309700\n",
      "Train Epoch: 5 [164000/244675 (67%)]\tLoss: 1.142147\n",
      "Train Epoch: 5 [165000/244675 (67%)]\tLoss: 1.002849\n",
      "Train Epoch: 5 [166000/244675 (68%)]\tLoss: 1.160874\n",
      "Train Epoch: 5 [167000/244675 (68%)]\tLoss: 1.135120\n",
      "Train Epoch: 5 [168000/244675 (69%)]\tLoss: 1.166356\n",
      "Train Epoch: 5 [169000/244675 (69%)]\tLoss: 1.380527\n",
      "Train Epoch: 5 [170000/244675 (69%)]\tLoss: 0.974945\n",
      "Train Epoch: 5 [171000/244675 (70%)]\tLoss: 1.134485\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 5 [172000/244675 (70%)]\tLoss: 1.231644\n",
      "Train Epoch: 5 [173000/244675 (71%)]\tLoss: 1.142774\n",
      "Train Epoch: 5 [174000/244675 (71%)]\tLoss: 1.155377\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 5 [175000/244675 (72%)]\tLoss: 0.903337\n",
      "Train Epoch: 5 [176000/244675 (72%)]\tLoss: 1.242089\n",
      "Train Epoch: 5 [177000/244675 (72%)]\tLoss: 1.086081\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 5 [179000/244675 (73%)]\tLoss: 0.953712\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 5 [180000/244675 (74%)]\tLoss: 0.889554\n",
      "Train Epoch: 5 [181000/244675 (74%)]\tLoss: 1.157306\n",
      "Train Epoch: 5 [182000/244675 (74%)]\tLoss: 1.462095\n",
      "Train Epoch: 5 [183000/244675 (75%)]\tLoss: 1.081661\n",
      "Train Epoch: 5 [184000/244675 (75%)]\tLoss: 1.014958\n",
      "Train Epoch: 5 [185000/244675 (76%)]\tLoss: 0.996769\n",
      "Train Epoch: 5 [186000/244675 (76%)]\tLoss: 1.204866\n",
      "Train Epoch: 5 [187000/244675 (76%)]\tLoss: 1.100423\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 5 [188000/244675 (77%)]\tLoss: 0.970853\n",
      "Train Epoch: 5 [189000/244675 (77%)]\tLoss: 1.127715\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 5 [190000/244675 (78%)]\tLoss: 1.084734\n",
      "Train Epoch: 5 [191000/244675 (78%)]\tLoss: 1.359795\n",
      "Train Epoch: 5 [192000/244675 (78%)]\tLoss: 0.638067\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 5 [193000/244675 (79%)]\tLoss: 0.976046\n",
      "Train Epoch: 5 [194000/244675 (79%)]\tLoss: 1.269087\n",
      "Train Epoch: 5 [195000/244675 (80%)]\tLoss: 1.148544\n",
      "Train Epoch: 5 [196000/244675 (80%)]\tLoss: 1.060256\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 5 [197000/244675 (81%)]\tLoss: 1.047391\n",
      "Train Epoch: 5 [198000/244675 (81%)]\tLoss: 1.134243\n",
      "Train Epoch: 5 [199000/244675 (81%)]\tLoss: 1.053314\n",
      "Train Epoch: 5 [200000/244675 (82%)]\tLoss: 1.406164\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 5 [201000/244675 (82%)]\tLoss: 0.952568\n",
      "Train Epoch: 5 [202000/244675 (83%)]\tLoss: 1.093503\n",
      "Train Epoch: 5 [203000/244675 (83%)]\tLoss: 1.485259\n",
      "Train Epoch: 5 [204000/244675 (83%)]\tLoss: 1.051947\n",
      "Train Epoch: 5 [205000/244675 (84%)]\tLoss: 1.006156\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 5 [206000/244675 (84%)]\tLoss: 0.920718\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 5 [207000/244675 (85%)]\tLoss: 1.358758\n",
      "Train Epoch: 5 [208000/244675 (85%)]\tLoss: 1.124787\n",
      "Train Epoch: 5 [209000/244675 (85%)]\tLoss: 1.178654\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 5 [210000/244675 (86%)]\tLoss: 1.295014\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 5 [211000/244675 (86%)]\tLoss: 0.968736\n",
      "Train Epoch: 5 [212000/244675 (87%)]\tLoss: 1.017541\n",
      "Train Epoch: 5 [213000/244675 (87%)]\tLoss: 1.425865\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 5 [214000/244675 (87%)]\tLoss: 1.350615\n",
      "Train Epoch: 5 [215000/244675 (88%)]\tLoss: 1.061765\n",
      "Train Epoch: 5 [216000/244675 (88%)]\tLoss: 0.954385\n",
      "Train Epoch: 5 [217000/244675 (89%)]\tLoss: 1.143065\n",
      "Train Epoch: 5 [218000/244675 (89%)]\tLoss: 0.955341\n",
      "Train Epoch: 5 [219000/244675 (90%)]\tLoss: 1.049827\n",
      "Train Epoch: 5 [220000/244675 (90%)]\tLoss: 1.186323\n",
      "Train Epoch: 5 [221000/244675 (90%)]\tLoss: 1.040090\n",
      "Train Epoch: 5 [222000/244675 (91%)]\tLoss: 1.029512\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 5 [223000/244675 (91%)]\tLoss: 1.047648\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 5 [224000/244675 (92%)]\tLoss: 1.180968\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 5 [225000/244675 (92%)]\tLoss: 1.287821\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 5 [226000/244675 (92%)]\tLoss: 0.902863\n",
      "Train Epoch: 5 [227000/244675 (93%)]\tLoss: 1.035604\n",
      "Train Epoch: 5 [228000/244675 (93%)]\tLoss: 1.093675\n",
      "Train Epoch: 5 [229000/244675 (94%)]\tLoss: 1.041082\n",
      "Train Epoch: 5 [230000/244675 (94%)]\tLoss: 0.995296\n",
      "Train Epoch: 5 [231000/244675 (94%)]\tLoss: 1.227711\n",
      "Train Epoch: 5 [232000/244675 (95%)]\tLoss: 0.938253\n",
      "Train Epoch: 5 [233000/244675 (95%)]\tLoss: 1.024593\n",
      "Train Epoch: 5 [234000/244675 (96%)]\tLoss: 1.220475\n",
      "Train Epoch: 5 [235000/244675 (96%)]\tLoss: 1.344785\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 5 [236000/244675 (96%)]\tLoss: 0.936266\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 5 [237000/244675 (97%)]\tLoss: 1.047675\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 5 [238000/244675 (97%)]\tLoss: 1.164144\n",
      "Train Epoch: 5 [239000/244675 (98%)]\tLoss: 1.167208\n",
      "Train Epoch: 5 [240000/244675 (98%)]\tLoss: 1.001259\n",
      "Train Epoch: 5 [241000/244675 (98%)]\tLoss: 1.144865\n",
      "Train Epoch: 5 [242000/244675 (99%)]\tLoss: 1.063755\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 5 [243000/244675 (99%)]\tLoss: 1.151356\n",
      "Train Epoch: 5 [244000/244675 (100%)]\tLoss: 1.393406\n",
      "['bati mon ether cracted hagoon iseted lite as parteer of greme', 'i wi not bolin bo r siso dasply soa', 't stoks spray is porfein nafto wordof barsin ether potental ataccors', 'word an doketts mari tlater that year', 'asmena snoeo was pinn of the cuite bery tewi f coms', 'he deveut erte intreston fluht ium a yar nextren suctrations', 'a polt mart refuses sonia aces toug re bacca', 'the proventes as rich in mineralgeposits iculingoold in coper', 'a coring to fose the deagreicx tedionlanuages deolswith bexter nat in terna mingristis', 'its under sterfice is concai from bhe for backword and con bex fromside toside'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.0795, Average CER: 0.346425 Average WER: 0.7525\n",
      "\n",
      "Train Epoch: 6 [0/244675 (0%)]\tLoss: 0.903622\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 6 [1000/244675 (0%)]\tLoss: 0.956272\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 6 [2000/244675 (1%)]\tLoss: 1.086772\n",
      "Train Epoch: 6 [3000/244675 (1%)]\tLoss: 1.094025\n",
      "Train Epoch: 6 [4000/244675 (2%)]\tLoss: 1.404442\n",
      "Train Epoch: 6 [5000/244675 (2%)]\tLoss: 0.840356\n",
      "Train Epoch: 6 [6000/244675 (2%)]\tLoss: 1.122769\n",
      "Train Epoch: 6 [7000/244675 (3%)]\tLoss: 1.249763\n",
      "Train Epoch: 6 [8000/244675 (3%)]\tLoss: 0.851334\n",
      "Train Epoch: 6 [9000/244675 (4%)]\tLoss: 1.153871\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 6 [10000/244675 (4%)]\tLoss: 0.995028\n",
      "Train Epoch: 6 [11000/244675 (4%)]\tLoss: 1.070004\n",
      "Train Epoch: 6 [12000/244675 (5%)]\tLoss: 1.052777\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 6 [13000/244675 (5%)]\tLoss: 1.098235\n",
      "Train Epoch: 6 [14000/244675 (6%)]\tLoss: 0.834048\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 6 [15000/244675 (6%)]\tLoss: 0.877323\n",
      "Train Epoch: 6 [16000/244675 (7%)]\tLoss: 1.020792\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 6 [17000/244675 (7%)]\tLoss: 0.922418\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Train Epoch: 6 [18000/244675 (7%)]\tLoss: 1.110758\n",
      "Train Epoch: 6 [19000/244675 (8%)]\tLoss: 1.119165\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 6 [20000/244675 (8%)]\tLoss: 1.046844\n",
      "Train Epoch: 6 [21000/244675 (9%)]\tLoss: 1.016426\n",
      "Train Epoch: 6 [22000/244675 (9%)]\tLoss: 1.217620\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 6 [23000/244675 (9%)]\tLoss: 1.135705\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 6 [24000/244675 (10%)]\tLoss: 1.432934\n",
      "Train Epoch: 6 [25000/244675 (10%)]\tLoss: 0.979439\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 6 [26000/244675 (11%)]\tLoss: 0.885566\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 6 [27000/244675 (11%)]\tLoss: 1.205391\n",
      "Train Epoch: 6 [28000/244675 (11%)]\tLoss: 1.096407\n",
      "Train Epoch: 6 [29000/244675 (12%)]\tLoss: 1.101360\n",
      "Train Epoch: 6 [30000/244675 (12%)]\tLoss: 0.892749\n",
      "Train Epoch: 6 [31000/244675 (13%)]\tLoss: 0.936776\n",
      "Train Epoch: 6 [32000/244675 (13%)]\tLoss: 1.117650\n",
      "Train Epoch: 6 [33000/244675 (13%)]\tLoss: 1.182262\n",
      "Train Epoch: 6 [34000/244675 (14%)]\tLoss: 0.919252\n",
      "Train Epoch: 6 [35000/244675 (14%)]\tLoss: 1.052153\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 6 [36000/244675 (15%)]\tLoss: 1.022185\n",
      "Train Epoch: 6 [37000/244675 (15%)]\tLoss: 1.213378\n",
      "Train Epoch: 6 [38000/244675 (16%)]\tLoss: 1.107845\n",
      "Train Epoch: 6 [39000/244675 (16%)]\tLoss: 0.981925\n",
      "Train Epoch: 6 [40000/244675 (16%)]\tLoss: 1.079723\n",
      "Train Epoch: 6 [41000/244675 (17%)]\tLoss: 1.088243\n",
      "Train Epoch: 6 [42000/244675 (17%)]\tLoss: 1.147475\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 6 [43000/244675 (18%)]\tLoss: 0.893798\n",
      "Train Epoch: 6 [44000/244675 (18%)]\tLoss: 1.051461\n",
      "Train Epoch: 6 [45000/244675 (18%)]\tLoss: 0.889204\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 6 [46000/244675 (19%)]\tLoss: 1.119157\n",
      "Train Epoch: 6 [47000/244675 (19%)]\tLoss: 1.221938\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 6 [48000/244675 (20%)]\tLoss: 1.268358\n",
      "Train Epoch: 6 [49000/244675 (20%)]\tLoss: 1.271228\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 6 [50000/244675 (20%)]\tLoss: 0.996025\n",
      "Train Epoch: 6 [51000/244675 (21%)]\tLoss: 1.391150\n",
      "Train Epoch: 6 [52000/244675 (21%)]\tLoss: 1.381251\n",
      "Train Epoch: 6 [53000/244675 (22%)]\tLoss: 0.962561\n",
      "Train Epoch: 6 [54000/244675 (22%)]\tLoss: 1.155362\n",
      "Train Epoch: 6 [55000/244675 (22%)]\tLoss: 1.190755\n",
      "Train Epoch: 6 [56000/244675 (23%)]\tLoss: 1.180092\n",
      "Train Epoch: 6 [57000/244675 (23%)]\tLoss: 1.062399\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 6 [58000/244675 (24%)]\tLoss: 0.974622\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 6 [59000/244675 (24%)]\tLoss: 0.849715\n",
      "Train Epoch: 6 [60000/244675 (25%)]\tLoss: 1.151247\n",
      "Train Epoch: 6 [61000/244675 (25%)]\tLoss: 0.864061\n",
      "Train Epoch: 6 [62000/244675 (25%)]\tLoss: 1.108534\n",
      "Train Epoch: 6 [63000/244675 (26%)]\tLoss: 0.927032\n",
      "Train Epoch: 6 [64000/244675 (26%)]\tLoss: 0.827049\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 6 [65000/244675 (27%)]\tLoss: 1.166519\n",
      "Train Epoch: 6 [66000/244675 (27%)]\tLoss: 1.059239\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 6 [67000/244675 (27%)]\tLoss: 1.136027\n",
      "Train Epoch: 6 [68000/244675 (28%)]\tLoss: 1.156361\n",
      "Train Epoch: 6 [69000/244675 (28%)]\tLoss: 1.092934\n",
      "Train Epoch: 6 [70000/244675 (29%)]\tLoss: 1.050063\n",
      "Train Epoch: 6 [71000/244675 (29%)]\tLoss: 1.079998\n",
      "Train Epoch: 6 [72000/244675 (29%)]\tLoss: 1.059118\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 6 [73000/244675 (30%)]\tLoss: 1.035732\n",
      "Train Epoch: 6 [74000/244675 (30%)]\tLoss: 0.777585\n",
      "Train Epoch: 6 [75000/244675 (31%)]\tLoss: 1.489193\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 6 [76000/244675 (31%)]\tLoss: 0.922318\n",
      "Train Epoch: 6 [77000/244675 (31%)]\tLoss: 1.113452\n",
      "Train Epoch: 6 [78000/244675 (32%)]\tLoss: 1.138979\n",
      "Train Epoch: 6 [79000/244675 (32%)]\tLoss: 1.139436\n",
      "Train Epoch: 6 [80000/244675 (33%)]\tLoss: 1.172926\n",
      "Train Epoch: 6 [81000/244675 (33%)]\tLoss: 0.994764\n",
      "Train Epoch: 6 [82000/244675 (34%)]\tLoss: 1.270107\n",
      "Train Epoch: 6 [83000/244675 (34%)]\tLoss: 1.045729\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 6 [84000/244675 (34%)]\tLoss: 1.270261\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 6 [85000/244675 (35%)]\tLoss: 1.004177\n",
      "Train Epoch: 6 [86000/244675 (35%)]\tLoss: 0.959922\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 6 [87000/244675 (36%)]\tLoss: 1.158366\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 6 [88000/244675 (36%)]\tLoss: 0.919239\n",
      "Train Epoch: 6 [89000/244675 (36%)]\tLoss: 1.190441\n",
      "Train Epoch: 6 [90000/244675 (37%)]\tLoss: 0.861289\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 6 [91000/244675 (37%)]\tLoss: 1.239502\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 6 [92000/244675 (38%)]\tLoss: 1.032040\n",
      "Train Epoch: 6 [93000/244675 (38%)]\tLoss: 0.911663\n",
      "Train Epoch: 6 [94000/244675 (38%)]\tLoss: 0.868014\n",
      "Train Epoch: 6 [95000/244675 (39%)]\tLoss: 1.090191\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 6 [96000/244675 (39%)]\tLoss: 1.106358\n",
      "Train Epoch: 6 [97000/244675 (40%)]\tLoss: 1.085121\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 6 [98000/244675 (40%)]\tLoss: 1.001872\n",
      "Train Epoch: 6 [99000/244675 (40%)]\tLoss: 1.199754\n",
      "Train Epoch: 6 [100000/244675 (41%)]\tLoss: 1.166901\n",
      "Train Epoch: 6 [101000/244675 (41%)]\tLoss: 1.060805\n",
      "Train Epoch: 6 [102000/244675 (42%)]\tLoss: 1.091692\n",
      "Train Epoch: 6 [103000/244675 (42%)]\tLoss: 0.722449\n",
      "Train Epoch: 6 [104000/244675 (43%)]\tLoss: 1.028628\n",
      "Train Epoch: 6 [105000/244675 (43%)]\tLoss: 0.892639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 6 [106000/244675 (43%)]\tLoss: 1.246053\n",
      "Train Epoch: 6 [107000/244675 (44%)]\tLoss: 1.647139\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 6 [108000/244675 (44%)]\tLoss: 1.038737\n",
      "Train Epoch: 6 [109000/244675 (45%)]\tLoss: 1.372427\n",
      "Train Epoch: 6 [110000/244675 (45%)]\tLoss: 1.305565\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 6 [111000/244675 (45%)]\tLoss: 0.831752\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 6 [112000/244675 (46%)]\tLoss: 1.127755\n",
      "Train Epoch: 6 [113000/244675 (46%)]\tLoss: 1.150884\n",
      "Train Epoch: 6 [114000/244675 (47%)]\tLoss: 0.985793\n",
      "Train Epoch: 6 [115000/244675 (47%)]\tLoss: 1.016747\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 6 [116000/244675 (47%)]\tLoss: 1.129075\n",
      "Train Epoch: 6 [117000/244675 (48%)]\tLoss: 0.897337\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 6 [118000/244675 (48%)]\tLoss: 1.098785\n",
      "Train Epoch: 6 [119000/244675 (49%)]\tLoss: 0.863438\n",
      "Train Epoch: 6 [120000/244675 (49%)]\tLoss: 1.195767\n",
      "Train Epoch: 6 [121000/244675 (49%)]\tLoss: 0.973744\n",
      "Train Epoch: 6 [122000/244675 (50%)]\tLoss: 1.022480\n",
      "Train Epoch: 6 [123000/244675 (50%)]\tLoss: 1.181984\n",
      "Train Epoch: 6 [124000/244675 (51%)]\tLoss: 1.098603\n",
      "Train Epoch: 6 [125000/244675 (51%)]\tLoss: 1.345965\n",
      "Train Epoch: 6 [126000/244675 (51%)]\tLoss: 0.829554\n",
      "Train Epoch: 6 [127000/244675 (52%)]\tLoss: 0.996357\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 6 [128000/244675 (52%)]\tLoss: 1.033010\n",
      "Train Epoch: 6 [129000/244675 (53%)]\tLoss: 1.212746\n",
      "Train Epoch: 6 [130000/244675 (53%)]\tLoss: 0.910658\n",
      "Train Epoch: 6 [131000/244675 (54%)]\tLoss: 1.035003\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 6 [132000/244675 (54%)]\tLoss: 1.085962\n",
      "Train Epoch: 6 [133000/244675 (54%)]\tLoss: 0.842173\n",
      "Train Epoch: 6 [134000/244675 (55%)]\tLoss: 0.981569\n",
      "Train Epoch: 6 [135000/244675 (55%)]\tLoss: 1.203079\n",
      "Train Epoch: 6 [136000/244675 (56%)]\tLoss: 1.062321\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 6 [137000/244675 (56%)]\tLoss: 1.044124\n",
      "Train Epoch: 6 [138000/244675 (56%)]\tLoss: 0.995847\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 6 [139000/244675 (57%)]\tLoss: 0.958821\n",
      "Train Epoch: 6 [140000/244675 (57%)]\tLoss: 1.124214\n",
      "Train Epoch: 6 [141000/244675 (58%)]\tLoss: 1.282078\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 6 [142000/244675 (58%)]\tLoss: 0.956561\n",
      "Train Epoch: 6 [143000/244675 (58%)]\tLoss: 1.085200\n",
      "Train Epoch: 6 [144000/244675 (59%)]\tLoss: 1.200630\n",
      "Train Epoch: 6 [145000/244675 (59%)]\tLoss: 1.168896\n",
      "Train Epoch: 6 [146000/244675 (60%)]\tLoss: 1.293094\n",
      "Train Epoch: 6 [147000/244675 (60%)]\tLoss: 1.135543\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 6 [148000/244675 (60%)]\tLoss: 1.020531\n",
      "Train Epoch: 6 [149000/244675 (61%)]\tLoss: 0.964304\n",
      "Train Epoch: 6 [150000/244675 (61%)]\tLoss: 0.916309\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 6 [151000/244675 (62%)]\tLoss: 1.253859\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 6 [152000/244675 (62%)]\tLoss: 0.946539\n",
      "Train Epoch: 6 [153000/244675 (63%)]\tLoss: 1.082818\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 6 [154000/244675 (63%)]\tLoss: 1.280395\n",
      "Train Epoch: 6 [155000/244675 (63%)]\tLoss: 0.838308\n",
      "Train Epoch: 6 [156000/244675 (64%)]\tLoss: 0.918300\n",
      "Train Epoch: 6 [157000/244675 (64%)]\tLoss: 0.879065\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 6 [158000/244675 (65%)]\tLoss: 1.008588\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 6 [159000/244675 (65%)]\tLoss: 1.405302\n",
      "Train Epoch: 6 [160000/244675 (65%)]\tLoss: 1.047931\n",
      "Train Epoch: 6 [161000/244675 (66%)]\tLoss: 1.093987\n",
      "Train Epoch: 6 [162000/244675 (66%)]\tLoss: 0.873724\n",
      "Train Epoch: 6 [163000/244675 (67%)]\tLoss: 1.058130\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 6 [164000/244675 (67%)]\tLoss: 0.868993\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 6 [165000/244675 (67%)]\tLoss: 1.153800\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 6 [166000/244675 (68%)]\tLoss: 1.348083\n",
      "Train Epoch: 6 [167000/244675 (68%)]\tLoss: 1.151912\n",
      "Train Epoch: 6 [168000/244675 (69%)]\tLoss: 0.921140\n",
      "Train Epoch: 6 [169000/244675 (69%)]\tLoss: 0.992345\n",
      "Train Epoch: 6 [170000/244675 (69%)]\tLoss: 1.169286\n",
      "Train Epoch: 6 [171000/244675 (70%)]\tLoss: 1.157953\n",
      "Train Epoch: 6 [172000/244675 (70%)]\tLoss: 0.726778\n",
      "Train Epoch: 6 [173000/244675 (71%)]\tLoss: 1.064912\n",
      "Train Epoch: 6 [174000/244675 (71%)]\tLoss: 1.207147\n",
      "Train Epoch: 6 [175000/244675 (72%)]\tLoss: 1.056426\n",
      "Train Epoch: 6 [176000/244675 (72%)]\tLoss: 1.005288\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 6 [177000/244675 (72%)]\tLoss: 1.256139\n",
      "Train Epoch: 6 [178000/244675 (73%)]\tLoss: 0.778290\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 6 [179000/244675 (73%)]\tLoss: 1.255085\n",
      "Train Epoch: 6 [180000/244675 (74%)]\tLoss: 0.933026\n",
      "Train Epoch: 6 [181000/244675 (74%)]\tLoss: 1.094296\n",
      "Train Epoch: 6 [182000/244675 (74%)]\tLoss: 0.912189\n",
      "Train Epoch: 6 [183000/244675 (75%)]\tLoss: 0.822841\n",
      "Train Epoch: 6 [184000/244675 (75%)]\tLoss: 0.946489\n",
      "Train Epoch: 6 [185000/244675 (76%)]\tLoss: 0.899927\n",
      "Train Epoch: 6 [186000/244675 (76%)]\tLoss: 0.953532\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 6 [187000/244675 (76%)]\tLoss: 0.949719\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 6 [188000/244675 (77%)]\tLoss: 0.974798\n",
      "Train Epoch: 6 [189000/244675 (77%)]\tLoss: 1.102919\n",
      "Train Epoch: 6 [190000/244675 (78%)]\tLoss: 1.017671\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 6 [191000/244675 (78%)]\tLoss: 0.909259\n",
      "Train Epoch: 6 [192000/244675 (78%)]\tLoss: 0.995597\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 6 [193000/244675 (79%)]\tLoss: 1.002875\n",
      "Train Epoch: 6 [194000/244675 (79%)]\tLoss: 1.185550\n",
      "Train Epoch: 6 [195000/244675 (80%)]\tLoss: 0.968742\n",
      "Train Epoch: 6 [196000/244675 (80%)]\tLoss: 0.826416\n",
      "Train Epoch: 6 [197000/244675 (81%)]\tLoss: 0.860440\n",
      "Train Epoch: 6 [198000/244675 (81%)]\tLoss: 1.366300\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 6 [199000/244675 (81%)]\tLoss: 1.091340\n",
      "Train Epoch: 6 [200000/244675 (82%)]\tLoss: 0.949618\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 6 [201000/244675 (82%)]\tLoss: 1.040026\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 6 [202000/244675 (83%)]\tLoss: 1.306474\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 6 [203000/244675 (83%)]\tLoss: 0.938450\n",
      "Train Epoch: 6 [204000/244675 (83%)]\tLoss: 1.168920\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 6 [205000/244675 (84%)]\tLoss: 1.175128\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 6 [206000/244675 (84%)]\tLoss: 0.840414\n",
      "Train Epoch: 6 [207000/244675 (85%)]\tLoss: 0.853089\n",
      "Train Epoch: 6 [208000/244675 (85%)]\tLoss: 1.157232\n",
      "Train Epoch: 6 [209000/244675 (85%)]\tLoss: 1.409122\n",
      "Train Epoch: 6 [210000/244675 (86%)]\tLoss: 1.208386\n",
      "Train Epoch: 6 [211000/244675 (86%)]\tLoss: 0.756420\n",
      "Train Epoch: 6 [212000/244675 (87%)]\tLoss: 1.120099\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 6 [213000/244675 (87%)]\tLoss: 1.139176\n",
      "Train Epoch: 6 [214000/244675 (87%)]\tLoss: 0.690549\n",
      "Train Epoch: 6 [215000/244675 (88%)]\tLoss: 0.907765\n",
      "Train Epoch: 6 [216000/244675 (88%)]\tLoss: 1.013285\n",
      "Train Epoch: 6 [217000/244675 (89%)]\tLoss: 1.311989\n",
      "Train Epoch: 6 [218000/244675 (89%)]\tLoss: 1.014191\n",
      "Train Epoch: 6 [219000/244675 (90%)]\tLoss: 1.370745\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 6 [220000/244675 (90%)]\tLoss: 1.023690\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 6 [221000/244675 (90%)]\tLoss: 1.026583\n",
      "Train Epoch: 6 [222000/244675 (91%)]\tLoss: 0.851398\n",
      "Train Epoch: 6 [223000/244675 (91%)]\tLoss: 1.198230\n",
      "Train Epoch: 6 [224000/244675 (92%)]\tLoss: 1.327422\n",
      "Train Epoch: 6 [225000/244675 (92%)]\tLoss: 1.357783\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 6 [226000/244675 (92%)]\tLoss: 1.250718\n",
      "Train Epoch: 6 [227000/244675 (93%)]\tLoss: 1.271567\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 6 [228000/244675 (93%)]\tLoss: 1.092328\n",
      "Train Epoch: 6 [229000/244675 (94%)]\tLoss: 1.250273\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 6 [230000/244675 (94%)]\tLoss: 1.142365\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 6 [231000/244675 (94%)]\tLoss: 0.984838\n",
      "Train Epoch: 6 [232000/244675 (95%)]\tLoss: 1.060628\n",
      "Train Epoch: 6 [233000/244675 (95%)]\tLoss: 1.001479\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 6 [234000/244675 (96%)]\tLoss: 0.956290\n",
      "Train Epoch: 6 [235000/244675 (96%)]\tLoss: 1.327307\n",
      "Train Epoch: 6 [236000/244675 (96%)]\tLoss: 0.894435\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 6 [237000/244675 (97%)]\tLoss: 1.098122\n",
      "Train Epoch: 6 [238000/244675 (97%)]\tLoss: 0.961058\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 6 [239000/244675 (98%)]\tLoss: 1.013004\n",
      "Train Epoch: 6 [240000/244675 (98%)]\tLoss: 0.803664\n",
      "Train Epoch: 6 [241000/244675 (98%)]\tLoss: 1.056860\n",
      "Train Epoch: 6 [242000/244675 (99%)]\tLoss: 1.157400\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 6 [243000/244675 (99%)]\tLoss: 0.925184\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 6 [244000/244675 (100%)]\tLoss: 0.782843\n",
      "['bati mon ethergrated ha goan isceted wit as parteer a gremen', 'i wi not bean waand byer siso gos pla so', 'te scuks spray is powfein ouf to ward off baers in ever potintial a tackors', 'ward in dokedt mari tlater that year', 'as woo snowero was pident of the gretet vel chebe of coms', 'he evel crele intresston fligt acumpaar inextreem sutrationts', 'a palt marte refus es sonia acess toug e bacca', 'the proventcse as rich in mineral ge posits inclegoold in coper', 'acuring to sosse the jeagrefic tudianlanuages deas with extera nat in turnof manguristics', 'its under serfice is conca from be for bacword and con ves fromcide to sid w'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.0114, Average CER: 0.320136 Average WER: 0.7132\n",
      "\n",
      "Train Epoch: 7 [0/244675 (0%)]\tLoss: 1.070250\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 7 [1000/244675 (0%)]\tLoss: 0.978858\n",
      "Train Epoch: 7 [2000/244675 (1%)]\tLoss: 0.975349\n",
      "Train Epoch: 7 [3000/244675 (1%)]\tLoss: 0.994828\n",
      "Train Epoch: 7 [4000/244675 (2%)]\tLoss: 0.946110\n",
      "Train Epoch: 7 [5000/244675 (2%)]\tLoss: 1.020612\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 7 [6000/244675 (2%)]\tLoss: 0.961649\n",
      "Train Epoch: 7 [7000/244675 (3%)]\tLoss: 0.910311\n",
      "Train Epoch: 7 [8000/244675 (3%)]\tLoss: 0.840595\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 7 [9000/244675 (4%)]\tLoss: 0.996318\n",
      "Train Epoch: 7 [10000/244675 (4%)]\tLoss: 0.863958\n",
      "Train Epoch: 7 [11000/244675 (4%)]\tLoss: 0.921021\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 7 [12000/244675 (5%)]\tLoss: 0.893382\n",
      "Train Epoch: 7 [13000/244675 (5%)]\tLoss: 1.042604\n",
      "Train Epoch: 7 [14000/244675 (6%)]\tLoss: 0.728439\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 7 [15000/244675 (6%)]\tLoss: 1.060375\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 7 [16000/244675 (7%)]\tLoss: 0.838178\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 7 [17000/244675 (7%)]\tLoss: 0.952145\n",
      "Train Epoch: 7 [18000/244675 (7%)]\tLoss: 1.239631\n",
      "Train Epoch: 7 [19000/244675 (8%)]\tLoss: 0.927905\n",
      "Train Epoch: 7 [20000/244675 (8%)]\tLoss: 1.157550\n",
      "Train Epoch: 7 [21000/244675 (9%)]\tLoss: 0.713506\n",
      "Train Epoch: 7 [22000/244675 (9%)]\tLoss: 0.824008\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 7 [23000/244675 (9%)]\tLoss: 0.918290\n",
      "Train Epoch: 7 [24000/244675 (10%)]\tLoss: 1.004852\n",
      "Train Epoch: 7 [25000/244675 (10%)]\tLoss: 1.259938\n",
      "Train Epoch: 7 [26000/244675 (11%)]\tLoss: 0.811706\n",
      "Train Epoch: 7 [27000/244675 (11%)]\tLoss: 0.944917\n",
      "Train Epoch: 7 [28000/244675 (11%)]\tLoss: 0.916261\n",
      "Train Epoch: 7 [29000/244675 (12%)]\tLoss: 1.000019\n",
      "Train Epoch: 7 [30000/244675 (12%)]\tLoss: 0.828061\n",
      "Train Epoch: 7 [31000/244675 (13%)]\tLoss: 0.821423\n",
      "Train Epoch: 7 [32000/244675 (13%)]\tLoss: 1.196266\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 7 [33000/244675 (13%)]\tLoss: 1.342473\n",
      "Train Epoch: 7 [34000/244675 (14%)]\tLoss: 0.893190\n",
      "Train Epoch: 7 [35000/244675 (14%)]\tLoss: 1.006475\n",
      "Train Epoch: 7 [36000/244675 (15%)]\tLoss: 0.813078\n",
      "Train Epoch: 7 [37000/244675 (15%)]\tLoss: 1.254786\n",
      "Train Epoch: 7 [38000/244675 (16%)]\tLoss: 1.280773\n",
      "Train Epoch: 7 [39000/244675 (16%)]\tLoss: 0.995102\n",
      "Train Epoch: 7 [40000/244675 (16%)]\tLoss: 0.813841\n",
      "Train Epoch: 7 [41000/244675 (17%)]\tLoss: 0.855822\n",
      "Train Epoch: 7 [42000/244675 (17%)]\tLoss: 1.113468\n",
      "Train Epoch: 7 [43000/244675 (18%)]\tLoss: 1.282775\n",
      "Train Epoch: 7 [44000/244675 (18%)]\tLoss: 1.099632\n",
      "Train Epoch: 7 [45000/244675 (18%)]\tLoss: 0.751230\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 7 [46000/244675 (19%)]\tLoss: 1.142588\n",
      "Train Epoch: 7 [47000/244675 (19%)]\tLoss: 1.426770\n",
      "Train Epoch: 7 [48000/244675 (20%)]\tLoss: 1.077442\n",
      "Train Epoch: 7 [49000/244675 (20%)]\tLoss: 1.094756\n",
      "Train Epoch: 7 [50000/244675 (20%)]\tLoss: 0.792148\n",
      "Train Epoch: 7 [51000/244675 (21%)]\tLoss: 0.871252\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 7 [53000/244675 (22%)]\tLoss: 1.007742\n",
      "Train Epoch: 7 [54000/244675 (22%)]\tLoss: 0.962640\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 7 [55000/244675 (22%)]\tLoss: 0.809343\n",
      "Train Epoch: 7 [56000/244675 (23%)]\tLoss: 0.822494\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 7 [57000/244675 (23%)]\tLoss: 1.342522\n",
      "Train Epoch: 7 [58000/244675 (24%)]\tLoss: 1.065502\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 7 [59000/244675 (24%)]\tLoss: 0.955992\n",
      "Train Epoch: 7 [60000/244675 (25%)]\tLoss: 0.940628\n",
      "Train Epoch: 7 [61000/244675 (25%)]\tLoss: 0.717817\n",
      "Train Epoch: 7 [62000/244675 (25%)]\tLoss: 1.025193\n",
      "Train Epoch: 7 [63000/244675 (26%)]\tLoss: 0.746159\n",
      "Train Epoch: 7 [64000/244675 (26%)]\tLoss: 1.030220\n",
      "Train Epoch: 7 [65000/244675 (27%)]\tLoss: 1.062115\n",
      "Train Epoch: 7 [66000/244675 (27%)]\tLoss: 0.949273\n",
      "Train Epoch: 7 [67000/244675 (27%)]\tLoss: 0.999615\n",
      "Train Epoch: 7 [68000/244675 (28%)]\tLoss: 0.886015\n",
      "Train Epoch: 7 [69000/244675 (28%)]\tLoss: 1.013739\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 7 [70000/244675 (29%)]\tLoss: 0.947663\n",
      "Train Epoch: 7 [71000/244675 (29%)]\tLoss: 0.934776\n",
      "Train Epoch: 7 [72000/244675 (29%)]\tLoss: 0.907168\n",
      "Train Epoch: 7 [73000/244675 (30%)]\tLoss: 0.688340\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 7 [74000/244675 (30%)]\tLoss: 1.056777\n",
      "Train Epoch: 7 [75000/244675 (31%)]\tLoss: 0.902298\n",
      "Train Epoch: 7 [76000/244675 (31%)]\tLoss: 1.245548\n",
      "Train Epoch: 7 [77000/244675 (31%)]\tLoss: 0.790857\n",
      "Train Epoch: 7 [78000/244675 (32%)]\tLoss: 0.911457\n",
      "Train Epoch: 7 [79000/244675 (32%)]\tLoss: 1.024324\n",
      "Train Epoch: 7 [80000/244675 (33%)]\tLoss: 0.942956\n",
      "Train Epoch: 7 [81000/244675 (33%)]\tLoss: 1.115100\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 7 [82000/244675 (34%)]\tLoss: 1.123740\n",
      "Train Epoch: 7 [83000/244675 (34%)]\tLoss: 0.965837\n",
      "Train Epoch: 7 [84000/244675 (34%)]\tLoss: 1.009910\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 7 [85000/244675 (35%)]\tLoss: 0.754769\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 7 [86000/244675 (35%)]\tLoss: 0.998177\n",
      "Train Epoch: 7 [87000/244675 (36%)]\tLoss: 0.930380\n",
      "Train Epoch: 7 [88000/244675 (36%)]\tLoss: 1.194270\n",
      "Train Epoch: 7 [89000/244675 (36%)]\tLoss: 0.844761\n",
      "Train Epoch: 7 [90000/244675 (37%)]\tLoss: 1.076185\n",
      "Train Epoch: 7 [91000/244675 (37%)]\tLoss: 0.784281\n",
      "Train Epoch: 7 [92000/244675 (38%)]\tLoss: 1.138327\n",
      "Train Epoch: 7 [93000/244675 (38%)]\tLoss: 1.028725\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 7 [94000/244675 (38%)]\tLoss: 0.931129\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Train Epoch: 7 [95000/244675 (39%)]\tLoss: 0.944605\n",
      "Train Epoch: 7 [96000/244675 (39%)]\tLoss: 0.942571\n",
      "Train Epoch: 7 [97000/244675 (40%)]\tLoss: 0.964300\n",
      "Train Epoch: 7 [98000/244675 (40%)]\tLoss: 1.058513\n",
      "Train Epoch: 7 [99000/244675 (40%)]\tLoss: 0.809299\n",
      "Train Epoch: 7 [100000/244675 (41%)]\tLoss: 1.039140\n",
      "Train Epoch: 7 [101000/244675 (41%)]\tLoss: 1.265882\n",
      "Train Epoch: 7 [102000/244675 (42%)]\tLoss: 0.854766\n",
      "Train Epoch: 7 [103000/244675 (42%)]\tLoss: 1.150902\n",
      "Train Epoch: 7 [104000/244675 (43%)]\tLoss: 0.856574\n",
      "Train Epoch: 7 [105000/244675 (43%)]\tLoss: 0.762603\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 7 [106000/244675 (43%)]\tLoss: 0.957823\n",
      "Train Epoch: 7 [107000/244675 (44%)]\tLoss: 1.201151\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 7 [108000/244675 (44%)]\tLoss: 0.955593\n",
      "Train Epoch: 7 [109000/244675 (45%)]\tLoss: 0.941018\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 7 [110000/244675 (45%)]\tLoss: 0.928415\n",
      "Train Epoch: 7 [111000/244675 (45%)]\tLoss: 1.064396\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 7 [112000/244675 (46%)]\tLoss: 0.991113\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 7 [113000/244675 (46%)]\tLoss: 1.057282\n",
      "Train Epoch: 7 [114000/244675 (47%)]\tLoss: 0.917062\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 7 [115000/244675 (47%)]\tLoss: 1.067994\n",
      "Train Epoch: 7 [116000/244675 (47%)]\tLoss: 1.146211\n",
      "Train Epoch: 7 [117000/244675 (48%)]\tLoss: 1.070780\n",
      "Train Epoch: 7 [118000/244675 (48%)]\tLoss: 0.999772\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 7 [119000/244675 (49%)]\tLoss: 0.907150\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Train Epoch: 7 [120000/244675 (49%)]\tLoss: 0.816438\n",
      "Train Epoch: 7 [121000/244675 (49%)]\tLoss: 1.153947\n",
      "Train Epoch: 7 [122000/244675 (50%)]\tLoss: 0.823028\n",
      "Train Epoch: 7 [123000/244675 (50%)]\tLoss: 0.781606\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 7 [124000/244675 (51%)]\tLoss: 1.109617\n",
      "Train Epoch: 7 [125000/244675 (51%)]\tLoss: 0.961492\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 7 [126000/244675 (51%)]\tLoss: 0.916026\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 7 [127000/244675 (52%)]\tLoss: 1.004694\n",
      "Train Epoch: 7 [128000/244675 (52%)]\tLoss: 0.857573\n",
      "Train Epoch: 7 [129000/244675 (53%)]\tLoss: 1.067257\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 7 [130000/244675 (53%)]\tLoss: 0.899301\n",
      "Train Epoch: 7 [131000/244675 (54%)]\tLoss: 0.957212\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 7 [132000/244675 (54%)]\tLoss: 0.966587\n",
      "Train Epoch: 7 [133000/244675 (54%)]\tLoss: 1.007416\n",
      "Train Epoch: 7 [134000/244675 (55%)]\tLoss: 1.100062\n",
      "Train Epoch: 7 [135000/244675 (55%)]\tLoss: 0.868285\n",
      "Train Epoch: 7 [136000/244675 (56%)]\tLoss: 1.000424\n",
      "Train Epoch: 7 [137000/244675 (56%)]\tLoss: 0.638896\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 7 [138000/244675 (56%)]\tLoss: 0.832555\n",
      "Train Epoch: 7 [139000/244675 (57%)]\tLoss: 0.795780\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 7 [140000/244675 (57%)]\tLoss: 0.948473\n",
      "Train Epoch: 7 [141000/244675 (58%)]\tLoss: 1.011447\n",
      "Train Epoch: 7 [142000/244675 (58%)]\tLoss: 0.724652\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 7 [143000/244675 (58%)]\tLoss: 0.877123\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 7 [145000/244675 (59%)]\tLoss: 1.093494\n",
      "Train Epoch: 7 [146000/244675 (60%)]\tLoss: 1.208143\n",
      "Train Epoch: 7 [147000/244675 (60%)]\tLoss: 1.077976\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 7 [148000/244675 (60%)]\tLoss: 0.790092\n",
      "Train Epoch: 7 [149000/244675 (61%)]\tLoss: 0.907008\n",
      "Train Epoch: 7 [150000/244675 (61%)]\tLoss: 0.959396\n",
      "Train Epoch: 7 [151000/244675 (62%)]\tLoss: 0.962651\n",
      "Train Epoch: 7 [152000/244675 (62%)]\tLoss: 0.969829\n",
      "Train Epoch: 7 [153000/244675 (63%)]\tLoss: 0.870560\n",
      "Train Epoch: 7 [154000/244675 (63%)]\tLoss: 1.052300\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 7 [155000/244675 (63%)]\tLoss: 0.885450\n",
      "Train Epoch: 7 [156000/244675 (64%)]\tLoss: 1.093204\n",
      "Train Epoch: 7 [157000/244675 (64%)]\tLoss: 0.973223\n",
      "Train Epoch: 7 [158000/244675 (65%)]\tLoss: 0.917661\n",
      "Train Epoch: 7 [159000/244675 (65%)]\tLoss: 0.900928\n",
      "Train Epoch: 7 [160000/244675 (65%)]\tLoss: 0.935864\n",
      "Train Epoch: 7 [161000/244675 (66%)]\tLoss: 0.982048\n",
      "Train Epoch: 7 [162000/244675 (66%)]\tLoss: 0.883938\n",
      "Train Epoch: 7 [163000/244675 (67%)]\tLoss: 0.938076\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 7 [164000/244675 (67%)]\tLoss: 1.091624\n",
      "Train Epoch: 7 [165000/244675 (67%)]\tLoss: 0.716393\n",
      "Train Epoch: 7 [166000/244675 (68%)]\tLoss: 0.999134\n",
      "Train Epoch: 7 [167000/244675 (68%)]\tLoss: 0.769617\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 7 [168000/244675 (69%)]\tLoss: 0.766945\n",
      "Train Epoch: 7 [169000/244675 (69%)]\tLoss: 1.002331\n",
      "Train Epoch: 7 [170000/244675 (69%)]\tLoss: 0.976205\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 7 [171000/244675 (70%)]\tLoss: 1.033899\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 7 [172000/244675 (70%)]\tLoss: 0.912314\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 7 [173000/244675 (71%)]\tLoss: 1.214906\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 7 [174000/244675 (71%)]\tLoss: 0.811651\n",
      "Train Epoch: 7 [175000/244675 (72%)]\tLoss: 1.064644\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 7 [176000/244675 (72%)]\tLoss: 1.026156\n",
      "Train Epoch: 7 [177000/244675 (72%)]\tLoss: 1.140479\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 7 [178000/244675 (73%)]\tLoss: 0.898808\n",
      "Train Epoch: 7 [179000/244675 (73%)]\tLoss: 1.021381\n",
      "Train Epoch: 7 [180000/244675 (74%)]\tLoss: 1.006751\n",
      "Train Epoch: 7 [181000/244675 (74%)]\tLoss: 0.874459\n",
      "Train Epoch: 7 [182000/244675 (74%)]\tLoss: 1.155779\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 7 [183000/244675 (75%)]\tLoss: 0.947923\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 7 [184000/244675 (75%)]\tLoss: 0.893564\n",
      "Train Epoch: 7 [185000/244675 (76%)]\tLoss: 0.827196\n",
      "Train Epoch: 7 [186000/244675 (76%)]\tLoss: 0.753085\n",
      "Train Epoch: 7 [187000/244675 (76%)]\tLoss: 0.970225\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 7 [188000/244675 (77%)]\tLoss: 1.031611\n",
      "Train Epoch: 7 [189000/244675 (77%)]\tLoss: 0.969598\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 7 [190000/244675 (78%)]\tLoss: 0.846042\n",
      "Train Epoch: 7 [191000/244675 (78%)]\tLoss: 1.060069\n",
      "Train Epoch: 7 [192000/244675 (78%)]\tLoss: 1.059805\n",
      "Train Epoch: 7 [193000/244675 (79%)]\tLoss: 0.755904\n",
      "Train Epoch: 7 [194000/244675 (79%)]\tLoss: 0.741755\n",
      "Train Epoch: 7 [195000/244675 (80%)]\tLoss: 1.044256\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 7 [196000/244675 (80%)]\tLoss: 1.015579\n",
      "Train Epoch: 7 [197000/244675 (81%)]\tLoss: 0.865315\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 7 [198000/244675 (81%)]\tLoss: 1.003682\n",
      "Train Epoch: 7 [199000/244675 (81%)]\tLoss: 1.054795\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 7 [200000/244675 (82%)]\tLoss: 0.995494\n",
      "Train Epoch: 7 [201000/244675 (82%)]\tLoss: 0.909596\n",
      "Train Epoch: 7 [202000/244675 (83%)]\tLoss: 1.207115\n",
      "Train Epoch: 7 [203000/244675 (83%)]\tLoss: 0.818730\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 7 [204000/244675 (83%)]\tLoss: 1.008628\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 7 [205000/244675 (84%)]\tLoss: 0.904014\n",
      "Train Epoch: 7 [206000/244675 (84%)]\tLoss: 0.827035\n",
      "Train Epoch: 7 [207000/244675 (85%)]\tLoss: 0.750196\n",
      "Train Epoch: 7 [208000/244675 (85%)]\tLoss: 1.121238\n",
      "Train Epoch: 7 [209000/244675 (85%)]\tLoss: 0.909197\n",
      "Train Epoch: 7 [210000/244675 (86%)]\tLoss: 1.040848\n",
      "Train Epoch: 7 [211000/244675 (86%)]\tLoss: 1.124107\n",
      "Train Epoch: 7 [212000/244675 (87%)]\tLoss: 0.840418\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 7 [213000/244675 (87%)]\tLoss: 0.905940\n",
      "Train Epoch: 7 [214000/244675 (87%)]\tLoss: 0.904106\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 7 [215000/244675 (88%)]\tLoss: 0.973276\n",
      "Train Epoch: 7 [216000/244675 (88%)]\tLoss: 1.187929\n",
      "Train Epoch: 7 [217000/244675 (89%)]\tLoss: 1.055777\n",
      "Train Epoch: 7 [218000/244675 (89%)]\tLoss: 0.801537\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 7 [219000/244675 (90%)]\tLoss: 1.358769\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 7 [220000/244675 (90%)]\tLoss: 1.699418\n",
      "Train Epoch: 7 [221000/244675 (90%)]\tLoss: 1.213104\n",
      "Train Epoch: 7 [222000/244675 (91%)]\tLoss: 0.930270\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Train Epoch: 7 [223000/244675 (91%)]\tLoss: 1.373360\n",
      "Train Epoch: 7 [224000/244675 (92%)]\tLoss: 1.029626\n",
      "Train Epoch: 7 [225000/244675 (92%)]\tLoss: 1.372067\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 7 [226000/244675 (92%)]\tLoss: 0.968620\n",
      "Train Epoch: 7 [227000/244675 (93%)]\tLoss: 1.263187\n",
      "Train Epoch: 7 [228000/244675 (93%)]\tLoss: 1.116660\n",
      "Train Epoch: 7 [229000/244675 (94%)]\tLoss: 1.066532\n",
      "Train Epoch: 7 [230000/244675 (94%)]\tLoss: 1.139006\n",
      "Train Epoch: 7 [231000/244675 (94%)]\tLoss: 1.101089\n",
      "Train Epoch: 7 [232000/244675 (95%)]\tLoss: 0.952347\n",
      "Train Epoch: 7 [233000/244675 (95%)]\tLoss: 1.027293\n",
      "Train Epoch: 7 [234000/244675 (96%)]\tLoss: 0.955180\n",
      "Train Epoch: 7 [235000/244675 (96%)]\tLoss: 0.798584\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 7 [236000/244675 (96%)]\tLoss: 1.004645\n",
      "Train Epoch: 7 [237000/244675 (97%)]\tLoss: 1.019927\n",
      "Train Epoch: 7 [238000/244675 (97%)]\tLoss: 1.104506\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 7 [239000/244675 (98%)]\tLoss: 0.825558\n",
      "Train Epoch: 7 [240000/244675 (98%)]\tLoss: 0.837086\n",
      "Train Epoch: 7 [241000/244675 (98%)]\tLoss: 1.233258\n",
      "Train Epoch: 7 [242000/244675 (99%)]\tLoss: 1.014944\n",
      "Train Epoch: 7 [243000/244675 (99%)]\tLoss: 1.136788\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 7 [244000/244675 (100%)]\tLoss: 1.353979\n",
      "['bote mong e ther prated ha gon easseted lite as farter o greme', 'i wi not beo lnd bo yor sisi des pla soo', 'ta scus sprory is powfu in nog to ward off bers in ther potental atackors', 'wrd an dolkeds mary belatede tat yere', 'as wo snobo was prine of the crouted varly chebe of coms', 'he devel teroly atrestsen flught au baer nextren suturations', 'a palt mart refuses sonia access ton backa', 'the proventcs as rich in mineral de posits ecle gold in coper', 'a cornto soser the geagrefic studion lanuages deals with exter nat inturno manguristics', 'its under serfice is concae from befor backwored and conves fromside do sid'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 1.0154, Average CER: 0.324707 Average WER: 0.7166\n",
      "\n",
      "Train Epoch: 8 [0/244675 (0%)]\tLoss: 1.230115\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 8 [1000/244675 (0%)]\tLoss: 1.282074\n",
      "Train Epoch: 8 [2000/244675 (1%)]\tLoss: 0.990515\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 8 [3000/244675 (1%)]\tLoss: 0.884109\n",
      "Train Epoch: 8 [4000/244675 (2%)]\tLoss: 1.151550\n",
      "Train Epoch: 8 [5000/244675 (2%)]\tLoss: 0.891409\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 8 [6000/244675 (2%)]\tLoss: 0.987271\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 8 [7000/244675 (3%)]\tLoss: 0.962765\n",
      "Train Epoch: 8 [8000/244675 (3%)]\tLoss: 0.926124\n",
      "Train Epoch: 8 [9000/244675 (4%)]\tLoss: 1.100433\n",
      "Train Epoch: 8 [10000/244675 (4%)]\tLoss: 1.103511\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 8 [11000/244675 (4%)]\tLoss: 1.164927\n",
      "Train Epoch: 8 [12000/244675 (5%)]\tLoss: 1.289199\n",
      "Train Epoch: 8 [13000/244675 (5%)]\tLoss: 0.824631\n",
      "Train Epoch: 8 [14000/244675 (6%)]\tLoss: 0.938754\n",
      "Train Epoch: 8 [15000/244675 (6%)]\tLoss: 1.020868\n",
      "Train Epoch: 8 [16000/244675 (7%)]\tLoss: 1.080847\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 8 [17000/244675 (7%)]\tLoss: 0.930280\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 8 [18000/244675 (7%)]\tLoss: 0.965955\n",
      "Train Epoch: 8 [19000/244675 (8%)]\tLoss: 1.125413\n",
      "Train Epoch: 8 [20000/244675 (8%)]\tLoss: 0.662507\n",
      "Train Epoch: 8 [21000/244675 (9%)]\tLoss: 0.948962\n",
      "Train Epoch: 8 [22000/244675 (9%)]\tLoss: 0.900625\n",
      "Train Epoch: 8 [23000/244675 (9%)]\tLoss: 0.635880\n",
      "Train Epoch: 8 [24000/244675 (10%)]\tLoss: 0.849935\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 8 [25000/244675 (10%)]\tLoss: 0.985776\n",
      "Train Epoch: 8 [26000/244675 (11%)]\tLoss: 1.025350\n",
      "Train Epoch: 8 [27000/244675 (11%)]\tLoss: 0.784651\n",
      "Train Epoch: 8 [28000/244675 (11%)]\tLoss: 0.780788\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 8 [29000/244675 (12%)]\tLoss: 0.993631\n",
      "Train Epoch: 8 [30000/244675 (12%)]\tLoss: 1.070168\n",
      "Train Epoch: 8 [31000/244675 (13%)]\tLoss: 1.125316\n",
      "Train Epoch: 8 [32000/244675 (13%)]\tLoss: 1.081877\n",
      "Train Epoch: 8 [33000/244675 (13%)]\tLoss: 0.932942\n",
      "Train Epoch: 8 [34000/244675 (14%)]\tLoss: 0.936826\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 8 [35000/244675 (14%)]\tLoss: 0.932178\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 8 [36000/244675 (15%)]\tLoss: 0.660757\n",
      "Train Epoch: 8 [37000/244675 (15%)]\tLoss: 1.001637\n",
      "Train Epoch: 8 [38000/244675 (16%)]\tLoss: 0.870611\n",
      "Train Epoch: 8 [39000/244675 (16%)]\tLoss: 0.961382\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 8 [40000/244675 (16%)]\tLoss: 0.933347\n",
      "Train Epoch: 8 [41000/244675 (17%)]\tLoss: 0.831643\n",
      "Train Epoch: 8 [42000/244675 (17%)]\tLoss: 1.139033\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 8 [43000/244675 (18%)]\tLoss: 0.918078\n",
      "Train Epoch: 8 [44000/244675 (18%)]\tLoss: 0.764605\n",
      "Train Epoch: 8 [45000/244675 (18%)]\tLoss: 1.147115\n",
      "Train Epoch: 8 [46000/244675 (19%)]\tLoss: 1.007183\n",
      "Train Epoch: 8 [47000/244675 (19%)]\tLoss: 0.863608\n",
      "Train Epoch: 8 [48000/244675 (20%)]\tLoss: 0.850775\n",
      "Train Epoch: 8 [49000/244675 (20%)]\tLoss: 1.011877\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 8 [50000/244675 (20%)]\tLoss: 0.981252\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 8 [51000/244675 (21%)]\tLoss: 0.615168\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 8 [52000/244675 (21%)]\tLoss: 0.806490\n",
      "Train Epoch: 8 [53000/244675 (22%)]\tLoss: 0.878750\n",
      "Train Epoch: 8 [54000/244675 (22%)]\tLoss: 0.847714\n",
      "Train Epoch: 8 [55000/244675 (22%)]\tLoss: 0.733492\n",
      "Train Epoch: 8 [56000/244675 (23%)]\tLoss: 0.798250\n",
      "Train Epoch: 8 [57000/244675 (23%)]\tLoss: 0.866633\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 8 [58000/244675 (24%)]\tLoss: 1.115054\n",
      "Train Epoch: 8 [59000/244675 (24%)]\tLoss: 0.825197\n",
      "Train Epoch: 8 [60000/244675 (25%)]\tLoss: 1.017316\n",
      "Train Epoch: 8 [61000/244675 (25%)]\tLoss: 0.831092\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 8 [62000/244675 (25%)]\tLoss: 0.908178\n",
      "Train Epoch: 8 [63000/244675 (26%)]\tLoss: 1.024988\n",
      "Train Epoch: 8 [64000/244675 (26%)]\tLoss: 1.042453\n",
      "Train Epoch: 8 [65000/244675 (27%)]\tLoss: 0.857585\n",
      "Train Epoch: 8 [66000/244675 (27%)]\tLoss: 0.645789\n",
      "Train Epoch: 8 [67000/244675 (27%)]\tLoss: 1.227842\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 8 [68000/244675 (28%)]\tLoss: 0.749731\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 8 [69000/244675 (28%)]\tLoss: 0.811546\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 8 [70000/244675 (29%)]\tLoss: 0.722076\n",
      "Train Epoch: 8 [71000/244675 (29%)]\tLoss: 1.202117\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 8 [72000/244675 (29%)]\tLoss: 0.905106\n",
      "Train Epoch: 8 [73000/244675 (30%)]\tLoss: 1.003995\n",
      "Train Epoch: 8 [74000/244675 (30%)]\tLoss: 1.103192\n",
      "Train Epoch: 8 [75000/244675 (31%)]\tLoss: 0.695328\n",
      "Train Epoch: 8 [76000/244675 (31%)]\tLoss: 0.968375\n",
      "Train Epoch: 8 [77000/244675 (31%)]\tLoss: 0.844381\n",
      "Train Epoch: 8 [78000/244675 (32%)]\tLoss: 0.821146\n",
      "Train Epoch: 8 [79000/244675 (32%)]\tLoss: 0.838312\n",
      "Train Epoch: 8 [80000/244675 (33%)]\tLoss: 0.925276\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 8 [81000/244675 (33%)]\tLoss: 0.733108\n",
      "Train Epoch: 8 [82000/244675 (34%)]\tLoss: 1.211628\n",
      "Train Epoch: 8 [83000/244675 (34%)]\tLoss: 0.810505\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 8 [84000/244675 (34%)]\tLoss: 0.751192\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 8 [85000/244675 (35%)]\tLoss: 1.136922\n",
      "Train Epoch: 8 [86000/244675 (35%)]\tLoss: 1.014242\n",
      "Train Epoch: 8 [87000/244675 (36%)]\tLoss: 1.238884\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 8 [88000/244675 (36%)]\tLoss: 0.962148\n",
      "Train Epoch: 8 [89000/244675 (36%)]\tLoss: 0.995432\n",
      "Train Epoch: 8 [90000/244675 (37%)]\tLoss: 0.978312\n",
      "Train Epoch: 8 [91000/244675 (37%)]\tLoss: 0.692673\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 8 [92000/244675 (38%)]\tLoss: 0.998363\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 8 [93000/244675 (38%)]\tLoss: 0.870458\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 8 [94000/244675 (38%)]\tLoss: 0.833679\n",
      "Train Epoch: 8 [95000/244675 (39%)]\tLoss: 0.741708\n",
      "Train Epoch: 8 [96000/244675 (39%)]\tLoss: 0.964451\n",
      "Train Epoch: 8 [97000/244675 (40%)]\tLoss: 1.072851\n",
      "Train Epoch: 8 [98000/244675 (40%)]\tLoss: 0.781043\n",
      "Train Epoch: 8 [99000/244675 (40%)]\tLoss: 0.953878\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 8 [100000/244675 (41%)]\tLoss: 0.794159\n",
      "Train Epoch: 8 [101000/244675 (41%)]\tLoss: 1.235530\n",
      "Train Epoch: 8 [102000/244675 (42%)]\tLoss: 0.789291\n",
      "Train Epoch: 8 [103000/244675 (42%)]\tLoss: 0.990297\n",
      "Train Epoch: 8 [104000/244675 (43%)]\tLoss: 0.827053\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 8 [105000/244675 (43%)]\tLoss: 1.072433\n",
      "Train Epoch: 8 [106000/244675 (43%)]\tLoss: 0.797838\n",
      "Train Epoch: 8 [107000/244675 (44%)]\tLoss: 0.867790\n",
      "Train Epoch: 8 [108000/244675 (44%)]\tLoss: 0.714335\n",
      "Train Epoch: 8 [109000/244675 (45%)]\tLoss: 1.123924\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 8 [110000/244675 (45%)]\tLoss: 0.855094\n",
      "Train Epoch: 8 [111000/244675 (45%)]\tLoss: 1.006051\n",
      "Train Epoch: 8 [112000/244675 (46%)]\tLoss: 1.031610\n",
      "Train Epoch: 8 [113000/244675 (46%)]\tLoss: 0.903738\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 8 [114000/244675 (47%)]\tLoss: 0.700513\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 8 [115000/244675 (47%)]\tLoss: 1.001036\n",
      "Train Epoch: 8 [116000/244675 (47%)]\tLoss: 0.950050\n",
      "Train Epoch: 8 [117000/244675 (48%)]\tLoss: 0.820789\n",
      "Train Epoch: 8 [118000/244675 (48%)]\tLoss: 1.080819\n",
      "Train Epoch: 8 [119000/244675 (49%)]\tLoss: 1.083047\n",
      "Train Epoch: 8 [120000/244675 (49%)]\tLoss: 1.041757\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 8 [121000/244675 (49%)]\tLoss: 0.989452\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 8 [122000/244675 (50%)]\tLoss: 0.905811\n",
      "Train Epoch: 8 [123000/244675 (50%)]\tLoss: 0.895744\n",
      "Train Epoch: 8 [124000/244675 (51%)]\tLoss: 1.027662\n",
      "Train Epoch: 8 [125000/244675 (51%)]\tLoss: 1.062817\n",
      "Train Epoch: 8 [126000/244675 (51%)]\tLoss: 0.834513\n",
      "Train Epoch: 8 [127000/244675 (52%)]\tLoss: 0.938905\n",
      "Train Epoch: 8 [128000/244675 (52%)]\tLoss: 0.945123\n",
      "Train Epoch: 8 [129000/244675 (53%)]\tLoss: 0.901044\n",
      "Train Epoch: 8 [130000/244675 (53%)]\tLoss: 0.824162\n",
      "Train Epoch: 8 [131000/244675 (54%)]\tLoss: 1.004763\n",
      "Train Epoch: 8 [132000/244675 (54%)]\tLoss: 0.918168\n",
      "Train Epoch: 8 [133000/244675 (54%)]\tLoss: 0.972476\n",
      "Train Epoch: 8 [134000/244675 (55%)]\tLoss: 0.855882\n",
      "Train Epoch: 8 [135000/244675 (55%)]\tLoss: 1.005701\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 8 [136000/244675 (56%)]\tLoss: 0.864128\n",
      "Train Epoch: 8 [137000/244675 (56%)]\tLoss: 0.720365\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 8 [138000/244675 (56%)]\tLoss: 0.697933\n",
      "Train Epoch: 8 [139000/244675 (57%)]\tLoss: 0.754722\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 8 [140000/244675 (57%)]\tLoss: 0.987690\n",
      "Train Epoch: 8 [141000/244675 (58%)]\tLoss: 1.224485\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 8 [142000/244675 (58%)]\tLoss: 0.815617\n",
      "Train Epoch: 8 [143000/244675 (58%)]\tLoss: 0.940464\n",
      "Train Epoch: 8 [144000/244675 (59%)]\tLoss: 0.927246\n",
      "Train Epoch: 8 [145000/244675 (59%)]\tLoss: 0.837563\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 8 [146000/244675 (60%)]\tLoss: 0.769208\n",
      "Train Epoch: 8 [147000/244675 (60%)]\tLoss: 1.118045\n",
      "Train Epoch: 8 [148000/244675 (60%)]\tLoss: 0.867438\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 8 [149000/244675 (61%)]\tLoss: 0.721938\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 8 [150000/244675 (61%)]\tLoss: 0.741132\n",
      "Train Epoch: 8 [151000/244675 (62%)]\tLoss: 0.886378\n",
      "Train Epoch: 8 [152000/244675 (62%)]\tLoss: 1.047470\n",
      "Train Epoch: 8 [153000/244675 (63%)]\tLoss: 0.777378\n",
      "Train Epoch: 8 [154000/244675 (63%)]\tLoss: 0.795581\n",
      "Train Epoch: 8 [155000/244675 (63%)]\tLoss: 0.813289\n",
      "Train Epoch: 8 [156000/244675 (64%)]\tLoss: 0.889967\n",
      "Train Epoch: 8 [157000/244675 (64%)]\tLoss: 0.846405\n",
      "Train Epoch: 8 [158000/244675 (65%)]\tLoss: 1.015978\n",
      "Train Epoch: 8 [159000/244675 (65%)]\tLoss: 1.079236\n",
      "Train Epoch: 8 [160000/244675 (65%)]\tLoss: 0.961125\n",
      "Train Epoch: 8 [161000/244675 (66%)]\tLoss: 0.854286\n",
      "Train Epoch: 8 [162000/244675 (66%)]\tLoss: 0.888211\n",
      "Train Epoch: 8 [163000/244675 (67%)]\tLoss: 0.767655\n",
      "Train Epoch: 8 [164000/244675 (67%)]\tLoss: 0.825145\n",
      "Train Epoch: 8 [165000/244675 (67%)]\tLoss: 0.955667\n",
      "Train Epoch: 8 [166000/244675 (68%)]\tLoss: 0.942437\n",
      "Train Epoch: 8 [167000/244675 (68%)]\tLoss: 0.961991\n",
      "Train Epoch: 8 [168000/244675 (69%)]\tLoss: 0.956524\n",
      "Train Epoch: 8 [169000/244675 (69%)]\tLoss: 0.926599\n",
      "Train Epoch: 8 [170000/244675 (69%)]\tLoss: 1.067263\n",
      "Train Epoch: 8 [171000/244675 (70%)]\tLoss: 0.866872\n",
      "Train Epoch: 8 [172000/244675 (70%)]\tLoss: 0.980684\n",
      "Train Epoch: 8 [173000/244675 (71%)]\tLoss: 0.963242\n",
      "Train Epoch: 8 [174000/244675 (71%)]\tLoss: 0.736711\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 8 [175000/244675 (72%)]\tLoss: 0.763159\n",
      "Train Epoch: 8 [176000/244675 (72%)]\tLoss: 0.905975\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 8 [177000/244675 (72%)]\tLoss: 0.834435\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 8 [178000/244675 (73%)]\tLoss: 0.700578\n",
      "Train Epoch: 8 [179000/244675 (73%)]\tLoss: 0.922930\n",
      "Train Epoch: 8 [180000/244675 (74%)]\tLoss: 1.065081\n",
      "Train Epoch: 8 [181000/244675 (74%)]\tLoss: 0.908702\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 8 [182000/244675 (74%)]\tLoss: 0.836372\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 8 [183000/244675 (75%)]\tLoss: 0.859117\n",
      "Train Epoch: 8 [184000/244675 (75%)]\tLoss: 0.865776\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 8 [185000/244675 (76%)]\tLoss: 0.901818\n",
      "Train Epoch: 8 [186000/244675 (76%)]\tLoss: 0.878784\n",
      "Train Epoch: 8 [187000/244675 (76%)]\tLoss: 1.004865\n",
      "Train Epoch: 8 [188000/244675 (77%)]\tLoss: 0.759969\n",
      "Train Epoch: 8 [189000/244675 (77%)]\tLoss: 0.980101\n",
      "Train Epoch: 8 [190000/244675 (78%)]\tLoss: 1.080805\n",
      "Train Epoch: 8 [191000/244675 (78%)]\tLoss: 0.737752\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 8 [192000/244675 (78%)]\tLoss: 1.147378\n",
      "Train Epoch: 8 [193000/244675 (79%)]\tLoss: 0.868256\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 8 [194000/244675 (79%)]\tLoss: 0.872610\n",
      "Train Epoch: 8 [195000/244675 (80%)]\tLoss: 0.960864\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 8 [196000/244675 (80%)]\tLoss: 1.011965\n",
      "Train Epoch: 8 [197000/244675 (81%)]\tLoss: 0.696948\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 8 [198000/244675 (81%)]\tLoss: 1.076209\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 8 [199000/244675 (81%)]\tLoss: 0.730242\n",
      "Train Epoch: 8 [200000/244675 (82%)]\tLoss: 0.848983\n",
      "Train Epoch: 8 [201000/244675 (82%)]\tLoss: 0.912627\n",
      "Train Epoch: 8 [202000/244675 (83%)]\tLoss: 0.704348\n",
      "Train Epoch: 8 [203000/244675 (83%)]\tLoss: 1.281672\n",
      "Train Epoch: 8 [204000/244675 (83%)]\tLoss: 1.009905\n",
      "Train Epoch: 8 [205000/244675 (84%)]\tLoss: 1.071408\n",
      "Train Epoch: 8 [206000/244675 (84%)]\tLoss: 0.953318\n",
      "Train Epoch: 8 [207000/244675 (85%)]\tLoss: 0.791540\n",
      "Train Epoch: 8 [208000/244675 (85%)]\tLoss: 1.146850\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 8 [209000/244675 (85%)]\tLoss: 0.849161\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 8 [210000/244675 (86%)]\tLoss: 0.928554\n",
      "Train Epoch: 8 [211000/244675 (86%)]\tLoss: 0.747221\n",
      "Train Epoch: 8 [212000/244675 (87%)]\tLoss: 0.860759\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 8 [213000/244675 (87%)]\tLoss: 0.989132\n",
      "Train Epoch: 8 [214000/244675 (87%)]\tLoss: 1.010308\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 8 [215000/244675 (88%)]\tLoss: 0.802732\n",
      "Train Epoch: 8 [216000/244675 (88%)]\tLoss: 0.902067\n",
      "Train Epoch: 8 [217000/244675 (89%)]\tLoss: 0.920056\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Train Epoch: 8 [218000/244675 (89%)]\tLoss: 0.933826\n",
      "Train Epoch: 8 [219000/244675 (90%)]\tLoss: 0.816617\n",
      "Train Epoch: 8 [220000/244675 (90%)]\tLoss: 0.921990\n",
      "Train Epoch: 8 [221000/244675 (90%)]\tLoss: 0.874692\n",
      "Train Epoch: 8 [222000/244675 (91%)]\tLoss: 1.059705\n",
      "Train Epoch: 8 [223000/244675 (91%)]\tLoss: 0.749526\n",
      "Train Epoch: 8 [224000/244675 (92%)]\tLoss: 1.140604\n",
      "Train Epoch: 8 [225000/244675 (92%)]\tLoss: 1.238133\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 8 [226000/244675 (92%)]\tLoss: 0.800703\n",
      "Train Epoch: 8 [227000/244675 (93%)]\tLoss: 0.699048\n",
      "Train Epoch: 8 [228000/244675 (93%)]\tLoss: 0.850561\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 8 [229000/244675 (94%)]\tLoss: 0.932504\n",
      "Train Epoch: 8 [230000/244675 (94%)]\tLoss: 0.620317\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 8 [231000/244675 (94%)]\tLoss: 1.049243\n",
      "Train Epoch: 8 [232000/244675 (95%)]\tLoss: 0.820158\n",
      "Train Epoch: 8 [233000/244675 (95%)]\tLoss: 0.812078\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 8 [234000/244675 (96%)]\tLoss: 1.124202\n",
      "Train Epoch: 8 [235000/244675 (96%)]\tLoss: 1.140967\n",
      "Train Epoch: 8 [236000/244675 (96%)]\tLoss: 1.014500\n",
      "Train Epoch: 8 [237000/244675 (97%)]\tLoss: 0.876422\n",
      "Train Epoch: 8 [238000/244675 (97%)]\tLoss: 0.879005\n",
      "Train Epoch: 8 [239000/244675 (98%)]\tLoss: 1.096978\n",
      "Train Epoch: 8 [240000/244675 (98%)]\tLoss: 0.913544\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 8 [241000/244675 (98%)]\tLoss: 0.682526\n",
      "Train Epoch: 8 [242000/244675 (99%)]\tLoss: 1.066238\n",
      "Train Epoch: 8 [243000/244675 (99%)]\tLoss: 0.817768\n",
      "Train Epoch: 8 [244000/244675 (100%)]\tLoss: 1.012684\n",
      "['bati mang ea ther brated hagon esceted lite as partoor ogremen', 'i wil not beu land bon yersyso das playycoa', 'the scurts spray is powifuin ough to ward off bars in ofther potintral atackors', 'wrd in dokets mari telater that year', 'as woe snoo was praen at the proite varly cheber of comers', 'he develnt cro the intresst in flat iu har nextreen sutuations', 'a palt marten refuses sonia access touge backa', 'the provence is rich in mineral deposits inclegoold an coper', 'accoring two sasseer the geagreficxtudian languages deas with the dexteral na in ternof mangristics', 'its under serfice is conkae from be four backward and con ves fromside toside'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 0.9030, Average CER: 0.288115 Average WER: 0.6592\n",
      "\n",
      "Train Epoch: 9 [0/244675 (0%)]\tLoss: 0.956760\n",
      "Train Epoch: 9 [1000/244675 (0%)]\tLoss: 0.808988\n",
      "Train Epoch: 9 [2000/244675 (1%)]\tLoss: 0.903634\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 9 [3000/244675 (1%)]\tLoss: 0.907949\n",
      "Train Epoch: 9 [4000/244675 (2%)]\tLoss: 0.784963\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 9 [5000/244675 (2%)]\tLoss: 0.771860\n",
      "Train Epoch: 9 [6000/244675 (2%)]\tLoss: 0.883678\n",
      "Train Epoch: 9 [7000/244675 (3%)]\tLoss: 0.759482\n",
      "Train Epoch: 9 [8000/244675 (3%)]\tLoss: 0.722781\n",
      "Train Epoch: 9 [9000/244675 (4%)]\tLoss: 0.824468\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 9 [10000/244675 (4%)]\tLoss: 1.139627\n",
      "Train Epoch: 9 [11000/244675 (4%)]\tLoss: 0.941117\n",
      "Train Epoch: 9 [12000/244675 (5%)]\tLoss: 0.932412\n",
      "Train Epoch: 9 [13000/244675 (5%)]\tLoss: 1.014285\n",
      "Train Epoch: 9 [14000/244675 (6%)]\tLoss: 0.866910\n",
      "Train Epoch: 9 [15000/244675 (6%)]\tLoss: 0.596344\n",
      "Train Epoch: 9 [16000/244675 (7%)]\tLoss: 0.965370\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 9 [17000/244675 (7%)]\tLoss: 0.910919\n",
      "Train Epoch: 9 [18000/244675 (7%)]\tLoss: 1.042062\n",
      "Train Epoch: 9 [19000/244675 (8%)]\tLoss: 0.949410\n",
      "Train Epoch: 9 [20000/244675 (8%)]\tLoss: 0.459738\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 9 [21000/244675 (9%)]\tLoss: 0.755910\n",
      "Train Epoch: 9 [22000/244675 (9%)]\tLoss: 0.952024\n",
      "Train Epoch: 9 [23000/244675 (9%)]\tLoss: 0.996837\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 9 [24000/244675 (10%)]\tLoss: 0.971501\n",
      "Train Epoch: 9 [25000/244675 (10%)]\tLoss: 1.032660\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 9 [26000/244675 (11%)]\tLoss: 0.773871\n",
      "Train Epoch: 9 [27000/244675 (11%)]\tLoss: 0.701627\n",
      "Train Epoch: 9 [28000/244675 (11%)]\tLoss: 1.096470\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 9 [29000/244675 (12%)]\tLoss: 1.033333\n",
      "Train Epoch: 9 [30000/244675 (12%)]\tLoss: 0.694024\n",
      "Train Epoch: 9 [31000/244675 (13%)]\tLoss: 0.989234\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 9 [32000/244675 (13%)]\tLoss: 0.658613\n",
      "Train Epoch: 9 [33000/244675 (13%)]\tLoss: 0.738630\n",
      "Train Epoch: 9 [34000/244675 (14%)]\tLoss: 0.686642\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 9 [35000/244675 (14%)]\tLoss: 0.738261\n",
      "Train Epoch: 9 [36000/244675 (15%)]\tLoss: 1.027197\n",
      "Train Epoch: 9 [37000/244675 (15%)]\tLoss: 0.769159\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 9 [38000/244675 (16%)]\tLoss: 0.825804\n",
      "Train Epoch: 9 [39000/244675 (16%)]\tLoss: 0.731447\n",
      "Train Epoch: 9 [40000/244675 (16%)]\tLoss: 0.817738\n",
      "Train Epoch: 9 [41000/244675 (17%)]\tLoss: 0.716167\n",
      "Train Epoch: 9 [42000/244675 (17%)]\tLoss: 0.718417\n",
      "Train Epoch: 9 [43000/244675 (18%)]\tLoss: 0.772434\n",
      "Train Epoch: 9 [44000/244675 (18%)]\tLoss: 0.769913\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 9 [45000/244675 (18%)]\tLoss: 0.811311\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 9 [46000/244675 (19%)]\tLoss: 0.872197\n",
      "Train Epoch: 9 [47000/244675 (19%)]\tLoss: 0.845356\n",
      "Train Epoch: 9 [48000/244675 (20%)]\tLoss: 0.914741\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 9 [49000/244675 (20%)]\tLoss: 0.707766\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Train Epoch: 9 [50000/244675 (20%)]\tLoss: 0.852370\n",
      "Train Epoch: 9 [51000/244675 (21%)]\tLoss: 0.910254\n",
      "Train Epoch: 9 [52000/244675 (21%)]\tLoss: 0.859141\n",
      "Train Epoch: 9 [53000/244675 (22%)]\tLoss: 0.714431\n",
      "Train Epoch: 9 [54000/244675 (22%)]\tLoss: 0.797282\n",
      "Train Epoch: 9 [55000/244675 (22%)]\tLoss: 0.762932\n",
      "Train Epoch: 9 [56000/244675 (23%)]\tLoss: 0.936988\n",
      "Train Epoch: 9 [57000/244675 (23%)]\tLoss: 0.623253\n",
      "Train Epoch: 9 [58000/244675 (24%)]\tLoss: 0.882246\n",
      "Train Epoch: 9 [59000/244675 (24%)]\tLoss: 0.899400\n",
      "Train Epoch: 9 [60000/244675 (25%)]\tLoss: 0.839575\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 9 [61000/244675 (25%)]\tLoss: 0.661812\n",
      "Train Epoch: 9 [62000/244675 (25%)]\tLoss: 0.867293\n",
      "Train Epoch: 9 [63000/244675 (26%)]\tLoss: 0.782957\n",
      "Train Epoch: 9 [64000/244675 (26%)]\tLoss: 0.886102\n",
      "Train Epoch: 9 [65000/244675 (27%)]\tLoss: 0.819239\n",
      "Train Epoch: 9 [66000/244675 (27%)]\tLoss: 0.633516\n",
      "Train Epoch: 9 [67000/244675 (27%)]\tLoss: 0.945238\n",
      "Train Epoch: 9 [68000/244675 (28%)]\tLoss: 0.829864\n",
      "Train Epoch: 9 [69000/244675 (28%)]\tLoss: 0.862130\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Train Epoch: 9 [70000/244675 (29%)]\tLoss: 0.894916\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 9 [71000/244675 (29%)]\tLoss: 0.750583\n",
      "Train Epoch: 9 [72000/244675 (29%)]\tLoss: 0.814347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 9 [73000/244675 (30%)]\tLoss: 0.864561\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 9 [74000/244675 (30%)]\tLoss: 0.924103\n",
      "Train Epoch: 9 [75000/244675 (31%)]\tLoss: 0.700093\n",
      "Train Epoch: 9 [76000/244675 (31%)]\tLoss: 1.046543\n",
      "Train Epoch: 9 [77000/244675 (31%)]\tLoss: 0.848561\n",
      "Train Epoch: 9 [78000/244675 (32%)]\tLoss: 0.451765\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 9 [79000/244675 (32%)]\tLoss: 0.756102\n",
      "Train Epoch: 9 [80000/244675 (33%)]\tLoss: 1.140982\n",
      "Train Epoch: 9 [81000/244675 (33%)]\tLoss: 0.692536\n",
      "Train Epoch: 9 [82000/244675 (34%)]\tLoss: 1.077128\n",
      "Train Epoch: 9 [83000/244675 (34%)]\tLoss: 0.844517\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 9 [84000/244675 (34%)]\tLoss: 0.762044\n",
      "Train Epoch: 9 [85000/244675 (35%)]\tLoss: 0.715356\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 9 [86000/244675 (35%)]\tLoss: 0.673375\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Train Epoch: 9 [87000/244675 (36%)]\tLoss: 0.873590\n",
      "Train Epoch: 9 [88000/244675 (36%)]\tLoss: 0.945828\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 9 [89000/244675 (36%)]\tLoss: 0.964430\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Train Epoch: 9 [90000/244675 (37%)]\tLoss: 0.795046\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 9 [91000/244675 (37%)]\tLoss: 0.757215\n",
      "Train Epoch: 9 [92000/244675 (38%)]\tLoss: 1.079403\n",
      "Train Epoch: 9 [93000/244675 (38%)]\tLoss: 0.910818\n",
      "Train Epoch: 9 [94000/244675 (38%)]\tLoss: 0.991859\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 9 [95000/244675 (39%)]\tLoss: 0.901366\n",
      "Train Epoch: 9 [96000/244675 (39%)]\tLoss: 0.800533\n",
      "Train Epoch: 9 [97000/244675 (40%)]\tLoss: 0.696878\n",
      "Train Epoch: 9 [98000/244675 (40%)]\tLoss: 0.848344\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 9 [99000/244675 (40%)]\tLoss: 0.972417\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 9 [100000/244675 (41%)]\tLoss: 0.872363\n",
      "Train Epoch: 9 [101000/244675 (41%)]\tLoss: 0.855783\n",
      "Train Epoch: 9 [102000/244675 (42%)]\tLoss: 1.003137\n",
      "Train Epoch: 9 [103000/244675 (42%)]\tLoss: 1.038369\n",
      "Train Epoch: 9 [104000/244675 (43%)]\tLoss: 0.838606\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 9 [105000/244675 (43%)]\tLoss: 0.907234\n",
      "Train Epoch: 9 [106000/244675 (43%)]\tLoss: 0.742935\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 9 [107000/244675 (44%)]\tLoss: 0.887570\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 9 [108000/244675 (44%)]\tLoss: 0.625952\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 9 [109000/244675 (45%)]\tLoss: 0.938130\n",
      "Train Epoch: 9 [110000/244675 (45%)]\tLoss: 1.111747\n",
      "Train Epoch: 9 [111000/244675 (45%)]\tLoss: 0.770279\n",
      "Train Epoch: 9 [112000/244675 (46%)]\tLoss: 0.518326\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 9 [113000/244675 (46%)]\tLoss: 1.273984\n",
      "Train Epoch: 9 [114000/244675 (47%)]\tLoss: 1.059441\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 9 [115000/244675 (47%)]\tLoss: 1.049374\n",
      "Train Epoch: 9 [116000/244675 (47%)]\tLoss: 0.851456\n",
      "Train Epoch: 9 [117000/244675 (48%)]\tLoss: 0.711798\n",
      "Train Epoch: 9 [118000/244675 (48%)]\tLoss: 0.876151\n",
      "Train Epoch: 9 [119000/244675 (49%)]\tLoss: 0.864325\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 9 [120000/244675 (49%)]\tLoss: 0.939055\n",
      "Train Epoch: 9 [121000/244675 (49%)]\tLoss: 0.885587\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 9 [122000/244675 (50%)]\tLoss: 0.816988\n",
      "Train Epoch: 9 [123000/244675 (50%)]\tLoss: 0.642367\n",
      "Train Epoch: 9 [124000/244675 (51%)]\tLoss: 0.592751\n",
      "Train Epoch: 9 [125000/244675 (51%)]\tLoss: 0.783771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 9 [126000/244675 (51%)]\tLoss: 0.833091\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 9 [127000/244675 (52%)]\tLoss: 1.082832\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 9 [128000/244675 (52%)]\tLoss: 0.755134\n",
      "Train Epoch: 9 [129000/244675 (53%)]\tLoss: 0.822137\n",
      "Train Epoch: 9 [130000/244675 (53%)]\tLoss: 0.872370\n",
      "Train Epoch: 9 [131000/244675 (54%)]\tLoss: 1.034860\n",
      "Train Epoch: 9 [132000/244675 (54%)]\tLoss: 1.120517\n",
      "Train Epoch: 9 [133000/244675 (54%)]\tLoss: 0.729145\n",
      "Train Epoch: 9 [134000/244675 (55%)]\tLoss: 0.826961\n",
      "Train Epoch: 9 [135000/244675 (55%)]\tLoss: 0.705826\n",
      "Train Epoch: 9 [136000/244675 (56%)]\tLoss: 0.798417\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 9 [137000/244675 (56%)]\tLoss: 0.831321\n",
      "Train Epoch: 9 [138000/244675 (56%)]\tLoss: 0.819129\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 9 [139000/244675 (57%)]\tLoss: 0.739955\n",
      "Train Epoch: 9 [140000/244675 (57%)]\tLoss: 0.912900\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 9 [141000/244675 (58%)]\tLoss: 0.723354\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Train Epoch: 9 [142000/244675 (58%)]\tLoss: 1.031459\n",
      "Train Epoch: 9 [143000/244675 (58%)]\tLoss: 0.999827\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Train Epoch: 9 [144000/244675 (59%)]\tLoss: 0.841175\n",
      "Train Epoch: 9 [145000/244675 (59%)]\tLoss: 0.640484\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 9 [146000/244675 (60%)]\tLoss: 0.821657\n",
      "Train Epoch: 9 [147000/244675 (60%)]\tLoss: 0.995640\n",
      "Train Epoch: 9 [148000/244675 (60%)]\tLoss: 0.792512\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 9 [149000/244675 (61%)]\tLoss: 0.844137\n",
      "Train Epoch: 9 [150000/244675 (61%)]\tLoss: 0.862073\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 9 [151000/244675 (62%)]\tLoss: 0.718804\n",
      "Train Epoch: 9 [152000/244675 (62%)]\tLoss: 0.935374\n",
      "Train Epoch: 9 [153000/244675 (63%)]\tLoss: 0.862503\n",
      "Train Epoch: 9 [154000/244675 (63%)]\tLoss: 1.025071\n",
      "Train Epoch: 9 [155000/244675 (63%)]\tLoss: 0.992645\n",
      "Train Epoch: 9 [156000/244675 (64%)]\tLoss: 0.697066\n",
      "Train Epoch: 9 [157000/244675 (64%)]\tLoss: 1.066022\n",
      "Train Epoch: 9 [158000/244675 (65%)]\tLoss: 0.827324\n",
      "Train Epoch: 9 [159000/244675 (65%)]\tLoss: 0.741619\n",
      "Train Epoch: 9 [160000/244675 (65%)]\tLoss: 0.919180\n",
      "Train Epoch: 9 [161000/244675 (66%)]\tLoss: 0.813916\n",
      "Train Epoch: 9 [162000/244675 (66%)]\tLoss: 0.754493\n",
      "Train Epoch: 9 [163000/244675 (67%)]\tLoss: 0.840156\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 9 [164000/244675 (67%)]\tLoss: 0.740848\n",
      "Train Epoch: 9 [165000/244675 (67%)]\tLoss: 0.769811\n",
      "Train Epoch: 9 [166000/244675 (68%)]\tLoss: 0.713158\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 9 [167000/244675 (68%)]\tLoss: 0.847058\n",
      "Train Epoch: 9 [168000/244675 (69%)]\tLoss: 0.620337\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 9 [169000/244675 (69%)]\tLoss: 0.642268\n",
      "Train Epoch: 9 [170000/244675 (69%)]\tLoss: 0.749646\n",
      "Train Epoch: 9 [171000/244675 (70%)]\tLoss: 0.892117\n",
      "Train Epoch: 9 [172000/244675 (70%)]\tLoss: 0.993281\n",
      "Train Epoch: 9 [173000/244675 (71%)]\tLoss: 1.080245\n",
      "Train Epoch: 9 [174000/244675 (71%)]\tLoss: 0.874797\n",
      "Train Epoch: 9 [175000/244675 (72%)]\tLoss: 0.858948\n",
      "Train Epoch: 9 [176000/244675 (72%)]\tLoss: 0.834926\n",
      "Train Epoch: 9 [177000/244675 (72%)]\tLoss: 1.248301\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 9 [178000/244675 (73%)]\tLoss: 0.776263\n",
      "Train Epoch: 9 [179000/244675 (73%)]\tLoss: 0.906227\n",
      "Train Epoch: 9 [180000/244675 (74%)]\tLoss: 1.186017\n",
      "Train Epoch: 9 [181000/244675 (74%)]\tLoss: 0.967469\n",
      "Train Epoch: 9 [182000/244675 (74%)]\tLoss: 0.727584\n",
      "Train Epoch: 9 [183000/244675 (75%)]\tLoss: 0.853676\n",
      "Train Epoch: 9 [184000/244675 (75%)]\tLoss: 0.814467\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 9 [185000/244675 (76%)]\tLoss: 0.823678\n",
      "Train Epoch: 9 [186000/244675 (76%)]\tLoss: 0.773296\n",
      "Train Epoch: 9 [187000/244675 (76%)]\tLoss: 0.769434\n",
      "Train Epoch: 9 [188000/244675 (77%)]\tLoss: 0.821922\n",
      "Train Epoch: 9 [189000/244675 (77%)]\tLoss: 0.796015\n",
      "Train Epoch: 9 [190000/244675 (78%)]\tLoss: 0.843043\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Train Epoch: 9 [191000/244675 (78%)]\tLoss: 0.728384\n",
      "Train Epoch: 9 [192000/244675 (78%)]\tLoss: 0.830299\n",
      "Train Epoch: 9 [193000/244675 (79%)]\tLoss: 0.742948\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 9 [194000/244675 (79%)]\tLoss: 0.907468\n",
      "Train Epoch: 9 [195000/244675 (80%)]\tLoss: 0.903063\n",
      "Train Epoch: 9 [196000/244675 (80%)]\tLoss: 0.755982\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 9 [197000/244675 (81%)]\tLoss: 0.848328\n",
      "Train Epoch: 9 [198000/244675 (81%)]\tLoss: 0.853569\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 9 [199000/244675 (81%)]\tLoss: 0.811240\n",
      "Train Epoch: 9 [200000/244675 (82%)]\tLoss: 0.654048\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 9 [201000/244675 (82%)]\tLoss: 0.793755\n",
      "Train Epoch: 9 [202000/244675 (83%)]\tLoss: 0.662311\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 9 [203000/244675 (83%)]\tLoss: 0.678582\n",
      "Train Epoch: 9 [204000/244675 (83%)]\tLoss: 0.683041\n",
      "Train Epoch: 9 [205000/244675 (84%)]\tLoss: 0.766856\n",
      "Train Epoch: 9 [206000/244675 (84%)]\tLoss: 0.814032\n",
      "Train Epoch: 9 [207000/244675 (85%)]\tLoss: 0.748694\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 9 [208000/244675 (85%)]\tLoss: 0.913869\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 9 [209000/244675 (85%)]\tLoss: 0.580646\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 9 [210000/244675 (86%)]\tLoss: 0.737869\n",
      "Train Epoch: 9 [211000/244675 (86%)]\tLoss: 0.853624\n",
      "Train Epoch: 9 [212000/244675 (87%)]\tLoss: 0.794324\n",
      "Train Epoch: 9 [213000/244675 (87%)]\tLoss: 0.837293\n",
      "Train Epoch: 9 [214000/244675 (87%)]\tLoss: 0.714877\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 9 [215000/244675 (88%)]\tLoss: 0.861553\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 9 [216000/244675 (88%)]\tLoss: 0.925761\n",
      "Train Epoch: 9 [217000/244675 (89%)]\tLoss: 0.863396\n",
      "Train Epoch: 9 [218000/244675 (89%)]\tLoss: 0.806334\n",
      "Train Epoch: 9 [219000/244675 (90%)]\tLoss: 0.867252\n",
      "Train Epoch: 9 [220000/244675 (90%)]\tLoss: 0.891909\n",
      "Train Epoch: 9 [221000/244675 (90%)]\tLoss: 0.907990\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 9 [222000/244675 (91%)]\tLoss: 1.051630\n",
      "Train Epoch: 9 [223000/244675 (91%)]\tLoss: 0.926947\n",
      "Train Epoch: 9 [224000/244675 (92%)]\tLoss: 0.859964\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 9 [225000/244675 (92%)]\tLoss: 0.777745\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 9 [226000/244675 (92%)]\tLoss: 0.655440\n",
      "Train Epoch: 9 [227000/244675 (93%)]\tLoss: 0.672577\n",
      "Train Epoch: 9 [228000/244675 (93%)]\tLoss: 0.563129\n",
      "Train Epoch: 9 [229000/244675 (94%)]\tLoss: 0.874000\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 9 [230000/244675 (94%)]\tLoss: 0.858305\n",
      "Train Epoch: 9 [231000/244675 (94%)]\tLoss: 0.923150\n",
      "Train Epoch: 9 [232000/244675 (95%)]\tLoss: 0.688280\n",
      "Train Epoch: 9 [233000/244675 (95%)]\tLoss: 0.792021\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Train Epoch: 9 [234000/244675 (96%)]\tLoss: 1.228271\n",
      "Train Epoch: 9 [235000/244675 (96%)]\tLoss: 1.002288\n",
      "Train Epoch: 9 [236000/244675 (96%)]\tLoss: 0.608673\n",
      "Train Epoch: 9 [237000/244675 (97%)]\tLoss: 0.918244\n",
      "Train Epoch: 9 [238000/244675 (97%)]\tLoss: 0.823486\n",
      "Train Epoch: 9 [239000/244675 (98%)]\tLoss: 0.837420\n",
      "Train Epoch: 9 [240000/244675 (98%)]\tLoss: 0.592405\n",
      "Train Epoch: 9 [241000/244675 (98%)]\tLoss: 0.955442\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 9 [242000/244675 (99%)]\tLoss: 0.717391\n",
      "Train Epoch: 9 [243000/244675 (99%)]\tLoss: 0.916360\n",
      "Train Epoch: 9 [244000/244675 (100%)]\tLoss: 0.913542\n",
      "['bati mong he therpracted hagoen aseted lite as pateer o greme', 'i wil not beun land ben  rsiso dos playcoa', 'the scuts spray is powful in augh to ward off bars in other potintral atackors', 'ward in doketts marie telater that year', 'aswoe snoo was praent of the crete vary chebe ot commers', 'he develpe early entrestan flyght icubavor in extreen suturations', 'a palt martn refuses sonia access toughe backa', 'the proventse is rich in mineral de posits incle goold in coper', 'a curding two sase the gegrafic tudian languages deals with extera nat in terna manguristics', 'its under serfice is conca from be four backword and conves fromside teside '] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 0.8404, Average CER: 0.267268 Average WER: 0.6241\n",
      "\n",
      "Train Epoch: 10 [0/244675 (0%)]\tLoss: 0.837317\n",
      "Train Epoch: 10 [1000/244675 (0%)]\tLoss: 0.862964\n",
      "Train Epoch: 10 [2000/244675 (1%)]\tLoss: 0.945005\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2777\n",
      "Train Epoch: 10 [3000/244675 (1%)]\tLoss: 0.867938\n",
      "Train Epoch: 10 [4000/244675 (2%)]\tLoss: 0.843859\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2627\n",
      "Train Epoch: 10 [5000/244675 (2%)]\tLoss: 0.832793\n",
      "Train Epoch: 10 [6000/244675 (2%)]\tLoss: 1.053636\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2794\n",
      "Train Epoch: 10 [7000/244675 (3%)]\tLoss: 0.937530\n",
      "Train Epoch: 10 [8000/244675 (3%)]\tLoss: 0.749868\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 13479\n",
      "Train Epoch: 10 [9000/244675 (4%)]\tLoss: 0.739646\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 10 [10000/244675 (4%)]\tLoss: 0.622379\n",
      "Train Epoch: 10 [11000/244675 (4%)]\tLoss: 0.674513\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3140\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 10 [12000/244675 (5%)]\tLoss: 0.928357\n",
      "Train Epoch: 10 [13000/244675 (5%)]\tLoss: 0.951633\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 10 [14000/244675 (6%)]\tLoss: 0.933471\n",
      "Train Epoch: 10 [15000/244675 (6%)]\tLoss: 0.745742\n",
      "Train Epoch: 10 [16000/244675 (7%)]\tLoss: 0.751114\n",
      "Train Epoch: 10 [17000/244675 (7%)]\tLoss: 0.710217\n",
      "Train Epoch: 10 [18000/244675 (7%)]\tLoss: 0.830291\n",
      "Train Epoch: 10 [19000/244675 (8%)]\tLoss: 0.692242\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 10 [20000/244675 (8%)]\tLoss: 0.527244\n",
      "Train Epoch: 10 [21000/244675 (9%)]\tLoss: 0.778939\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3042\n",
      "Train Epoch: 10 [22000/244675 (9%)]\tLoss: 1.040629\n",
      "Train Epoch: 10 [23000/244675 (9%)]\tLoss: 0.792614\n",
      "Train Epoch: 10 [24000/244675 (10%)]\tLoss: 0.569420\n",
      "Train Epoch: 10 [25000/244675 (10%)]\tLoss: 0.699061\n",
      "Train Epoch: 10 [26000/244675 (11%)]\tLoss: 0.915687\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2610\n",
      "Train Epoch: 10 [27000/244675 (11%)]\tLoss: 0.861254\n",
      "Train Epoch: 10 [28000/244675 (11%)]\tLoss: 0.735748\n",
      "Train Epoch: 10 [29000/244675 (12%)]\tLoss: 0.695187\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3313\n",
      "Train Epoch: 10 [30000/244675 (12%)]\tLoss: 0.815875\n",
      "Train Epoch: 10 [31000/244675 (13%)]\tLoss: 0.662909\n",
      "Train Epoch: 10 [32000/244675 (13%)]\tLoss: 1.098239\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Train Epoch: 10 [33000/244675 (13%)]\tLoss: 0.811734\n",
      "Train Epoch: 10 [34000/244675 (14%)]\tLoss: 0.797271\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4885\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2904\n",
      "Train Epoch: 10 [35000/244675 (14%)]\tLoss: 0.710262\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6411\n",
      "Train Epoch: 10 [36000/244675 (15%)]\tLoss: 0.605372\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2938\n",
      "Train Epoch: 10 [37000/244675 (15%)]\tLoss: 0.755719\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 10 [38000/244675 (16%)]\tLoss: 0.634752\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3007\n",
      "Train Epoch: 10 [39000/244675 (16%)]\tLoss: 1.196120\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 10 [40000/244675 (16%)]\tLoss: 0.979340\n",
      "Train Epoch: 10 [41000/244675 (17%)]\tLoss: 0.960229\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 10 [42000/244675 (17%)]\tLoss: 0.911131\n",
      "Train Epoch: 10 [43000/244675 (18%)]\tLoss: 0.927502\n",
      "Train Epoch: 10 [44000/244675 (18%)]\tLoss: 0.802538\n",
      "Train Epoch: 10 [45000/244675 (18%)]\tLoss: 0.831341\n",
      "Train Epoch: 10 [46000/244675 (19%)]\tLoss: 0.757835\n",
      "Train Epoch: 10 [47000/244675 (19%)]\tLoss: 0.605031\n",
      "Train Epoch: 10 [48000/244675 (20%)]\tLoss: 0.785131\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2811\n",
      "Train Epoch: 10 [49000/244675 (20%)]\tLoss: 0.788904\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2765\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2996\n",
      "Train Epoch: 10 [50000/244675 (20%)]\tLoss: 0.740344\n",
      "Train Epoch: 10 [51000/244675 (21%)]\tLoss: 0.679497\n",
      "Train Epoch: 10 [52000/244675 (21%)]\tLoss: 0.860907\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2898\n",
      "Train Epoch: 10 [53000/244675 (22%)]\tLoss: 0.868961\n",
      "Train Epoch: 10 [54000/244675 (22%)]\tLoss: 0.641780\n",
      "Train Epoch: 10 [55000/244675 (22%)]\tLoss: 0.785441\n",
      "Train Epoch: 10 [56000/244675 (23%)]\tLoss: 1.213637\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2644\n",
      "Train Epoch: 10 [57000/244675 (23%)]\tLoss: 0.648026\n",
      "Train Epoch: 10 [58000/244675 (24%)]\tLoss: 0.652635\n",
      "Train Epoch: 10 [59000/244675 (24%)]\tLoss: 0.743037\n",
      "Train Epoch: 10 [60000/244675 (25%)]\tLoss: 0.902025\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2886\n",
      "Train Epoch: 10 [61000/244675 (25%)]\tLoss: 0.801171\n",
      "Train Epoch: 10 [62000/244675 (25%)]\tLoss: 0.611228\n",
      "Train Epoch: 10 [63000/244675 (26%)]\tLoss: 0.664090\n",
      "Train Epoch: 10 [64000/244675 (26%)]\tLoss: 0.950484\n",
      "Train Epoch: 10 [65000/244675 (27%)]\tLoss: 0.706912\n",
      "Train Epoch: 10 [66000/244675 (27%)]\tLoss: 0.854448\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 6141\n",
      "Train Epoch: 10 [67000/244675 (27%)]\tLoss: 0.711714\n",
      "Train Epoch: 10 [68000/244675 (28%)]\tLoss: 0.597507\n",
      "Train Epoch: 10 [69000/244675 (28%)]\tLoss: 0.672925\n",
      "Train Epoch: 10 [70000/244675 (29%)]\tLoss: 0.714485\n",
      "Train Epoch: 10 [71000/244675 (29%)]\tLoss: 0.831104\n",
      "Train Epoch: 10 [72000/244675 (29%)]\tLoss: 0.876064\n",
      "Train Epoch: 10 [73000/244675 (30%)]\tLoss: 1.021055\n",
      "Train Epoch: 10 [74000/244675 (30%)]\tLoss: 0.760874\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 29227\n",
      "Train Epoch: 10 [75000/244675 (31%)]\tLoss: 0.801096\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3958\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5179\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 10 [76000/244675 (31%)]\tLoss: 0.894082\n",
      "Train Epoch: 10 [77000/244675 (31%)]\tLoss: 1.000219\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Train Epoch: 10 [78000/244675 (32%)]\tLoss: 0.597547\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4119\n",
      "Train Epoch: 10 [79000/244675 (32%)]\tLoss: 0.762715\n",
      "Train Epoch: 10 [80000/244675 (33%)]\tLoss: 0.857117\n",
      "Train Epoch: 10 [81000/244675 (33%)]\tLoss: 0.688974\n",
      "Train Epoch: 10 [82000/244675 (34%)]\tLoss: 0.898153\n",
      "Train Epoch: 10 [83000/244675 (34%)]\tLoss: 0.799997\n",
      "Train Epoch: 10 [84000/244675 (34%)]\tLoss: 0.660709\n",
      "Train Epoch: 10 [85000/244675 (35%)]\tLoss: 0.772451\n",
      "Train Epoch: 10 [86000/244675 (35%)]\tLoss: 0.997588\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 8554\n",
      "Train Epoch: 10 [87000/244675 (36%)]\tLoss: 0.828873\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3733\n",
      "Train Epoch: 10 [88000/244675 (36%)]\tLoss: 0.811416\n",
      "Train Epoch: 10 [89000/244675 (36%)]\tLoss: 0.842241\n",
      "Train Epoch: 10 [90000/244675 (37%)]\tLoss: 0.599310\n",
      "Train Epoch: 10 [91000/244675 (37%)]\tLoss: 0.597006\n",
      "Train Epoch: 10 [92000/244675 (38%)]\tLoss: 0.696291\n",
      "Train Epoch: 10 [93000/244675 (38%)]\tLoss: 0.825537\n",
      "Train Epoch: 10 [94000/244675 (38%)]\tLoss: 0.745540\n",
      "Train Epoch: 10 [95000/244675 (39%)]\tLoss: 0.738020\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2852\n",
      "Train Epoch: 10 [96000/244675 (39%)]\tLoss: 0.936167\n",
      "Train Epoch: 10 [97000/244675 (40%)]\tLoss: 0.812788\n",
      "Train Epoch: 10 [98000/244675 (40%)]\tLoss: 0.877664\n",
      "Train Epoch: 10 [99000/244675 (40%)]\tLoss: 1.162156\n",
      "Train Epoch: 10 [100000/244675 (41%)]\tLoss: 0.711572\n",
      "Train Epoch: 10 [101000/244675 (41%)]\tLoss: 0.655437\n",
      "Train Epoch: 10 [102000/244675 (42%)]\tLoss: 1.170585\n",
      "Train Epoch: 10 [103000/244675 (42%)]\tLoss: 0.720543\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3082\n",
      "Train Epoch: 10 [104000/244675 (43%)]\tLoss: 0.960880\n",
      "Train Epoch: 10 [105000/244675 (43%)]\tLoss: 0.625165\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7137\n",
      "Train Epoch: 10 [106000/244675 (43%)]\tLoss: 0.707955\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3019\n",
      "Train Epoch: 10 [107000/244675 (44%)]\tLoss: 0.763580\n",
      "Train Epoch: 10 [108000/244675 (44%)]\tLoss: 0.864710\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Train Epoch: 10 [109000/244675 (45%)]\tLoss: 0.836432\n",
      "Train Epoch: 10 [110000/244675 (45%)]\tLoss: 0.668792\n",
      "Train Epoch: 10 [111000/244675 (45%)]\tLoss: 0.766385\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3094\n",
      "Train Epoch: 10 [112000/244675 (46%)]\tLoss: 0.808032\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2673\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3180\n",
      "Train Epoch: 10 [113000/244675 (46%)]\tLoss: 0.706341\n",
      "Train Epoch: 10 [114000/244675 (47%)]\tLoss: 0.698602\n",
      "Train Epoch: 10 [115000/244675 (47%)]\tLoss: 0.826203\n",
      "Train Epoch: 10 [116000/244675 (47%)]\tLoss: 0.975344\n",
      "Train Epoch: 10 [117000/244675 (48%)]\tLoss: 0.719671\n",
      "Train Epoch: 10 [118000/244675 (48%)]\tLoss: 0.611549\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2702\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3226\n",
      "Train Epoch: 10 [119000/244675 (49%)]\tLoss: 0.621027\n",
      "Train Epoch: 10 [120000/244675 (49%)]\tLoss: 0.879395\n",
      "Train Epoch: 10 [121000/244675 (49%)]\tLoss: 0.723942\n",
      "Train Epoch: 10 [122000/244675 (50%)]\tLoss: 0.652782\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2662\n",
      "Train Epoch: 10 [123000/244675 (50%)]\tLoss: 0.796582\n",
      "Train Epoch: 10 [124000/244675 (51%)]\tLoss: 0.600893\n",
      "Train Epoch: 10 [125000/244675 (51%)]\tLoss: 0.744885\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 40321\n",
      "Train Epoch: 10 [126000/244675 (51%)]\tLoss: 0.971101\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5300\n",
      "Train Epoch: 10 [127000/244675 (52%)]\tLoss: 0.843169\n",
      "Train Epoch: 10 [128000/244675 (52%)]\tLoss: 1.064805\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5311\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Train Epoch: 10 [129000/244675 (53%)]\tLoss: 0.655301\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 10 [130000/244675 (53%)]\tLoss: 0.839454\n",
      "Train Epoch: 10 [131000/244675 (54%)]\tLoss: 0.980388\n",
      "Train Epoch: 10 [132000/244675 (54%)]\tLoss: 0.828915\n",
      "Train Epoch: 10 [133000/244675 (54%)]\tLoss: 0.818557\n",
      "Train Epoch: 10 [134000/244675 (55%)]\tLoss: 0.742878\n",
      "Train Epoch: 10 [135000/244675 (55%)]\tLoss: 0.961631\n",
      "Train Epoch: 10 [136000/244675 (56%)]\tLoss: 0.775834\n",
      "Train Epoch: 10 [137000/244675 (56%)]\tLoss: 0.902965\n",
      "Train Epoch: 10 [138000/244675 (56%)]\tLoss: 0.562888\n",
      "Train Epoch: 10 [139000/244675 (57%)]\tLoss: 0.869596\n",
      "Train Epoch: 10 [140000/244675 (57%)]\tLoss: 0.780303\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2690\n",
      "Train Epoch: 10 [141000/244675 (58%)]\tLoss: 0.486472\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2927\n",
      "Train Epoch: 10 [142000/244675 (58%)]\tLoss: 0.969750\n",
      "Train Epoch: 10 [143000/244675 (58%)]\tLoss: 0.675976\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2685\n",
      "Train Epoch: 10 [145000/244675 (59%)]\tLoss: 0.946163\n",
      "Train Epoch: 10 [146000/244675 (60%)]\tLoss: 0.973999\n",
      "Train Epoch: 10 [147000/244675 (60%)]\tLoss: 0.895091\n",
      "Train Epoch: 10 [148000/244675 (60%)]\tLoss: 0.742156\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2650\n",
      "Train Epoch: 10 [149000/244675 (61%)]\tLoss: 0.730450\n",
      "Train Epoch: 10 [150000/244675 (61%)]\tLoss: 0.799557\n",
      "Train Epoch: 10 [151000/244675 (62%)]\tLoss: 0.696387\n",
      "Train Epoch: 10 [152000/244675 (62%)]\tLoss: 0.934853\n",
      "Train Epoch: 10 [153000/244675 (63%)]\tLoss: 0.822363\n",
      "Train Epoch: 10 [154000/244675 (63%)]\tLoss: 0.908183\n",
      "Train Epoch: 10 [155000/244675 (63%)]\tLoss: 0.779995\n",
      "Train Epoch: 10 [156000/244675 (64%)]\tLoss: 0.624196\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4315\n",
      "Train Epoch: 10 [157000/244675 (64%)]\tLoss: 1.084630\n",
      "Train Epoch: 10 [158000/244675 (65%)]\tLoss: 0.735361\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3025\n",
      "Train Epoch: 10 [159000/244675 (65%)]\tLoss: 0.662446\n",
      "Train Epoch: 10 [160000/244675 (65%)]\tLoss: 1.042472\n",
      "Train Epoch: 10 [161000/244675 (66%)]\tLoss: 0.743247\n",
      "Train Epoch: 10 [162000/244675 (66%)]\tLoss: 0.759905\n",
      "Train Epoch: 10 [163000/244675 (67%)]\tLoss: 0.736563\n",
      "Train Epoch: 10 [164000/244675 (67%)]\tLoss: 0.807072\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2961\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2633\n",
      "Train Epoch: 10 [165000/244675 (67%)]\tLoss: 0.733592\n",
      "Train Epoch: 10 [166000/244675 (68%)]\tLoss: 0.686442\n",
      "Train Epoch: 10 [167000/244675 (68%)]\tLoss: 0.577953\n",
      "Train Epoch: 10 [168000/244675 (69%)]\tLoss: 0.655335\n",
      "Train Epoch: 10 [169000/244675 (69%)]\tLoss: 0.604235\n",
      "Train Epoch: 10 [170000/244675 (69%)]\tLoss: 0.938145\n",
      "Train Epoch: 10 [171000/244675 (70%)]\tLoss: 0.680126\n",
      "Train Epoch: 10 [172000/244675 (70%)]\tLoss: 0.677196\n",
      "Train Epoch: 10 [173000/244675 (71%)]\tLoss: 0.627093\n",
      "Train Epoch: 10 [174000/244675 (71%)]\tLoss: 0.681658\n",
      "Train Epoch: 10 [175000/244675 (72%)]\tLoss: 0.672795\n",
      "Train Epoch: 10 [176000/244675 (72%)]\tLoss: 0.656646\n",
      "Train Epoch: 10 [177000/244675 (72%)]\tLoss: 0.809344\n",
      "Train Epoch: 10 [178000/244675 (73%)]\tLoss: 0.665764\n",
      "Train Epoch: 10 [179000/244675 (73%)]\tLoss: 0.782235\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 5674\n",
      "Train Epoch: 10 [180000/244675 (74%)]\tLoss: 0.764729\n",
      "Train Epoch: 10 [181000/244675 (74%)]\tLoss: 0.620779\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2679\n",
      "Train Epoch: 10 [182000/244675 (74%)]\tLoss: 0.837793\n",
      "Train Epoch: 10 [183000/244675 (75%)]\tLoss: 1.099493\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3203\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2915\n",
      "Train Epoch: 10 [184000/244675 (75%)]\tLoss: 0.811215\n",
      "Train Epoch: 10 [185000/244675 (76%)]\tLoss: 0.776065\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4706\n",
      "Train Epoch: 10 [186000/244675 (76%)]\tLoss: 0.869785\n",
      "Train Epoch: 10 [187000/244675 (76%)]\tLoss: 0.740706\n",
      "Train Epoch: 10 [188000/244675 (77%)]\tLoss: 0.528898\n",
      "Train Epoch: 10 [189000/244675 (77%)]\tLoss: 0.529318\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2806\n",
      "Train Epoch: 10 [190000/244675 (78%)]\tLoss: 1.035156\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3266\n",
      "Train Epoch: 10 [191000/244675 (78%)]\tLoss: 0.661142\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4799\n",
      "Train Epoch: 10 [192000/244675 (78%)]\tLoss: 0.856012\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2783\n",
      "Train Epoch: 10 [193000/244675 (79%)]\tLoss: 0.705499\n",
      "Train Epoch: 10 [194000/244675 (79%)]\tLoss: 0.822351\n",
      "Train Epoch: 10 [195000/244675 (80%)]\tLoss: 0.523494\n",
      "Train Epoch: 10 [196000/244675 (80%)]\tLoss: 0.895762\n",
      "Train Epoch: 10 [197000/244675 (81%)]\tLoss: 0.821490\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4130\n",
      "Train Epoch: 10 [198000/244675 (81%)]\tLoss: 0.878798\n",
      "Train Epoch: 10 [199000/244675 (81%)]\tLoss: 0.694910\n",
      "Train Epoch: 10 [200000/244675 (82%)]\tLoss: 0.863612\n",
      "Train Epoch: 10 [201000/244675 (82%)]\tLoss: 0.792985\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 4079\n",
      "Train Epoch: 10 [202000/244675 (83%)]\tLoss: 0.534321\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3889\n",
      "Train Epoch: 10 [203000/244675 (83%)]\tLoss: 0.759654\n",
      "Train Epoch: 10 [204000/244675 (83%)]\tLoss: 0.924182\n",
      "Train Epoch: 10 [205000/244675 (84%)]\tLoss: 0.676564\n",
      "Train Epoch: 10 [206000/244675 (84%)]\tLoss: 0.543456\n",
      "Train Epoch: 10 [207000/244675 (85%)]\tLoss: 0.749093\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2742\n",
      "Train Epoch: 10 [208000/244675 (85%)]\tLoss: 0.817335\n",
      "Train Epoch: 10 [209000/244675 (85%)]\tLoss: 0.880275\n",
      "Train Epoch: 10 [210000/244675 (86%)]\tLoss: 0.723239\n",
      "Train Epoch: 10 [211000/244675 (86%)]\tLoss: 0.961438\n",
      "Train Epoch: 10 [212000/244675 (87%)]\tLoss: 0.751880\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 7828\n",
      "Train Epoch: 10 [213000/244675 (87%)]\tLoss: 0.917084\n",
      "Train Epoch: 10 [214000/244675 (87%)]\tLoss: 0.697915\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2604\n",
      "Train Epoch: 10 [215000/244675 (88%)]\tLoss: 1.183088\n",
      "Train Epoch: 10 [216000/244675 (88%)]\tLoss: 0.661644\n",
      "Train Epoch: 10 [217000/244675 (89%)]\tLoss: 0.744128\n",
      "Train Epoch: 10 [218000/244675 (89%)]\tLoss: 0.854796\n",
      "Train Epoch: 10 [219000/244675 (90%)]\tLoss: 0.586019\n",
      "Train Epoch: 10 [220000/244675 (90%)]\tLoss: 0.876816\n",
      "Train Epoch: 10 [221000/244675 (90%)]\tLoss: 0.623203\n",
      "Train Epoch: 10 [222000/244675 (91%)]\tLoss: 0.661791\n",
      "Train Epoch: 10 [223000/244675 (91%)]\tLoss: 0.904579\n",
      "Train Epoch: 10 [224000/244675 (92%)]\tLoss: 0.912123\n",
      "Train Epoch: 10 [225000/244675 (92%)]\tLoss: 0.980152\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3330\n",
      "Train Epoch: 10 [226000/244675 (92%)]\tLoss: 0.568773\n",
      "Train Epoch: 10 [227000/244675 (93%)]\tLoss: 0.921762\n",
      "Train Epoch: 10 [228000/244675 (93%)]\tLoss: 0.910141\n",
      "Train Epoch: 10 [229000/244675 (94%)]\tLoss: 0.624633\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2881\n",
      "Train Epoch: 10 [230000/244675 (94%)]\tLoss: 0.629061\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3624\n",
      "Train Epoch: 10 [231000/244675 (94%)]\tLoss: 0.727212\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2967\n",
      "Train Epoch: 10 [232000/244675 (95%)]\tLoss: 0.599853\n",
      "Train Epoch: 10 [233000/244675 (95%)]\tLoss: 0.626862\n",
      "Train Epoch: 10 [234000/244675 (96%)]\tLoss: 0.578876\n",
      "Train Epoch: 10 [235000/244675 (96%)]\tLoss: 1.130442\n",
      "Train Epoch: 10 [236000/244675 (96%)]\tLoss: 0.691569\n",
      "Train Epoch: 10 [237000/244675 (97%)]\tLoss: 0.936147\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2909\n",
      "Train Epoch: 10 [238000/244675 (97%)]\tLoss: 0.799993\n",
      "Train Epoch: 10 [239000/244675 (98%)]\tLoss: 0.672735\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2708\n",
      "Train Epoch: 10 [240000/244675 (98%)]\tLoss: 0.782854\n",
      "Train Epoch: 10 [241000/244675 (98%)]\tLoss: 0.859066\n",
      "Train Epoch: 10 [242000/244675 (99%)]\tLoss: 0.801957\n",
      "Train Epoch: 10 [243000/244675 (99%)]\tLoss: 0.813255\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 9130\n",
      "Train Epoch: 10 [244000/244675 (100%)]\tLoss: 0.866548\n",
      "['bate mon e therpracted hagoen iceted life as partoer o gremen', 'i will not beu lnd bon rsiso dos playsoo', 'the scuks spray is powfu in augh to ward off bars in other potintral atackors', 'ward in doketts married tleater that year', 'as woer snoo was prasnt of the grete very chebe ith comers', 'he develpe earlle entresstan flight acumibaor in extreen suturations', 'a pal martn refuses sonia access toughe backa', 'the provents is rich in mineral deposits incle goold in coper', 'accurding two sase thegeagrafic studian languages deals with hdexteral nat in tenal manguristics', 'its under serfice is concae from beforbackward and conves fromside teside'] ['bathalumang ether granted hagorn a second life as part of their agreement', 'i will not be alarmed though your sister does play so well', \"a skunk's spray is powerful enough to ward off bears and other potential attackers\", 'ward and dawkins married later that year', 'as well snelgrove was president of the greater barrie chamber of commerce', 'he developed early interests in flight and human behaviour in extreme situations', 'appalled martin refuses sonia access to rebecca', 'the province is rich in mineral deposits including gold and copper', 'according to saussure the geographic study of languages deals with external not internal linguistics', 'its under surface is concave from before backward and convex from side to side']\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2973\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3347\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2737\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2639\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2667\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2771\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 3998\n",
      "Skipped a batch because it is too large (>2600). Sequence length is 2990\n",
      "Test set: Average loss: 0.8058, Average CER: 0.255305 Average WER: 0.6050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "attempt_number = 1\n",
    "\n",
    "learning_rate = 5e-4\n",
    "batch_size = 10\n",
    "epochs = 10\n",
    "\n",
    "losses_train, losses_val = main(train_filenames_labels, val_filenames_labels, \n",
    "                                root, attempt_number, learning_rate, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([list(i) for i in zip(*losses_train)])\n",
    "writer = pd.ExcelWriter('losses_train_' + f'attemptnumber_{attempt_number}' + '.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer,sheet_name='losses_train',index=False)\n",
    "writer.save()\n",
    "\n",
    "df = pd.DataFrame(losses_val)\n",
    "writer = pd.ExcelWriter('losses_val_' + f'attemptnumber_{attempt_number}' + '.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer,sheet_name='losses_val',index=False)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
